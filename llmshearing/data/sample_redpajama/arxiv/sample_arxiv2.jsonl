{"text": "\\section{Introduction}\n\\label{sec:intro}\nSpoofing attacks and countermeasures for automatic speaker verification (ASV) have aroused keen interests in academia and the industry. \nWhile ASV systems aim to verify the identity of target speakers, spoofing attacks attempt to manipulate the verification results using various technologies, leading to dramatic performance degradation \\cite{wu2015asvspoof, kinnunen2017asvspoof, todisco2019asvspoof, yamagishi2021asvspoof, wang2021comparative}.\nIn order to ensure the robustness and security of ASV systems, CM is a necessary technique to detect and defend spoofing attacks.\n\nThe vulnerability of ASV systems was revealed in \\cite{wu2015asvspoof, kinnunen2017asvspoof, todisco2019asvspoof, yamagishi2021asvspoof}, under speech synthesis and voice conversion (VC) attacks. \nCurrently, various techniques have been proposed to perform effective attacks, including audio replay \\cite{wu2014study}, adversarial noise \\cite{kreuk2018fooling, das2020attacker}, more advanced text-to-speech (TTS) and VC models \\cite{de2010evaluation, wu2014voice, kons2013voice}.\nIn addition, many works have been done to investigate state-of-the-art CM strategies.\nThe current solutions leverage end-to-end deep neural networks (DNNs) \\cite{monteiro2019development, monteiro2020generalized}, trying to distinguish artifacts and unnatural cues of spoofing speech from bona fide speech.\nAnd thanks to a series of challenges and datasets \\cite{wu2015asvspoof, kinnunen2017asvspoof, todisco2019asvspoof, yamagishi2021asvspoof}, many novel techniques were introduced to achieve promising CM performances \\cite{monteiro2019development, monteiro2020generalized,zhang2021one, wu2022partial,chen2020generalization, jung2021aasist,li2021replay, tak2021graph, jung2019replay,shim2020integrated,jung2020study,shim2019self, tak2022automatic,tak2021end,kamble2018effectiveness,tak2020spoofing,tak2018novel}. \n\nHowever, previous works only take the stand-alone CM models into account.\nRecently, a spoofing aware speaker verification (SASV) challenge \\cite{jung2022sasv} was proposed as a special session in ISCA INTERSPEECH 2022.\nThis challenge aims to facilitate the research of integrated CM and ASV models, arguing that jointly optimizing CM and ASV models will lead to better performance.\nTo measure the performance of integrated models, a SASV-EER was proposed in this challenge as a primary metric, which is a variant of classic equal error rate (EER). Under this metric, the test utterances in trials belong to one of three types: impostors, target speakers, and spoofing attacks.\nIn further, the SASV-EER can be subsetted into SV-EER (impostors vs. targets) and SPF-EER (targets vs. spoof). The former is for evaluating speaker verification performance, and the latter is for evaluating anti-spoofing performance.\nIn this way, this metric expects the model can accept target speakers and reject any alternatives, including the impostors and spoofing attacks.\nThis metric is a straightforward assessment for integrated SASV systems.\n\nThis paper described our submitted system for the SASV Challenge 2022.\nIn order to take advantage of existing well-designed models in CM and ASV areas, we proposed a novel multi-model and multi-scale fusion framework.\nCompared with purely scoring fusion and embedding fusion methods, this framework utilizes embeddings from CM models in the first-level fusion, propagating CM embeddings into a CM block to obtain a CM score.\nIn the second-level fusion, the CM score and ASV scores directly from ASV systems will be concatenated into a prediction block for the final decision.\nIn contrast to our previous work \\cite{wu2022sasv} that only simply concatenates the embeddings from different CM models, we considered the potentials of pooling strategies in terms of feature aggregation, and investigated various pooling methods \\cite{you2019multi, snyder2018x, zhu2018self, okabe2018attentive} when fusing embeddings across different CM models.\nBased on the proposed fusion framework, we presented the fusion strategies of a series of state-of-the-art CM and ASV models with different pooling strategies to boost the fusion results.\nAs a result, the best single fusion system has achieved the SASV-EER of 0.97\\% on the evaluation set.\nThen by ensembling the top-5 fusion systems, the final SASV-EER reached 0.89\\% on the evaluation set, while this number in the best baseline system from the SASV challenge is 6.37\\%.\n\n\\begin{figure*}[ht]\n  \\centering\n  \\centerline{\\includegraphics[width=1.0\\linewidth]{figures/method.png}}\n  \\caption{The proposed multi-model \\& multi-level fusion framework.}\n  \\label{fig:method}\n\\end{figure*}\n\\section{Background}\n\n\\subsection{SASV strategies}\nGiven the enrollment utterance $x_{e}$ and the testing utterance $x_{t}$, spoofing-aware speaker verification (SASV) systems aim at telling $y_{t} = 1$ if $x_{t}$ comes from the same speaker as $x_{e}$, or $y_{t} = 0$ if $x_{t}$ comes from another speaker or $x_{t}$ is a spoofing attack. There are two typical strategies for constructing a SASV system: multi-task learning strategy and fusion-based strategy. \n\nThe multi-task learning strategy trains the models jointly with both speaker verification and anti-spoofing objectives, which is intuitive to be adopted. \nThe two objectives share the same backbone and thus the features and embeddings, while each objective has its own predicting head and loss function. \nIt is worth noting that speaker verification and anti-spoofing objectives are contradictory in some respects. \nThe former drives the model to erase device and environment information to more robustly identify speakers; in contrast, the latter prompts the model to capture device and environment traces, then tells forged spoofing from authentic utterances \\cite{shim2020integrated}.\n\nAlternatively, the fusion-based strategy has the potential to reach better SASV performance leveraging state-of-the-art CM and ASV models trained on large-scale datasets. \nConsidering this superiority, we propose our solutions for the SASV challenge based on a novel multi-model and multi-level fusion strategy. \n\n\\subsection{Baseline systems}\nThe challenge organizer provides two baseline systems. Each system is based upon ECAPA-TDNN model \\cite{desplanques2020ecapa} as the ASV subsystem and AASIST model \\cite{jung2021aasist} as the CM subsystem. The key difference between the two systems is how they fuse ASV and CM subsystem - Baseline1 adopts the score-level fusion, while Baseline2 adopts the embedding-level fusion. \n\n\\section{Method}\n\n\\begin{figure*}[ht]\n\\centering\n\\subfigure[ECAPA-TDNN]{\n\\centering\n\\includegraphics[width=0.32\\linewidth]{figures/experiments/ecapa_sv_test.pdf}\n}\n\\subfigure[AASIST]{\n\\centering\n\\includegraphics[width=0.32\\linewidth]{figures/experiments/aas_cm_test.pdf}\n}\n\\subfigure[Proposed top-5 ensemble system]{\n\\centering\n\\includegraphics[width=0.32\\linewidth]{figures/experiments/fusion-sap-768.pdf}\n}\n\\centering\n\\caption{The histogram plots of the output scores predicted by ASV, CM, and the proposed top-5 ensemble system. Other proposed system variants also have similar histogram shapes as (c).}\n\\label{fig:hist}\n\\end{figure*}\n\nAlthough achieving acceptable performances, baseline systems' fusion strategies are relatively simple and naive. Baseline1's score-level fusion does not guarantee that the ASV score and the CM score belong to a unified space and an identical magnitude. Baseline2 crudely concatenates three embeddings and throws the product into a DNN, lacking fine-grained fusion. These inadequacies motivate us to explore further possibilities of fusion strategies in the SASV task.\n\nIn this paper, we proposed a multi-model and multi-level fusion strategy. In terms of the width, it employs multiple pre-trained ASV and CM models as plug-and-play components, where users can expand or shrink the scale of the model according to their needs. In terms of the depth of this strategy, it fuses CM embeddings to calculate a score in the first-level fusion, which is then integrated with the ASV models' outputs, and yields the final prediction in the second-level fusion.\n\n\\subsection{Overall structure}\nThe overall framework is shown in Figure \\ref{fig:method}, where $x_{e}$ and $x_{t}$ are the input enrollment and testing utterances, respectively; \\textit{ASV-1}, \\textit{ASV-2}, ..., \\textit{ASV-m} denote $m$ pre-trained ASV models; \\textit{CM-1}, \\textit{CM-2}, ..., \\textit{CM-n} denote $n$ pre-trained CM models. Given a trial $\\{x_{e}, x_{t}\\}$, a series of cosine scores $\\{s_{sv}^1, s_{sv}^2, \\ldots, s_{sv}^m\\}$ are derived from $m$ ASV models. Given the testing utterance $x_{t}$, a series of CM embeddings $\\{h^{1}, h^{1}, \\ldots, h^{n}\\}$ are extracted through $n$ CM models. Next comes the first level of fusion, where the $n$ embeddings are integrated into a $h_{cm}$ by concatenation or a pooling method.\nOur previous work \\cite{wu2022sasv} investigated the capacity of concatenation for SASV. \nIn this work, we further extend the potential of concatenation by making the CM block in Figure \\ref{fig:method} deeper, and we also adopt pooling methods to further improve the fusion performance.\nFurther, $h_{cm}$ goes through a CM Block to better digest fused embeddings and then a 2-dimension countermeasure score $s_{cm}$ is predicted. With $s_{cm}$ and $\\{s_{sv}^1, s_{sv}^2, \\ldots, s_{sv}^m\\}$ well prepared, they are concatenated and fed into the Predictor to yield the final prediction $\\hat{y_t}$, where the second-level fusion is performed.\n\n\\subsection{Strategies in the first-level fusion}\nFor the first-level fusion, we attempt concatenation \\cite{wu2022sasv} or one of the four kinds of pooling methods to synthesize $h_{cm}$ separately, and have conducted extensive experiments accordingly.\nSuppose $\\bm{H}=\\{h^{1}, h^{1}, \\ldots, h^{n}\\}$, and the length of all the CM embeddings are projected into a same length $d_h$ by feed forward layers before performing concatenation or pooling. The four candidate pooling methods are Temporal Average Pooling (TAP) \\cite{you2019multi}, Temporal Statistics Pooling (TSP) \\cite{snyder2018x}, Self-attentive Pooling (SAP) \\cite{zhu2018self}, and Attentive Statistics Pooling (ASP) \\cite{okabe2018attentive}.\n\nTAP is to calculate the mean value along the channels to obtain the $h_{cm}$. \n\nTSP calculates channel-wise mean and standard deviation, then concatenates the mean vector and standard deviation vector together as $h_{cm}$.\n\nIn SAP, the self-attention mechanism takes $\\bm{H}$ as input and outputs an annotation matrix $\\bm{A}$:\n\\begin{equation}\n     \\bm{A} = softmax(tanh(\\bm{H}^T\\bm{W}_1)\\bm{W}_2)\n    \\label{eq:SAP-weight}\n\\end{equation}\nwhere $\\bm{W}_1$ is a matrix of size $d_h\\times d_a$; $\\bm{W}_2$ is a matrix of size $d_a\\times d_r$, and $d_r$ is a hyper-parameter that represents the number of attention heads; The $softmax(\u00b7)$ is performed column-wise. We set $d_r=1$, therefore $\\bm{A}$ degenerates into an annotation vector. Weighted by $\\bm{A}$, $h_{cm}$ is calculated as the weighted mean:\n\\begin{equation}\n     h_{cm} =\\Tilde{\\mu} = \\bm{HA}\n    \\label{eq:SAP-mean}\n\\end{equation}\n\nFor ASP, not only it calculate a attention-weighted mean as SAP do, but also it calculate a attention-weighted standard deviation:\n\\begin{equation}\n     \\Tilde{\\sigma} =\\sqrt{\\sum_{i=1}^{n}{\\alpha^i h^i\\odot h^i }-\\Tilde{\\mu}\\odot\\Tilde{\\mu}}\n    \\label{eq:ASP-sigma}\n\\end{equation}\nwhere $\\alpha^i$denotes the $i^{th}$ element of the annotation vector $\\bm{A}$, $\\odot$ represents the Hadamard product. By concatenating  $\\Tilde{\\mu}$ and $\\Tilde{\\sigma}$, $h_{cm}$ is derived.\n\nAfter one of the above pooling methods or concatenation, derived $h_{cm}$ goes through CM Block, which is a multi-layer perceptron, and generates a two-dimension score reflecting the possibilities the testing utterances is the target or not.\n\n\\subsection{Loss function}\nThe ASV and CM models parameters are well pre-trained and thus frozen.\nThe learnable modules include CM Block and the Predictor, which are multi-layer perceptrons. To prompt the model to learn to distinguish the target trials from other trials, we use the cross-entropy loss on $s_{cm}$ output by CM Block and $\\hat{y}_{t}$ output by the Predictor respectively.\n\n\\section{Experiments}\n\\label{sec:exp}\n\n\\subsection{Experimental setup}\nIn the SASV Challenge 2022 \\cite{jung2022sasv}, participants are restricted to utilise ASVspoof 2019 \\cite{wang2020asvspoof} and VoxCeleb2 \\cite{chung2018voxceleb2} datasets for training the anti-spoofing and ASV model, respectively. \n\nThree EERs, namely SV-EER, SPF-EER and SASV-EER are measured as the evaluation metrics, and SASV-EER is the main metric in the Challenge.\n\nFor the ASV models, we use Resnet34 \\cite{he2016deep}, ECAPA-TDNN \\cite{desplanques2020ecapa} and MFA-Conformer \\cite{zhang2022conformer}.\nFor the countermeasure models, we use AASIST \\cite{jung2021aasist}, AASIST-L, and RawGAT-ST \\cite{tak2021end}, where AASIST-L is a light version of AASIST.\nThe fusion model in Figure~\\ref{fig:method} is trained by Adam optimizer with an initial learning rate of 0.0001.\nWe set the batch size as 32, and the epoch number as 40.\n\n\\begin{table*}[ht]\n\\centering\n\\renewcommand\\arraystretch{1.0}\n\\setlength\\tabcolsep{12pt}\n\\caption{Performance of all systems on the ASVspoof 2019 development and evaluation sets.}\n\\label{tab:all-eer}\n\\begin{tabular}{lccccccc}\n\\hline\n\\hline\n & \\multirow{2}{*}{\\textbf{System}} &  \\multicolumn{2}{c}{\\textbf{SV-EER}} & \\multicolumn{2}{c}{\\textbf{SPF-EER}} & \\multicolumn{2}{c}{\\textbf{SASV-EER}} \\\\ \n  &  & Dev & Eval & Dev & Eval & Dev  & Eval \\\\ \n\\hline\n(A) & ECAPA-TDNN   & 1.64   & 1.86  & 20.28  & 30.75 & 17.37 & 23.84     \\\\\n\\hline\n(B) & AASIST   & 46.01   & 49.24  & 0.07  & 0.67 & 15.86 & 24.38\n \\\\\n\\hline\n(C1) & Baseline1 \\cite{jung2022sasv}   & 32.88   & 35.32  & 0.06 & 0.67 & 13.07 & 19.31\n \\\\\n(C2) & Baseline2 \\cite{jung2022sasv}  & 12.87   & 11.48  & 0.13  & 0.78 & 4.85 & 6.37\n \\\\ \n\\hline\n(D) & MFA-Conformer + AASIST   & 1.48   & 1.47  & 0.20  & 1.08 & 0.88 & 1.35\n \\\\\n \\hline\n(E1) & SV-ALL + AASIST   & 1.42  & 1.30  & 0.27  & 1.61 & 0.81 & 1.41\n \\\\\n(E2) & SV-ALL + AASIST-L  & 1.42  & 1.33  & 0.47 & 3.99 & 0.88 & 2.95\n \\\\\n(E3) & SV-ALL + RawGAT-ST & 1.82  & 1.64  & 0.40  & 0.82 & 1.28 & 1.39\n \\\\\n \\hline\n(F1) & MFA-Conformer + CM-ALL-CAT-256   & 1.91  & 1.66 & 0.20 & 0.64 & 1.01 & 1.30\n \\\\\n(F2) & ECAPA-TDNN + CM-ALL-CAT-256  & 1.39   & 1.73 & 0.20  & 0.74 & 0.81 & 1.40\n \\\\\n(F3) & Resnet34 + CM-ALL-CAT-256 & 1.28 & 1.12  & 0.26 & 1.43 & 0.74 & 1.32\n \\\\\n \\hline\n(G1) & SV-ALL + CM-ALL-CAT-256 & 1.27   & 1.20  & 0.20 & 1.15 & 0.81 & 1.17\n \\\\\n(G2) & SV-ALL + CM-ALL-CAT-512 & 1.35 & 1.15 & 0.20 & 1.12 & 0.74 & 1.14\n \\\\\n(G3) & SV-ALL + CM-ALL-CAT-768 & 1.34 & 1.12 & 0.20 & 0.99 & 0.81 & \\textbf{1.08}\n \\\\\n(G4) & SV-ALL + CM-ALL-CAT-1024 & 1.28 & 1.21 & 0.20 & 0.83 & 0.74 & \\textbf{1.08}\n \\\\\n(G5) & SV-ALL + CM-ALL-CAT-2048 & 1.35 & 1.10 & 0.23 & 1.41 & 0.74 & 1.31\n \\\\\n \\hline\n\n(H1) & SV-ALL + CM-ALL-TAP-768 & 1.35 & 0.99 & 0.2 & 1.10 & 0.61 & \\textbf{1.02}\n \\\\\n(H2) & SV-ALL + CM-ALL-TSP-768 & 1.21 & 1.12 & 0.20 & 1.47 & 0.74 & 1.31\n \\\\\n (H3) & SV-ALL + CM-ALL-SAP-768 & 1.15 & 1.04 & 0.17 & 0.93 & 0.54 & \\textbf{0.99}\n \\\\\n(H4) & SV-ALL + CM-ALL-ASP-768 & 1.18 & 1.37 & 0.15 & 1.58 & 0.67 & 1.51\n \\\\\n(H5) & SV-ALL + CM-ALL-TAP-1024 & 1.28 & 1.15 & 0.13 & 0.56 & 0.61 & \\textbf{0.97}\n \\\\\n(H6) & SV-ALL + CM-ALL-TSP-1024 & 1.11 & 1.12 & 0.20 & 1.77 & 0.61 & 1.43\n \\\\\n(H7) & SV-ALL + CM-ALL-SAP-1024 & 1.15 & 1.16 & 0.20 & 1.45 & 0.61 & 1.28\n \\\\\n (H8) & SV-ALL + CM-ALL-ASP-1024 & 1.51 & 1.68 & 0.40 & 1.01 & 1.08 & 1.49\n \\\\\n \\hline\n (I1) & Top-5 Ensemble & 1.08 & 1.01 & 0.20 & 0.71 & 0.67 & \\textbf{0.89}\n \\\\\n\\hline\n\\hline\n\\end{tabular}\n\\end{table*}\n\n\n\\subsection{Experimental results and analysis}\nAs the requirements by SASV Challenge 2022, we evaluated systems on ASVspoof 2019 development and evaluation sets and reported SA-EER, SPF-EER and SASV EER, shown in Table \\ref{tab:all-eer}. \nA and B denote systems using pure ASV or CM. We omit other systems using only single ASV or CM due to space limitation, yet they are with the same trend.\nC1-C2 denote two baselines provided by the Challenge organizer. \nD-H8 are variants based on the proposed fusion strategy. D denote the fusion systems using only one ASV model and one CM model, e.g., \n'ECAPA-TDNN + AASIST' denotes the fusion of ECAPA-TDNN as the ASV model, and AASIST as the CM model.\nOther systems using only one ASV and CM models are with the same trend.\nE1-E3 denote systems fusing all three ASV models with one CM model. Note that D-E3 ignore the first-level fusion.\nF1-F3 denote systems fusing one ASV model and all three CM models.\nG1-H8 represent systems incorporating all three ASV models and all three CM models, but with different first-level fusion strategies and different sizes of CM Block. For example, 'SV-ALL + CM-ALL-CAT-768' denotes its first-level fusion uses concatenation (abbreviated as 'CAT' in the table), and fused $h_{cm}$ is projected to 768 dimensions in CM Block's first layer. I1 is the ensemble system involving the top-5 best evaluation set SASV-EER systems in A-H8. Figure \\ref{fig:method} illustrates the histogram plots of three typical systems: (a) ECAPA-TDNN, a SOTA ASV system; (b) AASIST, a SOTA CM system; (c) Proposed top-5 ensemble system.\n\n\\subsubsection{Single-objective systems}\n\\textit{Only using speaker verification models.} ECAPA-TDNN performed well on the speaker verification sub-task and achieved 1.86\\%, 1.38\\% and 1.08\\% SV-EERs on the evaluation set, respectively. However, they perform unacceptably on the anti-spoofing sub-task, yielding 30.75\\%, 30.22\\%, 29.76\\% SPF-EER. Spoiled by the spoofing attacks, it is unpractical to perform SASV tasks using only the ASV models. \nTake ECAPA-TDNN model as an example, as shown in Figure \\ref{fig:hist} (a), the pure ASV model can separate target and non-target trials while can hardly distinguish spoofing trials from genuine trials. This phenomenon is predictable because the objective of speaker verification models tends to erase the in-congruent device and environment information to more robustly identify speakers. However, these real-world traces should have helped to defend against spoofing attacks.\n\n\\textit{Only using anti-spoofing models.} In contrast, AASIST, the state-of-the-art CM model, can significantly discriminate spoofing utterances, but randomly guess on the speaker identification sub-task, mainly because of their speaker-unrelated objectives. They achieve SPF-EERs of 0.67\\%, 0.84\\%, 0.96\\% on anti-spoofing, while yielding SV-EERs close to 50\\% for speaker verification. This phenomenon can also be observed in Figure \\ref{fig:hist} (b), where the target and non-target trials' distribution are almost totally overlapped. \n\n\\subsubsection{Baseline systems}\nCompared to the above single-objective models, two baseline systems reveal superiority in the SASV challenge. Among them, Baseline2 achieves a SASV-EER of 8.75\\% on the evaluation set, which is better than Baseline1's 19.15\\%. We argue that the reason is that the simple score-level fusion used in Baseline1 does not guarantee that ASV and CM scores belong to a unified space with a consistent magnitude. As shown in Figure \\ref{fig:hist}, the ASV score ranges from -1 to 1, while the CM score ranges from -20 to 15. Straightforward addition will make the ASV score submerged by the CM score. In comparison, Baseline2 uses a trainable deep neural network to digest ASV and CM embeddings better, which can contribute to distinguishing the target from non-target and spoofing trials. However, the performance of Baseline2 on the speaker verification sub-task is still not satisfactory, which motivates us to propose the multi-model \nand multi-level fusion strategy.\n\n\\subsubsection{Proposed fusion systems}\nThe proposed strategy fuses three SOTA ASV models and three SOTA CM models. \nWith different settings, variety of variant fusion systems (D-H8) are elaborately designed, and an additional ensemble system (I1) is constructed by integrating the top-5 best evaluation set SASV-EER systems from A-H8. Table \\ref{tab:all-eer} illustrates the performance of all systems, and G1 is the best result of our previous work \\cite{wu2022sasv}.\n\nFrom Table \\ref{tab:all-eer}, all the proposed systems outperform the baseline models with a large margin on the SASV-EER metrics, while retaining universally good performances on SV-EER and SPF-EER. \nThe proposed models perform consistently well on the speaker verification task, and quite a few can reach or even surpass the performance of the SOTA ASV models. G1-H8 show better performance than D-F3 in general, benefiting from the more comprehensive model fusion. \n\nUsing concatenation for the first-level fusion, we attempt different CM Block sizes. From G1 to G5, as we increase the size of the CM Block, the SASV-EER of the system decreases. \nUntil adding a layer with an output dimension of 2048 to the bottom of the CM Block, the performance saturates.\n\nSince G3 and G4 have the best SASV-EER, we employ their CM Block settings and further explore the impact of replacing concatenation with different pooling methods.\nIn H1-H8, under both CM block settings, TAP bring more benefits to the system than other methods. \nBoth statistics pooling methods (TSP, ASP) are less effective than TAP, SAP, and concatenation. \nA possible reason is that we only compute the standard deviation for up to three CM embeddings, which is not stable during training. \nRather than representing useful knowledge, the standard deviation seems to be more of a noise. \nWe argue that if more CM models enroll, the system will benefit from statistics pooling methods, and we will leave it as future work.\n\nThe best individual system is with SASV-EER as 0.97\\%. Moreover, the top-5 ensemble system (I1) achieves a SASV-EER as low as 0.89\\%, which is a 86\\% relative improvement compared to Baseline2. \nFigure \\ref{fig:hist} (c) shows that the distributions of the target, non-target and spoofing trials are well-separated, which verifies the effectiveness of the proposed method.\n\n\\section{Conclusion}\n\nThis work proposes a novel multi-model and multi-level fusion strategy to tackle the SASV task.\nThe two-level fusion method can take advantage of the state-of-the-art ASV and CM models.\nAs a result, the best single fusion system achieves the SASV-EER of 0.97\\%.\nWhat's more, by ensembling the top-5 systems, the final SASV-EER reaches 0.89\\% on the evaluation set, which is 86\\% relative reduction compared to the best baseline, Baseline2, and 24\\% relative reduction compared to our previous work \\cite{wu2022sasv}.\nIn the future work, we will introduce more CM models to investigate the potential of the proposed method.\n\n\\input{main.bbl}\n\\bibliographystyle{IEEEtran}\n\n\n\\end{document}\n", "meta": {"timestamp": "2022-03-30T02:27:11", "yymm": "2203", "arxiv_id": "2203.15377", "language": "en", "url": "https://arxiv.org/abs/2203.15377"}}
{"text": "\\section{Introduction}\n\\label{SEC:intro}\n\nWe consider the following Cauchy problem for the quadratic nonlinear Schr\\\"odinger equation (NLS) on the two-dimensional torus $\\mathbb{T}^2 = (\\mathbb{R} / \\mathbb{Z})^2$:\n\\begin{equation}\n\\begin{cases}\ni \\partial_t u + \\Delta u = |u|^2 \\\\\nu|_{t = 0} = u_0.\n\\end{cases}\n\\label{qNLS}\n\\end{equation}\n\n\n\\noindent\nOur main goal is to prove local well-posedness of \\eqref{qNLS} in the low regularity setting.\nOver the  last forty years, the Cauchy problems of NLS with various types of nonlinearities \nhave been studied extensively.\nThe following NLS with a gauge-invariant nonlinearity:\n\\begin{equation}\ni \\partial_t u + \\Delta u =  \\pm |u|^{p-1} u \n\\label{NLS}\n\\end{equation}\n\n\\noindent\nis the most well-known and well-studied example.\nIn the case of the $d$-dimensional torus $\\mathbb{T}^d = (\\mathbb{R}/\\mathbb{Z})^d$, Bourgain  \\cite{B93} \nintroduced the Fourier restriction norm method (via the $X^{s, b}$-spaces)\nand proved local well-posedness of \\eqref{NLS}\nin  the low regularity setting.\nIn particular, when $d = 2$, he proved the following \n$L^4$-Strichartz estimate (with a derivative loss):\n\\begin{align}\n\\| e^{it \\Delta}  f\\|_{L^4([0, 1]; L^4(\\mathbb{T}^2))} \\lesssim \\|f\\|_{H^s}\n\\label{Str1}\n\\end{align}\n\n\\noindent\nfor any $s> 0$, which allowed him to prove\nlocal well-posedness of the cubic NLS, \\eqref{NLS} with $p = 3$, \nin $H^s(\\mathbb{T}^2)$, for any $s> 0$. We point out that the estimate \\eqref{Str1}\nfails when $s = 0$ and \nthe well-posedness issue of the cubic NLS\nin the critical space $L^2(\\mathbb{T}^2)$ remains\na challenging open problem; see \\cite{B93, TTz, Kishi}.\n\nLet us now turn to the quadratic NLS \\eqref{qNLS}.\nBy interpolating \\eqref{Str1} with the trivial $L^2$-bound, \nwe obtain the $L^3$-Strichartz estimate with a derivative loss.\nThen by proceeding as in~\\cite{B93}, \nwe immediately obtain local well-posedness\nof \\eqref{qNLS} in $H^s(\\mathbb{T}^2)$ for any $s > 0$.\nDue to the derivative loss in the Strichartz estimate, \nhowever, its well-posedness in $L^2(\\mathbb{T}^2)$ has remained open \nfor the last thirty years.\nIn this paper, we prove that \\eqref{qNLS} is\nindeed locally well-posed in $L^2(\\mathbb{T}^2)$.\n\n\n\n\\begin{theorem}\\label{THM:LWP}\nThe quadratic NLS \\eqref{qNLS} is locally well-posed in $L^2(\\mathbb{T}^2)$. More precisely, given any $u_0 \\in L^2(\\mathbb{T}^2)$, there exists $T = T(\\| u_0 \\|_{L^2}) > 0$ and a unique solution $u\n\\in C([-T, T]; L^2(\\mathbb{T}^2))$ to \\eqref{qNLS} with $u|_{t=0} = u_0$.\n\\end{theorem}\n\n\nOur proof is based on the Fourier restriction norm method and, as such, \n the uniqueness holds only in (the local-in-time version of) the relevant $X^{s,b}$-space.\n\nA few remarks are in order.\nIn \\cite{Kishi3}, Kishimoto proved ill-posedness of \\eqref{qNLS} in $H^s(\\mathbb{T}^2)$ for $s < 0$\nand thus\nTheorem \\ref{THM:LWP} is sharp.  See Remark \\ref{REM:1} below.\nTheorem \\ref{THM:LWP} is also sharp in the sense that \nlocal-in-time solutions constructed in Theorem \\ref{THM:LWP}\ncan not be in general extended globally in time.\nIndeed,  the second author \\cite{O1} proved a finite-time blowup result\nfor \\eqref{qNLS}. See also~\\cite{FO}.\nThis argument can be easily extended to $L^2(\\mathbb{T}^2)$, yielding the following proposition.\n\n\\begin{proposition}\nLet $s \\geq 0$ and $u_0 \\in H^s(\\mathbb{T}^2)$. If the initial data $u_0$ satisfies\n\\[ \\Im \\int_{\\mathbb{T}^2} u_0 dx < 0 \\quad \\text{or} \\quad \\Re \\int_{\\mathbb{T}^2} u_0 dx \\neq 0,  \\]\nthen the forward maximal existence time $T^*$ of the solution $u$ to \\eqref{qNLS} \nwith $u|_{t = 0} = u_0$ is finite\nand we have  $\\liminf_{t \\nearrow T^*} \\| u (t) \\|_{H^s} = \\infty$.\n\\end{proposition}\n\n\nSee \\cite{FO} on the lifespan of solutions to \\eqref{qNLS}.\nWe point out that, in the one-dimensional case, Fujiwara and Georgiev \\cite{FG}\nrecently proved the criterion for global existence.\nConsider the quadratic NLS \\eqref{qNLS} on the one-dimensional torus $\\mathbb{T}$.\nThen, there exists a global $L^2$-solution $u$ to \\eqref{qNLS} on $\\mathbb{T}$\nif and only if $\\Re u_0 = 0$ and $\\Im u_0 = \\mu$\nfor some $\\mu \\geq 0$.  In particular, any global $L^2$-solution\nis necessarily constant in space. It would be of interest to investigate this issue\nin the two-dimensional case.\n\n\n\nAs mentioned above, our proof of Theorem \\ref{THM:LWP} is based\non the Fourier restriction norm method.\nMore precisely, Theorem \\ref{THM:LWP} follows\nfrom a standard contraction argument, once we proof the following bilinear estimate.\n\n\\begin{proposition}\\label{PROP:bilin}\nLet $0 < T \\leq 1$. Let $\\delta_1 > \\delta_2 > 0$ be sufficiently small.\nThen,  for $s\\ge 0 $, we have\n\\begin{align} \\| u \\overline{v} \\|_{X_T^{s,-\\frac 12 + \\delta_1}} \\lesssim \\| u \\|_{X_T^{s, \\frac 12 + \\delta_2}} \\| v \\|_{X_T^{s, \\frac 12 + \\delta_2}}. \n\\label{bilin1}\n\\end{align}\n\\end{proposition}\n\nHere, $X^{s, b}_T$ denotes the local-in-time version of the $X^{s, b}$-space.\nSee Section \\ref{SEC:2} for the definition.\nAs mentioned above, the $L^3$-Strichartz comes with a derivative loss\nand hence can not be used directly to prove \\eqref{bilin1}.\nWe instead separate the proof into two cases:\n(i)~non-resonant interaction and (ii) nearly resonant interaction.\nIn the non-resonant case, thanks to the gain of derivative via\nmultilinear dispersion, \nwe can make up for the loss of the derivative in the $L^3$-Strichartz estimate.\nIn the nearly resonant case, we notice that the angle\nbetween the second incoming wave and the outgoing wave\nis almost perpendicular.\nThis angular restriction allows us to prove the estimate without any derivative loss.\nSee also \\cite{Tao4, CKSTT08}.\nSee Section~\\ref{SEC:bilin} for details.\n\n\nWe conclude this introduction by several remarks.\n\n\n\\begin{remark}\\label{REM:1} \\rm\n(i) Unlike \\cite{B93}, \nour argument does not rely on intricate number theoretic properties\nand thus Theorem \\ref{THM:LWP} also holds on a general torus\n$\\mathbb{T}^2_{\\pmb{\\alpha}} = (\\mathbb{R}/\\alpha_1 \\mathbb{Z}) \\times (\\mathbb{R}/\\alpha_2 \\mathbb{Z}) $\nfor any ratio $\\pmb{\\alpha} = (\\alpha_1, \\alpha_2)$ with  $\\alpha_1, \\alpha_2 > 0$.\n   This is essentially due to the fact that\n   the key bilinear Strichartz estimate\n   and the counting lemma (see Lemmas \\ref{LEM:bilin} and \\ref{LEM:count} below) hold on a general flat torus (see Lemmas 2.5 and 2.9 in~\\cite{Kish13}) and the $L^4$-Strichartz estimate in Lemma \\ref{LEM:L4} also holds on  a general torus with an $\\varepsilon$ derivative loss (see Theorem 2.4 in \\cite{BD15}).\nWe also mention\n\\cite{B07, CW, GOW, BD15, KV, DGG}\nfor further discussions on the Strichartz estimates\nand well-posedness of NLS on irrational tori.\n\n\n\n\n\\smallskip\n\n\\noindent\n(ii) The bilinear estimate \\eqref{bilin1} also holds\nfor $u v$ and $\\overline{u} \\overline{v}$ in place of $u \\overline{v}$ on the left-hand side.\nIndeed,  for $\\overline{u} \\overline{v}$, Gr\\\"unrock \\cite{Gr}\nproved the corresponding bilinear estimate for $s > - \\frac 12$.\nAs for $uv$, a slight modification of the proof of Proposition \\ref{PROP:bilin}\nyields the corresponding bilinear estimate for $s \\ge 0$.\nSee Remark \\ref{REM:bilin}.\nThese bilinear estimates yield local well-posedness\nin the corresponding ranges.\n\nThe quadratic NLS on $\\mathbb{T}^2$ is critical\nin $H^{-1}(\\mathbb{T}^2)$ with respect to the scaling symmetry.\nNonetheless, Kishimoto \\cite{Kishi3} proved ill-posedness of the \nquadratic NLS on $\\mathbb{T}^2$ in $H^s(\\mathbb{T}^2)$\nfor (a) $s < 0 $ with the nonlinearity $\\mathcal{N}(u) =  |u|^2$\nand  \n(b) $s \\le -1$ with $\\mathcal{N}(u) = u^2$ or $\\overline{u}^2$.\nAs mentioned above, the well-posedness theory of \nthe quadratic NLS~\\eqref{qNLS} with the nonlinearity $|u|^2$\nis now complete.\nOn the other hand, as for the nonlinearity $u^2$ or $\\overline u^2$, \nthere is still a gap between the well-posedness and ill-posedness regularities.\n\n\n\\smallskip\n\n\n\\noindent\n(iii) \nIt is conjectured in \\cite{B93} that, on $\\mathbb{T}^2$, the $L^p$-Strichartz estimate holds\nwithout any derivative loss as long as $  p < 4$.\nAt this point, the $L^p$-Strichartz estimate on $\\mathbb{T}^2$\nfor $2< p < 4$ is known to hold with a slight loss of derivative\nand  this conjecture remains open.\nBy considering a multilinear version of the Strichartz estimate, however, \nwe obtain \nthe following tri-linear version of the $L^3$-Strichartz estimate without any derivative loss:\n\\begin{align}\n\\bigg|\\int_0^1 \\int_{\\mathbb{T}^2} \\prod_{j = 1}^3 \\big(e^{it \\Delta} \\phi_j\\big)^*\ndx dt \\bigg|\n\\lesssim \\prod_{j = 1}^3 \\|\\phi_j \\|_{L^2},\n\\label{tri1}\n\\end{align}\n\n\\noindent\nwhere $u^*$ denotes $ u$ or $\\overline u$.\nThe estimate \\eqref{tri1} follows easily from \nthe bilinear estimate \\eqref{bilin1} (also for $uv$ and $\\overline u \\overline v$)\nand the duality.\nWe point out that it is crucial that \nwe have a product structure on the left-hand side of \\eqref{tri1}.\n\n\n\\smallskip\n\n\\noindent\n(iv) Lastly, let us consider the following quadratic NLS on $\\mathbb{T}^2$\nwith a gauge-invariant nonlinearity:\n\\begin{equation}\ni \\partial_t u + \\Delta u = \\pm |u| u .\n\\label{NLS9}\n\\end{equation}\n\n\\noindent\nThis equation is of particular interest in view of the mass and energy conservations.\nIn particular, local well-posedness in $L^2(\\mathbb{T}^2)$\ntogether with the mass conservation would imply\nglobal well-posedness in the same space.\n\n\nWhen $s> 0$, local well-posedness of \\eqref{NLS9} in $H^s(\\mathbb{T}^2)$\neasily \nfollows from the $L^4$-Strichartz estimate~\\eqref{Str1}\n(more precisely, Lemma \\ref{LEM:L4} below).\nWhen $s = 0$, however, the non-algebraic nature of the nonlinearity\nmakes the local well-posedness problem of \\eqref{NLS9} in $L^2(\\mathbb{T}^2)$\nrather challenging.\nFor example, the tri-linear estimate \\eqref{tri1}\nis not useful to study \\eqref{NLS9}\nand,  moreover, multilinear analysis via the Fourier restriction\nnorm method (such as that presented in this paper)\nis not applicable due to the presence of \n$|u|$.\nWhile there are well-posedness results\n\\cite{OOP, Lee}\non the periodic  NLS with non-algebraic gauge-invariant nonlinearities, \none would need a much more intricate argument\nto prove local well-posedness of \\eqref{NLS9} in $L^2(\\mathbb{T}^2)$.\n\n\n\\end{remark}\n\n\n\n\n\n\n\n\n\n\n\\section{Notations and preliminary lemmas}\n\\label{SEC:2}\n\n\n\n\\subsection{Notations}\nFor a spacetime function $u$ defined on $\\mathbb{R} \\times \\mathbb{T}^2$, we write $\\mathcal{F}_{t,x} u$ or $\\widehat u$ to denote the spacetime Fourier transform of $u$.\nIn the following, we drop the inessential factor of $2\\pi$.\nWe  also set $\\jb{\\,\\cdot\\,} = (1 + |\\cdot|^2)^\\frac{1}{2}$.\n\nGiven a dyadic number $N  \\ge 1$, we let $P_N$ be the spatial  frequency projector onto the\n frequencies \n\\[ \\mathfrak{P}_N := \\big\\{ (\\tau, n)\\in \\mathbb{R}\\times \\mathbb{Z}^2 : \\tfrac{N}{2} < |n| \\leq N \\big\\}.\\] \nAlso, given a dyadic number $L \\geq 1$, we define $Q_L$ to be the \nmodulation projector onto the space-time frequencies \n\\[\\mathfrak{S}_L :=\\big \\{ (\\tau, n)\\in \\mathbb{R}\\times\\mathbb{Z}^2: \\tfrac{L}{2} < \\big| \\tau + |n|^2 \\big| \\leq L \\big\\}.\\]\n For brevity, we also set  $P_{N,L} = P_N Q_L$. \n\n\nIn what follows, $N$ and $L$ (possibly with subscripts) always denote dyadic numbers $\\geq 1$. We write $\\overline{N}_{ij\\cdots} := \\max(N_i, N_j, \\dots )$ and $\\underline{N}_{ij\\cdots} := \\min ( N_i, N_j, \\dots )$. We also write $N_{\\text{max}} := \\overline{N}_{012}$, $N_{\\text{min}} := \\underline{N}_{012}$, $L_{\\text{max}} := \\overline{L}_{012}$, $L_{\\text{min}} := \\underline{L}_{012}$, and $L_{\\text{med}} := L_0 L_1 L_2 / L_{\\text{min}} L_{\\text{max}}$.\n\nWe use $A \\lesssim B$ to denote $A \\leq CB$ for some constant $C > 0$, and we write $A \\sim B$ to denote $A \\lesssim B$ and $B \\lesssim A$. We also write $A \\ll B$ if $A \\leq cB$ for some sufficiently small $c > 0$.\nWe may use subscripts to denote dependence on external parameters; for example,\n $A\\lesssim_{p, q} B$ means $A\\le C(p, q) B$,\n where the constant $C(p, q)$ depends on parameters $p$ and $q$.\n In addition, we use $a+$ (and $a-$, respectively) to denote $a+\\varepsilon$ (and $a-\\varepsilon$, respectively)\nfor arbitrarily small $\\varepsilon > 0$.\n\n\n\\subsection{Preliminary lemmas}\nIn this subsection, we recall some useful lemmas. \nWe first recall the following bilinear  Strichartz estimate. For the proof, see Lemma 2.5 in~\\cite{Kish13}.\n\\begin{lemma}\\label{LEM:bilin}\nLet $N_j, L_j \\geq 1$,  $j = 0,1,2$,  be dyadic numbers. Suppose that $u_1, u_2 \\in L^2(\\mathbb{R} \\times \\mathbb{T}^2)$ satisfy\n\\[ \\textup{supp } \\widehat{u_1} \\subset \\mathfrak{P}_{N_1} \\cap \\mathfrak{S}_{L_1}\n\\quad\\text{and}\\quad \\textup{supp } \\widehat{u_2} \\subset \\mathfrak{P}_{N_2} \\cap \\mathfrak{S}_{L_2}.  \\]\nThen, we have\n\\[ \\|  \\mathcal{F}_{t, x}(u_1 \\overline{u_2}) \\|_{L_\\tau^2 \\ell_n^2(\\mathfrak{P}_{N_0})} \n\\lesssim \\underline{L}_{12}^{\\frac 12} \\bigg( \\frac{\\overline{L}_{12}}{N_0} + 1 \\bigg)^{\\frac 12} N_{\\textup{min}}^{\\frac 12} \\| u_1 \\|_{L_{t,x}^2} \\| u_2 \\|_{L_{t,x}^2}. \\]\n\\end{lemma}\n\nWe also recall the following counting lemma. For the proof, see Lemma 2.9\\,(ii) in \\cite{Kish13}, which was stated in general dimension $d \\geq 2$ but we only need the $d = 2$ case.\n\n\\begin{lemma}\\label{LEM:count}\nLet $N \\gg 1$, $N^{-1} \\leq \\mu, \\nu \\ll N$, $M \\geq 0$, and set\n\\[ D := \\{ \\xi = (\\xi_1, \\xi_2) \\in \\mathbb{R}^2: N \\leq |\\xi| \\leq N + \\mu, M \\leq \\xi_1 \\leq M + \\nu \\}. \\]\nLet $\\mathcal{R}$ be an arbitrary rotation operator on $\\mathbb{R}^2$. \nMoreover, with $e_1 = ( 1,0) \\in \\mathbb{R}^2$, set \n\\[ K := \\Big\\{ \\xi \\in \\mathbb{R}^2: \\frac{\\alpha}{2} \\leq \\angle(\\xi, e_1) \\leq 2\\alpha \\Big\\},\\]\n\n\n\\noindent\nwhere $\\angle(\\xi, e_1) $ denotes the angle between $\\xi$ and $e_1$.\nSuppose that \n\\[ \n\\bigg( \\frac{\\mu + \\min\\{\\nu, 1\\}}{N} \\bigg)^{\\frac 12 } \\ll \\alpha \\leq \\frac{\\pi}{4}. \\]\nThen, we have\n\\[ |\\mathbb{Z}^2 \\cap \\mathcal{R}(D \\cap K)| \\lesssim \\max\\{ \\nu, 1 \\} \\big(\\alpha^{-1}(\\mu + \\min\\{ \\nu, 1 \\}) + 1\\big). \\]\n\\end{lemma}\n\n\n\\subsection{Fourier restriction norm method}\nIn studying well-posedness of NLS on $\\mathbb{T}^d$, Bourgain \\cite{B93}\nintroduced the Fourier restriction norm method (see also \\cite{KM93}), \nutilizing the following $X^{s, b}$-spaces.\n Given $s, b \\in \\mathbb{R}$, \n we define the space $X^{s,b}(\\mathbb{R} \\times \\mathbb{T}^2)$ \n to be  the completion of functions that are $C^\\infty$ in space and Schwartz in time with respect to the following norm:\n\\begin{align}\n \\| u \\|_{X^{s,b}(\\mathbb{R} \\times \\mathbb{T}^2)} := \\big\\| \\jb{n}^s \\jb{\\tau + |n|^2}^b \\widehat u(\\tau, n) \\big\\|_{ L_\\tau^2 \\ell_n^2(\\mathbb{R} \\times \\mathbb{Z}^2) }. \n\\label{Xsb}\n\\end{align}\n\n\\noindent\nFor $T > 0$, we define the space $X_T^{s,b}$ to be the restriction of \nthe $X^{s, b}$-space onto the time interval \n$[-T, T]$ via the norm:\n\\begin{equation}\n\\| u \\|_{X_T^{s,b}} := \\inf \\big\\{ \\| v \\|_{X^{s,b}}: v|_{[-T, T]} = u \\big\\}.\n\\label{XT}\n\\end{equation}\n\n\n\\noindent\nNote that $X^{s, b}_T$ is complete.\nGiven any $s \\in \\mathbb{R}$ and $b > \\frac 12$, we have $X_T^{s,b} \\subset C([-T, T]; H^s(\\mathbb{T}^2))$.\n\nWe now recall the following linear estimates. See \\cite{B93, GTV, Tao}.\n\n\\begin{lemma}\\label{LEM:Xlin}\nLet $s \\in \\mathbb{R}$ and $0 < T \\leq 1$.\n\n\\smallskip \\noindent\n\\textup{(i)} For any $b \\in \\mathbb{R}$, we have\n\\[ \\| e^{it\\Delta} \\phi \\|_{X_T^{s,b}} \\lesssim_b \\| \\phi \\|_{H^s}. \\]\n\n\\smallskip \\noindent\n\\textup{(ii)} Let $-\\frac 12 < b' \\leq 0 \\leq b \\leq b'+1$. Then, we have\n\\[ \\bigg\\| \\int_0^t e^{i(t-t')\\Delta} F(t') dt' \\bigg\\|_{X_T^{s,b}} \\lesssim_{b,b'} T^{1-b+b'} \\| F \\|_{X_T^{s,b'}}. \\]\n\\end{lemma}\n\nLastly,  we record the following $L^4$-Strichartz estimate on $\\mathbb{T}^2$. \n\n\\begin{lemma}\\label{LEM:L4}\nLet $s > 0$ and $b > \\frac 12$. Then, we have\n\\begin{align} \\| u \\|_{L_{t,x}^4([0,1] \\times \\mathbb{T}^2)} \\lesssim_{s, b} \\| u \\|_{X^{s, b}}. \n\\label{L42}\n\\end{align}\n\\end{lemma}\n\nFor the proof, see \\cite{B93, B95}.  We point out that the $L^4$-Strichartz estimate \n\\eqref{L42} does not hold when $s=0$.  See \\cite{TTz}.\nAs mentioned in the introduction, the $L^4$-Strichartz estimate \\eqref{Str1}\nwith an $\\varepsilon$-loss of derivative\nholds on a general flat torus\n$\\mathbb{T}^2_{\\pmb{\\alpha}} = (\\mathbb{R}/\\alpha_1 \\mathbb{Z}) \\times (\\mathbb{R}/\\alpha_2 \\mathbb{Z}) $\nfor any ratio $\\pmb{\\alpha} = (\\alpha_1, \\alpha_2)$ with  $\\alpha_1, \\alpha_2 > 0$;\nsee  Theorem~2.4 in \\cite{BD15}.\nThen, the estimate \\eqref{L42}\non a general flat torus $\\mathbb{T}^2_{\\pmb{\\alpha}}$ follows from a standard transference principle (see Lemma 2.9 in \\cite{Tao}).\n\n\n\\section{Proof of Proposition \\ref{PROP:bilin}}\n\\label{SEC:bilin}\n\nIn this section, we  present the proof of  Proposition~\\ref{PROP:bilin}.\nBy the triangle inequality $\\jb{n}^s \\lesssim \\jb{n_1}^s \\jb{n_2}^s$\nunder $n = n_1 + n_2$ for $s \\ge 0$, \n our goal is to prove  the following bilinear estimate (with $s = 0$):\n\\begin{align}\n \\| u \\overline{v} \\|_{X_T^{0,-\\frac 12 + \\delta_1}} \\lesssim \\| u \\|_{X_T^{0, \\frac 12 + \\delta_2}} \n \\| v \\|_{X_T^{0, \\frac 12 + \\delta_2}}, \n\\label{bilin1a}\n\\end{align}\n\n\n\\noindent\nwhere $0 < T \\leq 1$ and $\\delta_1, \\delta_2 > 0$ are sufficiently small, \nsatisfying  $\\delta_2 < \\delta_1 < 2\\delta_2$. \nIn view of the definitions \\eqref{Xsb} and \\eqref{XT} of the $X^{s, b}$-spaces, \nthe bilinear estimate \\eqref{bilin1a} follows\nonce we prove\n\\begin{equation}\n\\begin{split}\n\\bigg\\| & \\sum_{\\substack{n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} \n\\intt_{\\tau = \\tau_1 - \\tau_2} \\frac{\\widehat u(\\tau_1, n_1) \\overline{\\widehat v(\\tau_2, n_2)}}{\\jb{\\tau_1 + |n_1|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau_2 + |n_2|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau + |n|^2}^{\\frac 12 - \\delta_1}} d\\tau_1 \\bigg\\|_{L_\\tau^2 \\ell_n^2} \\\\\n& \\lesssim \\| u \\|_{L_{t,x}^2} \\| v \\|_{L_{t,x}^2}.\n\\end{split}\n\\label{bilin2}\n\\end{equation}\n\n\\noindent\nBy duality, the estimate \\eqref{bilin2} \nfollows once we prove the following estimate:\\\n\\begin{equation}\n\\begin{split}\n\\bigg| &\\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} \n\\iintt_{\\tau = \\tau_1 - \\tau_2} \\frac{\\widehat u (\\tau_1, n_1) \\overline{\\widehat v (\\tau_2, n_2) }\\overline{\\widehat w (\\tau, n)}}{\\jb{\\tau_1 + |n_1|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau_2 + |n_2|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau + |n|^2}^{\\frac 12 - \\delta_1}}\nd\\tau_1 d\\tau \\bigg| \\\\\n& \\lesssim \\| u \\|_{L_{t,x}^2} \\| v \\|_{L_{t,x}^2} \\| w \\|_{L_{t,x}^2}.\n\\end{split}\n\\label{bilin3}\n\\end{equation}\n\n\\noindent\nWe first note that if  $n = 0$, $n_1 = 0$, or $n_2 = 0$, \nthen the estimate \\eqref{bilin3} follows easily from  the Cauchy-Schwarz inequality,\nprovided that $\\delta_1 > 0$ is sufficiently small.\n Hence,  for the remaining of this section, we assume that $n \\neq 0$, $n_1 \\neq 0$, and $n_2 \\neq 0$.\n\nBefore proceeding further, we recall\nthe following key algebraic relation:\n\\begin{equation}\n(\\tau + |n|^2) - (\\tau_1 + |n_1|^2) + (\\tau_2 + |n_2|^2) = -2 n \\cdot n_2, \n\\label{phase}\n\\end{equation}\n\n\\noindent\nwhere  $n = n_1 - n_2$ and $\\tau = \\tau_1 - \\tau_2$.\nThe main difficulty in proving \\eqref{bilin3} (namely, the bilinear estimate \\eqref{bilin1} with $s = 0$)\ncomes from the failure of \nthe $L^4$-Strichartz estimate (without a derivative loss).\nIn order to overcome this difficulty, we separately estimate the contributions coming\nfrom \n(i) non-resonant case: $|n \\cdot n_2| \\gtrsim |n|^\\varepsilon |n_2|^\\varepsilon$\nand (ii) nearly resonant case: $|n \\cdot n_2| \\ll |n|^\\varepsilon |n_2|^\\varepsilon$\nfor some small $ \\varepsilon > 0$.\nIn the non-resonant case, \nwe can use the multilinear dispersion\nto make up for the derivative loss in the $L^4$-Strichartz estimate (Lemma~\\ref{LEM:L4}).\nIn the nearly resonant case, by noting that the frequencies $n$ and $n_2$\nare almost perpendicular, \nwe make use of this angular restriction to prove\na multilinear estimate without a derivative loss\nin the spirit of \\cite{Tao4, CKSTT08}.\n\n\n\n\n\n\n\\subsection{Non-resonant interaction}\nIn this subsection, we consider the non-resonant case:\n\\begin{align}\n|n \\cdot n_2| \\gtrsim |n|^\\varepsilon |n_2|^\\varepsilon\n\\label{nr1}\n\\end{align}\n\n\\noindent\n for some small $\\varepsilon > 0$ sufficiently small.\nFrom \\eqref{phase} and \\eqref{nr1}, we have\n\\begin{align}\n\\begin{split}\n\\MAX :\\! & =  \\max \\big( \\jb{\\tau + |n|^2}, \\jb{\\tau_1 + |n_1|^2}, \\jb{\\tau_2 + |n_2|^2} \\big)\\\\\n& \\gtrsim \\jb{n \\cdot n_2}\n\\gtrsim \\jb{n}^\\varepsilon \\jb{n_2}^\\varepsilon.\n\\end{split}\n\\label{nr2}\n\\end{align}\nWe then consider the following three cases.\nIn the following, we set   $0 < \\delta_1 < \\frac 12 $.\n\n\n\\medskip \\noindent\n$\\bullet$ \\textbf{Case 1:} $\\text{MAX} = \\jb{\\tau + |n|^2}$. \n\nIn this case, we directly prove \\eqref{bilin2}.\nThen, with  $\\varepsilon_1 = \\frac \\varepsilon 2(\\frac 12 - \\delta_1)> 0$, \nit follows from \\eqref{nr2} that \n\\begin{equation}\n\\jb{n_1}^{\\varepsilon_1} \\jb{n_2}^{\\varepsilon_1} = \\jb{n+n_2}^{\\varepsilon_1} \\jb{n_2}^{\\varepsilon_1} \\lesssim \\jb{n}^{2\\varepsilon_1} \\jb{n_2}^{2\\varepsilon_1} \\lesssim \\jb{\\tau + |n|^2}^{\\frac 12 - \\delta_1}.\n\\label{tn0}\n\\end{equation}\n\n\\noindent\nLet $\\widehat U(\\tau_1, n_1) = \\jb{n_1}^{-\\varepsilon_1} \\jb{\\tau_1 + |n_1|^2}^{-\\frac 12 - \\delta_2} \\widehat u(\\tau_1, n_1)$\nand\n$\\widehat V(\\tau_2, n_2) = \\jb{n_2}^{-\\varepsilon_1} \\jb{\\tau_2 + |n_2|^2}^{-\\frac 12 - \\delta_2} \\widehat v(\\tau_2, n_2)$.\nThen, by \\eqref{tn0}, Plancherel's theorem, H\\\"older's inequality, and the $L^4$-Strichartz estimate (Lemma \\ref{LEM:L4}), we have\n\\begin{align*}\n\\text{LHS of \\eqref{bilin2}} \n&\\lesssim \\| U \\overline V\\|_{L^2_{t, x}}\n\\leq \\| U \\|_{L^4_{t, x}}\\|  V\\|_{L^4_{t, x}}\n\\lesssim \\|U\\|_{X^{\\varepsilon_1, \\frac 12 + \\delta_2}}\n\\|V\\|_{X^{\\varepsilon_1, \\frac 12 + \\delta_2}}\\\\\n&= \\| u \\|_{L_{t,x}^2} \\| v \\|_{L_{t,x}^2}\n\\end{align*}\n\n\\noindent\nfor any $\\delta_2 > 0$.\n\n\\medskip \\noindent\n$\\bullet$ \\textbf{Case 2:} $\\text{MAX} = \\jb{\\tau_1 + |n_1|^2}$. \n\nIn this case, we prove \\eqref{bilin3}. \nWith  $\\varepsilon_2 = \\varepsilon(\\frac 12 - \\delta_1)> 0$, \nit follows from \\eqref{nr2} that \n\\begin{equation}\n\\jb{n}^{\\varepsilon_2} \\jb{n_2}^{\\varepsilon_2} \\lesssim \\jb{\\tau_1 + |n_1|^2}^{\\frac 12 - \\delta_1}.\n\\label{tn1}\n\\end{equation}\n\n\\noindent\nThen, letting\n$\\widehat V(\\tau_2, n_2) = \\jb{n_2}^{-\\varepsilon_2} \\jb{\\tau_2 + |n_2|^2}^{-\\frac 12 - \\delta_2} \\widehat v(\\tau_2, n_2)$\nand \n$\\widehat W(\\tau, n) = \\jb{n}^{-\\varepsilon_2} \\jb{\\tau + |n|^2}^{-\\frac 12 - \\delta_2} \\widehat w(\\tau, n)$, \nit follows from \n the Cauchy-Schwarz inequality in $\\tau_1$ and $n_1$, \\eqref{tn1}, Plancherel's theorem, H\\\"older's inequality, and the $L^4$-Strichartz estimate (Lemma \\ref{LEM:L4}) that \n\\begin{align*}\n \\text{LHS of \\eqref{bilin3}} \n&\\lesssim \\|u\\|_{L^2_{t, x}} \n\\| V \\|_{L^4_{t, x}}\\|W\\|_{L^4_{t, x}}\\\\\n&\\lesssim \\|u\\|_{L^2_{t, x}} \n\\| V \\|_{X^{\\varepsilon_2, \\frac 12 +\\delta_2}} \\|W\\|_{X^{\\varepsilon_2, \\frac 12 +\\delta_2}} \\\\\n&= \\| u \\|_{L_{t,x}^2} \\| v \\|_{L_{t,x}^2} \\| w \\|_{L_{t,x}^2}\n\\end{align*}\n\n\\noindent\nfor any $\\delta_2 > 0$.\n\n\\medskip\n\\noindent\n$\\bullet$ \\textbf{Case 3:} $\\text{MAX} = \\jb{\\tau_2 + |n_2|^2}$.\n\nThis case follows from proceeding as in Case 2\nand thus we omit details.\n\n\n\n\n\\subsection{Resonant interaction}\n\\label{SUBSEC:res}\nWe now consider the resonant case $|n \\cdot n_2| \\ll |n|^\\varepsilon |n_2|^\\varepsilon$.\nWith $\\theta = \\theta (n, n_2) = \\frac{1}{|n|^{1-\\varepsilon} |n_2|^{1-\\varepsilon}}$, we rewrite this condition as\n\\begin{align*}\n|n \\cdot n_2| \\ll \\theta |n| |n_2|.\n\\end{align*}\n\n\\noindent\nIn the following, we divide the argument  into three subcases, depending on the sizes\nof $n$ and~$n_2$.\n\n\\medskip \\noindent\n$\\bullet$ \\textbf{Case 1:} $|n|^{\\frac 12} \\lesssim |n_2| \\lesssim |n|^2$.\n\nIn this case, we follow the idea from the proof of Proposition 10.1 in \\cite{Tao4} (see also \\cite{CKSTT08}). \n\n\n\\smallskip \\noindent\n$\\circ$ \\textbf{Subcase 1.a:} $|n| \\sim |n_2|$.\n\nBy applying the dyadic decompositions to the frequencies $n$ and $n_2$, we have\n\\begin{equation}\n\\begin{split}\n\\text{LHS of } \\eqref{bilin3} \n&\\lesssim \\sum_{\\substack{N \\sim N_2 \\geq 1\\\\\\text{dyadic}} }\n\\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} \\iintt_{\\tau = \\tau_1 - \\tau_2} \n\\mathbf 1_{|\\cos \\angle(n, n_2)| \\ll \\theta} \\\\\n&\\quad \\times \\frac{\\widehat u (\\tau_1, n_1) \\overline{\\widehat{P_{N_2}v} (\\tau_2, n_2)} \\overline{\\widehat{P_N w} (\\tau, n)}}{\\jb{\\tau_1 + |n_1|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau_2 + |n_2|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau + |n|^2}^{\\frac 12 - \\delta_1}} d\\tau_1 d\\tau \\bigg|.\n\\end{split}\n\\label{bilin_a1}\n\\end{equation}\n\n\\noindent\nNote that when $|n| \\sim |n_2| \\sim N$, we have $\\theta = \\frac{1}{|n|^{1 - \\varepsilon} |n_2|^{1 - \\varepsilon}} \\sim N^{-2 + 2 \\varepsilon}$. We now restrict $n$ and $n_2$ to the following angular sectors:\n\\begin{align}\n\\begin{split}\nA_\\ell : \\!& = \\big\\{ n \\in \\mathbb{Z}^2: |n| \\sim N, \\,  \\arg(n) = \\ell N^{-2 + 2 \\varepsilon} + O(N^{-2 + 2 \\varepsilon}) \\big\\}, \\\\\nA_{\\ell_2} :\\!&= \\big\\{ n_2 \\in \\mathbb{Z}^2: |n_2| \\sim N_2, \\, \\arg(n_2) = \\ell_2 N^{-2 + 2 \\varepsilon} + O(N^{-2 + 2 \\varepsilon}) \\big\\},\n\\end{split}\n\\label{A0}\n\\end{align}\n\n\\noindent\nwhere $\\ell, \\ell_2 \\in I_N: = [1, 2\\pi N^{2 - \\varepsilon}] \\cap \\mathbb{Z}$.\nSince $|\\cos \\angle (n, n_2)| \\ll \\theta \\sim N^{-2 + 2\\varepsilon}$, we have $|\\arg(n) - \\arg(n_2)| = \\frac{\\pi}{2} + O(N^{-2 + 2\\varepsilon})$ or $|\\arg(n) - \\arg(n_2)| = \\frac{3\\pi}{2} + O(N^{-2 + 2\\varepsilon})$. This means that for each fixed $\\ell$, there exists a set  $L_2(\\l)$ of size $O(1)$ such that \n$\\mathbf 1_{A_\\ell} (n)\\cdot \\mathbf 1_{A_{\\ell_2}}(n_2)= 0$\nunless $\\l_2 \\in L_2(\\l)$.\n\n\nContinuing with the right-hand side of \\eqref{bilin_a1}, \nwe insert the angular restrictions \\eqref{A0}\nand set \n\\[ \\widehat v_{N_2, \\l_2}(\\tau_2, n_2) =  \\mathbf 1_{A_{\\ell_2}}(n_2)  \\cdot \\widehat{P_{N_2}v} (\\tau_2, n_2)\n\\quad \\text{and}\\quad\n\\widehat w_{N, \\l} (\\tau, n) = \\mathbf 1_{A_\\ell}(n) \\cdot \\widehat{P_N w} (\\tau, n). \\]\n\n\\noindent\n Then, by \napplying  the Cauchy-Schwarz inequality in $\\tau_1$ and $n_1$, \nthen  in $\\tau$ and $n$, and H\\\"older's inequality in $\\tau_1$ and $n_1$, \nwe obtain\n\\begin{equation}\n\\begin{split}\n\\text{LHS of } \\eqref{bilin3}\n &\\lesssim \\sum_{\\substack{N \\sim N_2 \\ge 1\\\\\\text{dyadic}}} \n \\sum_{\\ell  \\in I_N} \n \\sum_{ \\ell_2 \\in L_2(\\ell)} \n \\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} \n \\iintt_{\\tau = \\tau_1 - \\tau_2}  \\mathbf 1_{|\\cos \\angle (n, n_2)| \\ll N^{-2 + 2\\varepsilon}} \\\\\n&\\quad \\times \\frac{ \\widehat u (\\tau_1, n_1) \\overline{\\widehat v_{N_2, \\l_2} (\\tau_2, n_2) }\n  \\overline{\\widehat  w_{N, \\l} (\\tau, n) }}\n  {\\jb{\\tau_1 + |n_1|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau_2 + |n_2|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau + |n|^2}^{\\frac 12 - \\delta_1}} d\\tau_1 d\\tau \\bigg| \\\\\n&\\leq  \\sum_{\\substack{N \\sim N_2 \\ge 1\\\\\\text{dyadic}}} \n \\sum_{\\ell  \\in I_N} \n \\sum_{ \\ell_2 \\in L_2(\\ell)} \n  \\| \\widehat u \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\bigg\\| \\sum_{n\\in \\mathbb{Z}^2} \\int \\mathbf 1_{|\\cos \\angle (n, n_1 - n)| \\ll N^{-2 + 2\\varepsilon}} \\\\\n&\\quad \\times \\frac{ \\widehat v_{N_2, \\l_2} (\\tau_1 - \\tau , n_1- n) \n  \\widehat  w_{N, \\l} (\\tau, n) }{ \\jb{\\tau_1 - \\tau + |n_1 - n|^2}^{\\frac 12 + \\delta_2} } d\\tau \\bigg\\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\\\\n&\\leq \\| u \\|_{L_{t, x}^2} \\sum_{\\substack{N \\sim N_2 \\ge 1\\\\\\text{dyadic}}} \n \\sum_{\\ell  \\in I_N} \n \\sum_{ \\ell_2 \\in L_2(\\ell)} \n  \\| \\widehat v_{N_2, \\l_2} \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| \\widehat w_{N, \\l}\\|_{L_\\tau^2 \\ell_n^2} \\\\\n&\\quad \\times \\sup_{\\tau_1, n_1} \\bigg( \\sum_n \\int \\frac{ \\mathbf 1_{|\\cos \\angle (n, n_1 - n)| \\ll N^{-2 + 2\\varepsilon}} \\mathbf 1_{A_{\\ell_2}}(n_1 - n)  \\cdot \\mathbf 1_{A_\\ell}(n)}{\\jb{\\tau_1 - \\tau + |n_1 - n|^2}^{1 + 2 \\delta_2}} d\\tau \\bigg)^{\\frac 12} \\\\\n&\\lesssim \\| u \\|_{L_{t, x}^2 }  \\sum_{\\substack{N \\sim N_2 \\ge 1\\\\\\text{dyadic}}} \n\\sum_{\\ell  \\in I_N} \n \\sum_{ \\ell_2 \\in L_2(\\ell)} \n  \\| \\widehat v_{N_2, \\l_2} \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| \\widehat w_{N, \\l}\\|_{L_\\tau^2 \\ell_n^2}\\\\\n& \\quad \\times \\sup_{n_1, \\l, \\l_2} |\\mathcal{A}_{N, \\l, \\l_2}(n_1)|^\\frac 12 , \n\\end{split}\n\\label{bilin_a2}\n\\end{equation}\n\n\\noindent\nwhere $\\mathcal{A}_{N, \\l, \\l_2}(n_1)$ is defined by \n\\[  \\mathcal{A}_{N, \\l, \\l_2}(n_1) = \\big\\{ n \\in \\mathbb{Z}^2: |\\cos \\angle (n, n_1 - n)| \\ll N^{-2 + 2\\varepsilon}, \\, n \\in A_\\ell,  \\, n_1 - n \\in A_{\\ell_2}\\big \\}.\\]\n\nIn view of \\eqref{A0}, \nfor  fixed $n_1 \\in \\mathbb{Z}^2$ and fixed $\\ell, \\ell_2\\in I_N$, \nwe note that any $n\\in \\mathcal{A}_{N, \\l. \\l_2}$\nis contained in  a rectangle with sides of length $\\sim$\n\\[ |n| N^{-2 + 2\\varepsilon} \\sim N^{-1 + 2\\varepsilon} \\quad \\text{and} \\quad |n_1 - n| N^{-2 + 2\\varepsilon} \\sim N^{-1 + 2\\varepsilon}. \\] \n\n\n\\noindent\nHence, for  $0 < \\varepsilon < \\frac 12$,\nwe have \n\\begin{align}\n\\sup_{n_1, \\l,  \\l_2} |\\mathcal{A}_{N, \\l, \\l_2}(n_1)|\\lesssim 1.\n\\label{A2}\n\\end{align}\n\n\\noindent\nTherefore, \nby estimating  \\eqref{bilin_a2} \nwith \\eqref{A2}, \n the Cauchy-Schwarz inequality in $\\ell$, and then in $N \\sim N_2$ we obtain the desired inequality \\eqref{bilin3}.\n\n\n\n\n\\smallskip \\noindent\n$\\circ$ \\textbf{Subcase 1.b:} $|n_2| \\ll |n| \\lesssim |n_2|^2$.\n\n Since $|n| \\gg |n_2|$ and $n_1 = n + n_2$, we have $|n_1| \\sim |n|$. \nNote that when  $|n| \\sim N \\lesssim |n_2|^2$, we have $\\theta = \\frac{1}{|n|^{1 - \\varepsilon} |n_2|^{1 - \\varepsilon}} \\lesssim N^{-\\frac 32 + \\frac 32 \\varepsilon}$. We proceed as in Subcase 1.a by restricting $n$ and $n_2$ to the following angular sectors:\n\\begin{align}\n\\begin{split}\nB_\\l : \\! & = \\big\\{ n \\in \\mathbb{Z}^2: |n| \\sim N, \\arg(n) = \\ell N^{-\\frac 32 + \\frac 32 \\varepsilon} + O(N^{-\\frac 32 + \\frac 32 \\varepsilon}) \\big\\}, \\\\\nB_{\\l_2} :\\! & = \\big\\{ n_2 \\in \\mathbb{Z}^2: N^{\\frac 12} \\lesssim |n_2| \\ll N, \\arg(n_2) = \\ell_2 N^{-\\frac 32 + \\frac 32 \\varepsilon} + O(N^{-\\frac 32 + \\frac 32 \\varepsilon}) \\big\\},\n\\end{split}\n\\label{B0}\n\\end{align}\n\n\\noindent\nwhere $\\ell, \\ell_2 \\in I_N': = [1, 2\\pi N^{\\frac 32 - \\frac 32\\varepsilon}] \\cap \\mathbb{Z}$.\nUnder $|\\cos \\angle (n, n_2)| \\ll N^{-\\frac 32 + \\frac 32 \\varepsilon}$, we have $|\\arg(n) - \\arg(n_2)| = \\frac{\\pi}{2} + O(N^{-\\frac 32 + \\frac 32 \\varepsilon})$ or $|\\arg(n) - \\arg(n_2)| = \\frac{3\\pi}{2} + O(N^{-\\frac 32 + \\frac 32 \\varepsilon})$. \nHence, \n for each fixed $\\ell$, there exists a set  $L_2'(\\l)$ of size $O(1)$ such that \n$\\mathbf 1_{B_\\ell} (n)\\cdot \\mathbf 1_{B_{\\ell_2}}(n_2)= 0$\nunless $\\l_2 \\in L_2'(\\l)$.\nWith a slight abuse of notation, set \n\\[ \\widehat v_{N, \\l_2}(\\tau_2, n_2) =  \\mathbf 1_{B_{\\ell_2}}(n_2)  \\cdot \\widehat{v} (\\tau_2, n_2)\n\\quad \\text{and}\\quad\n\\widehat w_{N, \\l} (\\tau, n) = \\mathbf 1_{B_\\ell}(n) \\cdot \\widehat{P_N w} (\\tau, n). \\]\n\n\\noindent\n Then, by proceeding as in \\eqref{bilin_a2}, i.e.~by\napplying  the Cauchy-Schwarz inequality in $\\tau_1$ and $n_1$, \nthen  in $\\tau$ and $n$, and H\\\"older's inequality in $\\tau_1$ and $n_1$, \nwe obtain\n\\begin{align*}\n\\text{LHS of } \\eqref{bilin3}\n &\\lesssim \\sum_{\\substack{N \\sim N_1\\ge 1\\\\\\text{dyadic}}}\n   \\sum_{\\ell  \\in I_N'} \n \\sum_{ \\ell_2 \\in L_2'(\\ell)} \n   \\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} \\iintt_{\\tau = \\tau_1 - \\tau_2} \\mathbf 1_{|\\cos \\angle (n, n_2)| \\ll N^{-\\frac 32 + \\frac 32 \\varepsilon}}  \\\\\n&\\quad \\times \\frac{\\widehat{P_{N_1} u} (\\tau_1, n_1) \\overline{\\widehat v_{N, \\l_2} (\\tau_2, n_2)}\n \\overline{\\widehat w_{N, \\l} (\\tau, n)}}{\\jb{\\tau_1 + |n_1|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau_2 + |n_2|^2}^{\\frac 12 + \\delta_2} \\jb{\\tau + |n|^2}^{\\frac 12 - \\delta_1}} d\\tau_1 d\\tau \\bigg| \\\\\n&\\leq  \\sum_{\\substack{N \\sim N_1  \\ge 1\\\\\\text{dyadic}}}\n   \\sum_{\\ell  \\in I_N'} \n \\sum_{ \\ell_2 \\in L_2'(\\ell)} \n\\big\\| \\widehat{P_{N_1} u} \\big\\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\bigg\\| \\sum_{n \\in  \\mathbb{Z}^2} \\int \\mathbf 1_{|\\cos \\angle (n, n_1 - n)| \\ll N^{-\\frac 32 + \\frac 32 \\varepsilon}}  \\\\\n&\\quad \\times \\frac{\\widehat v_{N, \\l_2}  (\\tau_1 - \\tau, n_1 - n)\\widehat w_{N, \\l}(\\tau, n) }{\\jb{\\tau_1 - \\tau + |n_1 - n|^2}^{\\frac 12 + \\delta_2}} d\\tau \\bigg\\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\\\\n&\\leq  \\sum_{\\substack{N \\sim N_1 \\ge 1\\\\\\text{dyadic}}}\n    \\sum_{\\ell  \\in I_N'} \n \\sum_{ \\ell_2 \\in L_2'(\\ell)} \n  \\| \\widehat{P_{N_1} u} \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| \\widehat v \\_{N, \\l_2}  \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \n  \\| w_{N, \\l} \\|_{L_\\tau^2 \\ell_n^2} \\\\\n&\\quad \\times \\sup_{\\tau_1, n_1} \\bigg( \\sum_n \\int \\frac{\\mathbf 1_{|\\cos \\angle (n, n_1 - n)| \\ll N^{-\\frac 32 + \\frac 32 \\varepsilon}} \\mathbf 1_{B_{\\ell_2}} (n_1 - n)\\cdot  \\mathbf 1_{B_\\ell} (n)}{\\jb{\\tau_1 - \\tau + |n_1 - n|^2}^{1 + 2 \\delta_2}} d\\tau \\bigg)^{1/2} \\\\\n&\\lesssim  \\sum_{\\substack{N \\sim N_1 \\ge 1\\\\\\text{dyadic}}}\n    \\sum_{\\ell  \\in I_N'} \n \\sum_{ \\ell_2 \\in L_2'(\\ell)} \n  \\| \\widehat{P_{N_1} u} \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| \\widehat v_{N, \\l_2} \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \n  \\| \\widehat w_{N, \\l_2} \\|_{L_\\tau^2 \\ell_n^2} \\\\\n& \\quad \\times \\sup_{n_1} |\\mathcal{B}_{N, \\l, \\l_2}(n_1)|^\\frac 12, \n\\end{align*}\n\n\n\\noindent\nwhere $\\mathcal{B}_{N, \\l, \\l_2}(n_1)$ is defined by \n\\[ \\mathcal{B}_{N, \\l, \\l_2}(n_1)\n = \\big \\{ n \\in \\mathbb{Z}^2: |\\cos \\angle (n, n_1 - n)| \\ll N^{-\\frac 32 + \\frac 32 \\varepsilon},, n \\in B_\\ell,,  n_1 - n \\in B_{\\ell_2} \\big\\}. \\]\n\n\\noindent\nThen, the desired bound \\eqref{bilin3} follows from \n the Cauchy-Schwarz inequality in $\\ell$ and then in  the Cauchy-Schwarz inequality in $N \\sim N_1$,\n once we prove \n\\begin{align}\n\\sup_{n_1, \\l,  \\l_2} |\\mathcal{B}_{N, \\l, \\l_2}(n_1)|\\lesssim 1.\n\\label{B2}\n\\end{align}\n\n\n\nIn view of \\eqref{B0}, \nfor  fixed $n_1 \\in \\mathbb{Z}^2$ and fixed $\\ell, \\ell_2\\in I_N'$, \nwe note that any $n\\in \\mathcal{B}_{N, \\l. \\l_2}$\nis contained in  a rectangle with sides of length $\\sim$\n\\[ |n| N^{-\\frac 32 + \\frac 32 \\varepsilon} \\sim N^{-\\frac 12 + \\frac 32 \\varepsilon} \\quad \\text{and} \\quad |n_1 - n| N^{-\\frac 32 + \\frac 32 \\varepsilon} \\ll N^{-\\frac 12 + \\frac 32 \\varepsilon}. \\] \n\n\\noindent\nTherefore, as long as $0 < \\varepsilon < \\frac 13$, there are at most $O(1)$ many \n$n'$s in $ \\mathcal{B}_{N, \\l. \\l_2}$, yielding \\eqref{B2}.\n\n\n\\smallskip \\noindent\n$\\circ$ \\textbf{Subcase 1.c:} $|n| \\ll |n_2|\\lesssim |n|^2$.\n\\\\ \\indent\nThis subcase follows from Subcase 1.b by switching the roles of $(\\tau, n)$ and $(\\tau_2, n_2)$\nand thus we omit details.\n\n\n\n\n\n\n\n\n\\medskip \\noindent\n$\\bullet$ \\textbf{Case 2:} $|n|^2 \\ll |n_2|$.\n\nWe divide the argument into two subcases, depending on the sizes\nof  $N_2$ and the largest modulation $L_{\\textup{max}} = \\max (L_0, L_1, L_2)$. \n\n\\medskip \\noindent\n$\\circ$ \\textbf{Subcase 2.a:} $L_{\\textup{max}} \\gtrsim N_2$ (high-modulation case).\n\nIn this subcase, recalling the notations in Section \\ref{SEC:2}, we dyadically decompose\n the spatial frequencies and modulations of  $u$, $v$, and $w$ so that\n  $\\textup{supp } \\widehat {P_{N_1, L_1} u} \\subset \\mathfrak{P}_{N_1} \\cap \\mathfrak{S}_{L_1}$, \n   $\\textup{supp } \\widehat {P_{N_2, L_2} v } \\subset \\mathfrak{P}_{N_2} \\cap \\mathfrak{S}_{L_2}$, and $\\textup{supp } \\widehat{P_{N_0, L_0} w} \\subset \\mathfrak{P}_{N_0} \\cap \\mathfrak{S}_{L_0}$ for dyadic $N_j, L_j \\geq 1$, $j = 0,1,2$. Note that we have $N_0^2 \\ll N_2 \\sim N_1$. Our main goal is to show the following lemma.\n\n\n\\begin{lemma}\\label{LEM:high}\nLet $N_j, L_j \\geq 1$, $j = 0,1,2$, be dyadic numbers with $N_0^2 \\ll N_2 \\sim N_1$ and $L_{\\textup{max}} \\gtrsim N_2$. Suppose that $f, g, h \\in L^2(\\mathbb{R} \\times \\mathbb{Z}^2)$ are real-valued nonnegative functions such that \n\\begin{align}\n \\textup{supp } f \\subset \\mathfrak{P}_{N_1} \\cap \\mathfrak{S}_{L_1}, \\quad \\textup{supp } g \\subset \\mathfrak{P}_{N_2} \\cap \\mathfrak{S}_{L_2}, \\quad \\text{and} \\quad  \\textup{supp } h \\subset \\mathfrak{P}_{N_0} \\cap \\mathfrak{S}_{L_0}.  \n\\label{high1}\n \\end{align}\nThen, we have\n\\begin{equation}\n\\begin{split}\n\\bigg| \\sum_{\\substack{n, n_1 \\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} &\\iintt_{\\tau = \\tau_1 - \\tau_2} f(\\tau_1, n_1) g(\\tau_2, n_2) h(\\tau, n) d\\tau d\\tau_1 \\bigg| \\\\\n&\\lesssim L_1^{\\frac 12 +} L_2^{\\frac 12 +} L_0^{\\frac 14 +} N_2^{0-} \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2}.\n\\end{split}\n\\label{high}\n\\end{equation}\n\\end{lemma}\n\nWe first present the proof of \\eqref{bilin3} in this case by assuming Lemma \\ref{LEM:high}.\nUnder $N_0^2 \\ll N_2 \\sim N_1$ and $L_{\\textup{max}} \\gtrsim N_2$, \nby  applying Lemma \\ref{LEM:high}, we have \n\\begin{align*}\n\\text{LHS of \\eqref{bilin3}} &\\lesssim \\sum_{\\substack{ N_1, N_2, N_0\\ge 1\\\\\\text{dyadic}}}\n \\sum_{\\substack{L_1, L_2, L_0 \\ge 1\\\\\\text{dyadic}}} L_1^{-\\frac 12 - \\delta_2} L_2^{-\\frac 12 - \\delta_2} L_0^{-\\frac 12 + \\delta_1} \\\\\n&\\qquad \\times L_1^{\\frac 12 +} L_2^{\\frac 12 +} L_0^{\\frac 14 +} N_2^{0-} \\| P_{N_1, L_1} u \\|_{L_{t,x}^2} \\| P_{N_2, L_2} v \\|_{L_{t,x}^2} \\| P_{N_0, L_0} w \\|_{L_{t,x}^2} \\\\\n&\\leq \\sum_{\\substack{N_1, N_2, N_0\\ge 1\\\\\\text{dyadic}}} \\sum_{\\substack{ L_1, L_2, L_0\\ge 1\\\\\\text{dyadic}}} L_1^{0-} L_2^{0-} L_0^{0-} N_2^{0-} \\| u \\|_{L_{t,x}^2} \\| v \\|_{L_{t,x}^2} \\| w \\|_{L_{t,x}^2} \\\\\n&\\lesssim \\| u \\|_{L_{t,x}^2} \\| v \\|_{L_{t,x}^2} \\| w \\|_{L_{t,x}^2},\n\\end{align*}\nas desired.\n\nWe now present the proof of Lemma \\ref{LEM:high}.\n\n\\begin{proof}[Proof of Lemma \\ref{LEM:high}]\nBy the Cauchy-Schwarz inequality in $\\tau$ and $n$, Lemma \\ref{LEM:bilin}, and the condition $L_{\\textup{max}} \\gtrsim N_2 \\gg N_0^2$, we have\n\\begin{align*}\n\\text{LHS of \\eqref{high}} &\\lesssim \\| h \\|_{L_\\tau^2 \\ell_n^2} \\underline{L}_{12}^{\\frac 12} \n\\bigg( \\frac{\\overline{L}_{12}}{N_0} + 1 \\bigg)^{\\frac 12} N_0^{\\frac 12} \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\\\\n&\\lesssim \\big( L_1^{\\frac 12} L_2^{\\frac 12} + \\underline{L}_{12}^{\\frac 12} N_2^{\\frac 14} \\big) \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2} \\\\\n&\\lesssim L_1^{\\frac 12 +} L_2^{\\frac 12 +} L_0^{\\frac 14 +} N_2^{0-} \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2}, \n\\end{align*}\n\n\\noindent\nyielding \\eqref{high}.\n\\end{proof}\n\n\\medskip \\noindent\n$\\circ$  \\textbf{Subcase 2.b:} $L_{\\textup{max}} \\ll N_2$ (low-modulation case).\n\nOur goal in this subcase is to show the following lemma.\n\\begin{lemma}\\label{LEM:low}\nLet $N_1, N_2, L_0, L_1, L_2 \\geq 1$ be dyadic numbers with $N_1 \\sim N_2$ and $L_{\\textup{max}} \\ll N_2$. Suppose that $f, g, h \\in L^2(\\mathbb{R} \\times \\mathbb{Z}^2)$ are real-valued nonnegative functions such that\n\\begin{align*}\n&\\textup{supp } f \\subset \\mathfrak{P}_{N_1} \\cap \\mathfrak{S}_{L_1}, \\quad \\textup{supp } g \\subset \\mathfrak{P}_{N_2} \\cap \\mathfrak{S}_{L_2}, \n\\quad \\text{and}\\\\\n&\\textup{supp } h \\subset \\{n \\in \\mathbb{Z}^2: 1 \\leq |n|^2 \\ll N_2 \\} \\cap \\mathfrak{S}_{L_0}.\n\\end{align*}\nThen, we have \n\\begin{equation}\n\\begin{split}\n\\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} &\\iintt_{\\tau = \\tau_1 - \\tau_2} f(\\tau_1, n_1) g(\\tau_2, n_2) h(\\tau, n) \\cdot \\mathbf 1_{|\\cos \\angle(n, n_2)| \\ll \\theta} \\, d\\tau d\\tau_1 \\bigg| \\\\\n&\\lesssim L_{\\textup{med}}^{\\frac 38} L_{\\textup{max}}^{\\frac 38} \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2},\n\\end{split}\n\\label{low}\n\\end{equation}\nwhere $\\theta = \\frac{1}{|n|^{1-\\varepsilon} |n_2|^{1-\\varepsilon}} \\ll 1$.\n\\end{lemma}\n\nWe first assume Lemma \\ref{LEM:low} and prove \\eqref{bilin3}. \nRecall that  $|n|^2 \\ll |n_2|\\sim |n_1|$ in this case. \nThen,  by Lemma \\ref{LEM:low} (with $\\theta$ as above) and the Cauchy-Schwarz inequality in $N_1 \\sim N_2$, we have\n\\begin{align*}\n& \\text{LHS of } \\eqref{bilin3}\\\\ \n&\\hphantom{X} \\lesssim \\sum_{\\substack{N_1 \\sim N_2\\ge 1\\\\\\text{dyadic}}} \n\\sum_{\\substack{ L_1, L_2, L_0\\ge 1\\\\\\text{dyadic}}} L_1^{-\\frac 12 - \\delta_2} L_2^{-\\frac 12 - \\delta_2} L_0^{-\\frac 12 + \\delta_1} \n\\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 - n_2}} \\iintt_{\\tau = \\tau_1 - \\tau_2}\n \\mathbf 1_{|\\cos \\angle (n, n_2)| \\ll \\theta} \\\\\n&\\hphantom{XXX}\n\\times \\mathbf 1_{|n|^2 \\ll N_2} \\widehat{P_{N_1, L_1} u} (\\tau_1, n_1) \\overline{\\widehat{P_{N_2, L_2} v}} (\\tau_2, n_2) \\overline{\\widehat{Q_{L_0} w}} (\\tau, n) d\\tau_1 d\\tau \\bigg| \\\\\n&\\hphantom{X}\n\\lesssim  \\sum_{\\substack{N_1 \\sim N_2\\ge 1\\\\\\text{dyadic}}} \n\\sum_{\\substack{ L_1, L_, L_0\\ge 1\\\\\\text{dyadic}}}\n L_1^{0-} L_2^{0-} L_0^{0-} \\big\\| \\widehat{P_{N_1, L_1} u} \\big\\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\big\\| \\widehat{P_{N_2, L_2} v} \\big\\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\big\\| \\widehat{Q_{L_0} w} \\big\\|_{L_\\tau^2 \\ell_n^2} \\\\\n&\\hphantom{X}\n\\lesssim \\| \\widehat u \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| \\widehat v \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| \\widehat w \\|_{L_\\tau^2 \\ell_n^2},\n\\end{align*}\n\n\\noindent\nas desired.\n\n\n\nWe now present the proof of Lemma \\ref{LEM:low}, \nwhere we use the idea  from the proofs of  of Propositions 3.6 and 3.9 in \\cite{Kish13}.\n\n\n\\begin{proof}[Proof of Lemma \\ref{LEM:low}]\n\nWe first consider the case when $|n|\\lesssim L_{\\text{max}}$.\n In this case, we decompose the spatial frequencies of $h$ on the left-hand side  of \\eqref{low} into dyadic blocks $\\{|n| \\sim N_0\\}$, where the dyadic number  $N_0 \\ge 1$ satisfies  $N_0^2 \\ll N_2$.\n  For  fixed $N_0$, we decompose the spatial frequencies of $f$ and $g$ into  balls of radius $\\sim N_0$,\n  indexed by $J_1 \\in \\mathcal{J}_1$ and $J_2 \\in \\mathcal{J}_2$, respectively.\n  With a slight abuse of notation, \n  we also use $J_1$ and $J_2$ to denote the balls themselves.\n  Note that for each fixed $J_1 \\in \\mathcal{J}_1$, the product $\\mathbf 1_{J_1} (\\tau_1, n_1)\\cdot  \\mathbf 1_{J_2} (\\tau_1 - \\tau, n_1 - n)$ is nonzero for at most $O(1)$ many $J_2 \\in \\mathcal{J}_2$, and\n  we denote the set of these indices $J_2$'s as $\\mathcal{J}_2 (J_1)$.\n\nNoting that \n\\[\n\\big| |n_1|^2 - |n_2|^2 \\big| = \\big| |n|^2 -  (\\tau + |n|^2) + (\\tau_1 + |n_1|^2) - (\\tau_2 + |n_2|^2) \\big| \\lesssim N_0^2 + L_{\\textup{max}},\n\\]\n\n\\noindent\nwe have \n\\begin{align}\n\\big| |n_1| - |n_2| \\big| \\lesssim N_2^{-1}  (N_0^2 + L_{\\textup{max}})  \\ll 1.\n\\label{C1}\n\\end{align}\n \n \n \\noindent\n Given $j \\in \\mathbb{N}$, let $A_j = \\{ ( \\tau_1, n_1) : j < |n_1| \\leq j+1 \\}$\n and  $B_j = \\{ ( \\tau_2, n_2) : j  < |n_2| \\leq j + 1 \\}$.\n In view of \\eqref{C1}, we see that  \n  the product $\\mathbf 1_{A_{j_1}} (\\tau_1, n_1) \\cdot \\mathbf 1_{B_{j_2}} (\\tau_2, n_2)$\n   is nonzero if and only if $|j_1 - j_2| \\le 1$.\nHence, by letting $f_{j_1,  J_1}(\\tau_1, n_1) = \\mathbf 1_{A_{j_1} \\cap J_1} (\\tau_1, n_1)\\cdot f(\\tau_1, n_1)$\nand $g_{j_2, J_2}(\\tau_2, n_2) = \n \\mathbf 1_{B_{j_2} \\cap J_2} (\\tau_2, n_2)\\cdot g(\\tau_2, n_2)$, \n we can estimate  \n the left-hand side of \\eqref{low}  by \n\\begin{align*}\n\\sum_{\\substack{N_0\\ge 1\\\\ \\text{dyadic}}} \\sum_{\\substack{J_1 \\in \\mathcal{J}_1 \\\\ J_2 \\in \\mathcal{J}_2 (J_1)}} \\sum_{\\substack{j_1, j_2\\in \\mathbb{N}\\\\ |j_1 - j_2| \\le 1}} \n\\bigg| \n\\sum_{\\substack{n, n_1 \\in \\mathbb{Z}^2\\\\ n = n_1 - n_2\\\\|n| \\sim N_0}} &  \\iintt_{\\tau = \\tau_1 - \\tau_2}  f_{j_1, J_1} (\\tau_1, n_1)\ng_{j_2, J_2} (\\tau_2, n_2) \n\\\\\n&\\times \nh(\\tau, n) \\cdot \\mathbf 1_{S_{\\tau, n, j_1, j_2, J_1, J_2}}(\\tau_1, n_1) d\\tau d\\tau_1 \\bigg|,\n\\end{align*}\n\n\\noindent\nwhere the set $S_{\\tau, n, j_1, j_2, J_1, J_2}$ is defined by\n\\begin{align}\n\\begin{split}\nS_{\\tau, n, j_1, j_2, J_1, J_2} := \\big\\{ &(\\tau_1, n_1) \\in \\mathbb{R} \\times \\mathbb{Z}^2: \n(\\tau_1, n_1) \\in   \\mathfrak{P}_{N_1} \\cap \\mathfrak{S}_{L_1} \\cap A_{j_1},  \\, n_1 \\in J_1 \\\\\n&(\\tau_1 - \\tau, n_1 - n) \\in  \\mathfrak{P}_{N_2} \\cap \\mathfrak{S}_{L_2}\\cap B_{j_2}, \\, \nn_1 - n \\in J_2, \\\\\n&  |\\cos \\angle (n, n_1 - n)| \\ll \\theta \\big\\}.\n\\end{split}\n\\label{C1a}\n\\end{align}\n\n\nBy the Cauchy-Schwarz inequality in $\\tau_1$ and $n_1$, H\\\"older's inequality in $\\tau$ and $n$, and H\\\"older's inequality in $j$, we then have\n\\begin{align*}\n&\\text{LHS of }\\eqref{low} \\\\\n&\\leq \\sum_{\\substack{N_0\\ge 1\\\\ \\text{dyadic}}} \\sum_{\\substack{J_1 \\in \\mathcal{J}_1\\\\ J_2 \\in \\mathcal{J}_2 (J_1)}} \\sum_{\\substack{j_1, j_2\\in \\mathbb{N}\\\\ |j_1 - j_2| \\le 1}} \n \\sum_{n: |n| \\sim N_0} \\int \n\\| \\mathbf 1_{S_{\\tau, n, j_1, j_2, J_1, J_2}}\\|_{L^2_{\\tau_1} \\l^2_{n_1}}\\\\ \n&\\qquad \\times \n\\| f_{j_1, J_1} (\\tau_1, n_1) g_{j_2, J_2} (\\tau_1 - \\tau, n_1 - n)\\|_{L^2_{\\tau_1} \\l^2_{n_1}}\n |h(\\tau, n)| d\\tau \\\\\n&\\leq \\sum_{\\substack{N_0\\ge 1\\\\ \\text{dyadic}}} \\sum_{\\substack{J_1 \\in \\mathcal{J}_1 \\\\ J_2 \\in \\mathcal{J}_2 (J_1)}} \\sum_{\\substack{j_1, j_2\\in \\mathbb{N}\\\\ |j_1 - j_2| \\le 1}} \n \\underline{L}_{12}^{\\frac 12}  \\| f _{j_1, J_1}  \\|_{L_{\\tau_1}^2 \\l_{n_1}^2}\n  \\| g_{j_2, J_2} \\|_{L_{\\tau_2}^2 \\l_{n_2}^2} \\| h \\|_{L_\\tau^2 \\l_n^2} \\\\\n&\\qquad \\times \\sup_{\\substack{\\tau, \\tau_1, n\\\\ |n| \\sim N_0}} \\big|\\widetilde S_{\\tau, \\tau_1, n, j_1, j_2, J_1, J_2}\\big|^{\\frac 12} \\\\\n&\\lesssim \n\\sum_{\\substack{N_0\\ge 1\\\\ \\text{dyadic}}} \\sum_{\\substack{J_1 \\in \\mathcal{J}_1 \\\\ J_2 \\in \\mathcal{J}_2 (J_1)}} \\underline{L}_{12}^{\\frac 12} \\| \\mathbf 1_{J_1}\\cdot f  \\|_{L_{\\tau_1}^2 \\l_{n_1}^2} \\|  \\mathbf 1_{J_2} \\cdot g\\|_{L_{\\tau_2}^2 \\l_{n_2}^2} \\| h \\|_{L_\\tau^2 \\l_n^2} \\\\\n&\\qquad \\times \\sup_{\\substack{\\tau, \\tau_1, n, j_1, j_2\\\\ |n| \\sim N_0}}  \\big|\\widetilde S_{\\tau, \\tau_1, n, j_1, j_2, J_1, J_2}\\big|^{\\frac 12} , \n\\end{align*}\n\n\n\\noindent\nwhere the set $\\widetilde S_{\\tau, \\tau_1, n, j_1, j_2,  J_1, J_2}$ is defined by\n\\begin{align}\n\\widetilde S_{\\tau, \\tau_1, n, j_1, j_2,  J_1, J_2} := &\\big\\{ n_1 \\in \\mathbb{Z}^2: (\\tau_1, n_1) \\in \nS_{\\tau, n, j_1, j_2, J_1, J_2} \\big\\}.\n\\label{C1b}\n\\end{align}\n\nWe claim that the following bound holds:\n\\begin{align}\n\\sup_{\\substack{J_1 \\in \\mathcal{J}_1\\\\J_2 \\in \\mathcal{J}_2(J_1)}}\n \\sup_{\\tau, \\tau_1, n, j: |n| \\sim N_0} \\big| \\widetilde  S_{\\tau, \\tau_1, n, j_1, j_2, J_1, J_2}\\big| \\lesssim \\min\\bigg( N_0,  \\frac{L_{\\textup{max}}}{N_0} \\bigg). \n \\label{C2}\n\\end{align}\n\n\\noindent\nFor now, let us assume \\eqref{C2}.\nNote that the right-hand side of \\eqref{C2} is bounded by $L_{\\max}^\\frac{1}{2}$.\nThen, by applying  the Cauchy-Schwarz inequality in $J_1 \\in \\mathcal{J}_1$ and\nsumming over dyadic $N_0 \\geq 1$ together with \\eqref{C2}, \nwe  obtain the desired bound \\eqref{low}.\n \n\n\n\nIt remains to prove \\eqref{C2}.\nFor $n_1 \\in \\widetilde  S_{\\tau, \\tau_1, n, j_1, j_2, J_1, J_2}$, \n  it follows from \\eqref{C1b} with \\eqref{C1a} that \n\\[ \\tau_1 \\in \\big( - |n_1|^2 + [-C L_1, C L_1] \\big) \\cap \\big( \\tau - |n_1 - n|^2 + [-C L_2, C L_2] \\big), \\]\nfor some constant $C > 0$. Since this intersection of  the two sets is nonempty,\nwe must have $\\big| -|n_1|^2 - (\\tau - |n_1 - n|^2) \\big| = O(\\overline{L}_{12})$, \nwhich in turn implies \n\\begin{align}\n n_1 \\cdot \\frac{n}{|n|} = -\\frac{\\tau}{2|n|} + \\frac{|n|}{2} + O\\bigg( \\frac{\\overline{L}_{12}}{N_0} \\bigg). \n\\label{C3}\n \\end{align}\n\n\\noindent\nOn the other hand,   \nfor $n_1 \\in \\widetilde  S_{\\tau, \\tau_1, n, j_1, j_2, J_1, J_2}$, \nwe also have\n$n_1 \\in J_1$ where $J_1$ is a ball of radius $\\sim N_0$.\nHence, together with \\eqref{C3}, we see that  the component of $n_1$ which is parallel to $n$ is restricted to an interval of length $\\sim \\min(N_0, L_{\\text{max}} / N_0)$. Furthermore, we have\n\\begin{align*}\n\\begin{split}\n |\\cos \\angle (n_1, n)| & = \\frac{|n_1 \\cdot n|}{|n_1| | n|} \n \\leq \\frac{|n|}{|n_1|}  + \\frac{|(n_1 - n)\\cdot n|}{|n_1||n|}\\\\\n &  \\leq \\frac{|n|}{|n_1|} + |\\cos \\angle (n_1 - n, n)| \\frac{|n_1 - n|}{|n_1|} \\ll \\frac{|n|}{|n_1|} + \\frac{|n_1 - n|^\\varepsilon}{|n|^{1-\\varepsilon} |n_1|} \\ll 1, \n \\end{split}\n\\end{align*}\n\n\\noindent\nsince $|n_1|\\sim |n_1 - n|\\gg |n|^2$.\nHence, by Lemma \\ref{LEM:count} with $\\mu  =  1$, \n $\\nu \\sim  \\min(N_0, L_{\\textup{max}} / N_0)$, and $\\alpha = \\frac \\pi 4$, we obtain\n \\eqref{C2}.\n \n \n \nThe case  $L_{\\text{max}} \\ll |n|$ is similar and much simpler. We apply the same steps as in the previous case ($L_{\\max}\\gtrsim |n|$), except that we do not need to decompose the spatial frequencies of $n$ into dyadic piece or  to localize $n_1$ and $n_2$ on balls of smaller radii.\nIn this case,  we  apply Lemma \\ref{LEM:count} with $\\nu \\sim 1$ to obtain the desired bound.\n\\end{proof}\n\n\n\\medskip \\noindent\n$\\bullet$ \\textbf{Case 3:} $|n_2|^2 \\ll |n|$.\n\nThis case is similar to Case 2, so we will be brief here. For the low-modulation case (i.e.~$L_{\\text{max}} \\ll N_0$), the same argument works by switching the roles of $(\\tau, n)$ and $(\\tau_2, n_2)$. \n\nFor the high-modulation case (i.e.~$L_{\\text{max}} \\gtrsim N_0$), we need to ensure that on the \nright-hand side of \\eqref{high}  in Lemma \\ref{LEM:high}, the power of $L_0$ is less than $\\frac 12$ (so that simply switching the roles of $(\\tau, n)$ and $(\\tau_2, n_2)$ does not work). With $f$, $g$, and $h$ as in the statement of Lemma~\\ref{LEM:high}, when $N_2^2 \\ll N_0 \\sim N_1$ and $L_{\\text{max}} \\gtrsim N_0$, we use the Cauchy-Schwarz inequality in $\\tau$ and $n$ and apply Lemma \\ref{LEM:bilin} to obtain\n\\begin{align*}\n\\text{LHS of } \\eqref{high} &\\lesssim \\| h \\|_{L_\\tau^2 \\ell_n^2}\n \\underline{L}_{12}^{\\frac 12} \\bigg( \\frac{\\overline{L}_{12}}{N_0} + 1 \\bigg)^{\\frac 12 } N_2^{\\frac 12} \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\\\\n&\\lesssim \\big( L_1^{\\frac 12} L_2^{\\frac 12} + \\underline{L}_{12}^{\\frac 12} N_0^{\\frac 14} \\big) \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2} \\\\\n&\\lesssim \\big( L_1^{\\frac 12} L_2^{\\frac 12} L_{\\text{max}}^{0+} + \\underline{L}_{12}^{\\frac 12} L_{\\text{max}}^{\\frac 14 +} \\big) N_0^{0-} \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2},\n\\end{align*}\n\n\\noindent\nwhich suffices for our purpose.\n\n\n\n\\begin{remark}\\label{REM:bilin}\n\\rm\nThe bilinear estimate \\eqref{bilin1} in Proposition \\ref{PROP:bilin}\nalso holds if we replace $u \\overline{v}$ by $uv$.\nIn fact, \na slight modification of the argument allows us to show \n\\begin{equation}\n\\| u v \\|_{X_T^{0,-\\frac 12 + \\delta_1}} \\lesssim \\| u \\|_{X_T^{0, \\frac 12 + \\delta_2}} \\| v \\|_{X_T^{0, \\frac 12 + \\delta_2}},\n\\label{bilin_uv}\n\\end{equation}\n\n\\noindent\nwhere $0 < T \\leq 1$ and $\\delta_1 >  \\delta_2 > 0$ are sufficiently small. \nNote that, in this case, by denoting $n_1$, $n_2$, and $n$ as the frequency of $u$, $v$, and \nthe duality term $w$, respectively.  we have $n = n_1 + n_2$ and $\\tau = \\tau_1 + \\tau_2$ so that \n\\[ (\\tau + |n|^2) - (\\tau_1 + |n_1|^2) - (\\tau_2 + |n_2|^2) = 2n_1 \\cdot n_2. \\]\n\n\\noindent\nHence, we need to perform case-by-case analysis, depending on the interaction between $n_1$ and $n_2$. \n\n\nThe proof of \\eqref{bilin_uv} follows essentially as in the proof of \\eqref{bilin1} except for Lemma \\ref{LEM:high}. With $f$, $g$, and $h$ as in the statement of Lemma \\ref{LEM:high}, when $N_1^2 \\ll N_2 \\sim N_0$ and $L_{\\text{max}} \\gtrsim N_2$,  the Cauchy-Schwarz inequality in $\\tau_2$ and $n_2$ and\n Lemma \\ref{LEM:bilin} yield \n\\begin{equation}\n\\begin{split}\n\\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 + n_2}} &\n\\iintt_{\\tau = \\tau_1 + \\tau_2} f(\\tau_1, n_1) g(\\tau_2, n_2) h(\\tau, n) d\\tau d\\tau_1 \\bigg| \\\\\n&\\lesssim \\big( L_0^{\\frac 12} L_1^{\\frac 12} N_2^{-\\frac 14} + \\underline{L}_{01}^{\\frac 12} N_2^{\\frac 14} \\big) \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2}.\n\\end{split}\n\\label{uv1}\n\\end{equation}\n\n\\noindent\nIn order to repeat the argument in Subcase 2.a in the proof of Proposition \\ref{PROP:bilin}, \nwe need the power of $L_0$ to be less than $\\frac 12$, especially  when $L_{\\text{max}} = L_0$. \nNamely, the bound \\eqref{uv1} can not be used directly as it is.\n\n\nBy H\\\"older's inequality in $\\tau$ and $n$, Young's inequality, and H\\\"older's inequality\nwith \\eqref{high1}, we obtain\n\\begin{equation}\n\\begin{split}\n\\bigg| \\sum_{\\substack{n, n_1\\in \\mathbb{Z}^2\\\\ n = n_1 + n_2}} &\n\\iintt_{\\tau = \\tau_1 + \\tau_2} f(\\tau_1, n_1) g(\\tau_2, n_2) h(\\tau, n) d\\tau d\\tau_1 \\bigg| \\\\\n&\\lesssim \\| f \\|_{\\ell_{n_1}^2 L_{\\tau_1}^{\\frac 8 7}} \\| g \\|_{\\ell_{n_2}^1 L_{\\tau_2}^2} \\| h \\|_{\\ell_n^2 L_{\\tau}^{\\frac 85}} \\\\\n&\\lesssim L_0^{\\frac 18} L_1^{\\frac 38} N_2 \\| f \\|_{L_{\\tau_1}^2 \\ell_{n_1}^2} \\| g \\|_{L_{\\tau_2}^2 \\ell_{n_2}^2} \\| h \\|_{L_\\tau^2 \\ell_n^2}.\n\\end{split}\n\\label{uv2}\n\\end{equation}\n\n\\noindent\nThen, by interpolating \\eqref{uv1} and \\eqref{uv2}, we  obtain the desired inequality. A similar inequality also holds  when $N_2^2 \\ll N_1 \\sim N_0$ and $L_{\\text{max}} \\gtrsim N_1$ by switching the roles of $(\\tau_1, n_1)$ and $(\\tau_2, n_2)$.\n\n\n\\end{remark}\n\n\n\n\\section{Local well-posedness of the quadratic NLS}\n\\label{SEC:LWP}\n\nIn this section, we present the proof of Theorem \\ref{THM:LWP}, i.e. local well-posedness of the quadratic NLS \\eqref{qNLS} in $L^2(\\mathbb{T}^2)$.\nBy writing \\eqref{qNLS} in the following Duhamel formulation, we have \n\\begin{equation}\nu(t) = \\Gamma(u) := e^{it\\Delta} u_0 - i \\int_0^t e^{i(t-t')\\Delta} |u|^2(t') dt'.\n\\label{Duh}\n\\end{equation}\n\n\\noindent\nLet $\\varepsilon > 0$ be sufficiently small. Then, by \\eqref{Duh}, Lemma \\ref{LEM:Xlin}, and Proposition \\ref{PROP:bilin}, we have\n\\begin{align*}\n\\| \\Gamma(u) \\|_{X_T^{0, \\frac 12 + \\varepsilon}} &\\leq \\| e^{it\\Delta} u_0 \\|_{X_T^{0, \\frac 12 + \\varepsilon}} + \\bigg\\| \\int_0^t e^{i(t-t')\\Delta} u(t') \\overline{u}(t') dt' \\bigg\\|_{X_T^{0, \\frac 12 + \\varepsilon}} \\\\\n&\\lesssim \\| u_0 \\|_{L^2} + T^{\\varepsilon }\\| u \\overline{u} \\|_{X^{0, -\\frac 12 + 2 \\varepsilon}} \\\\\n&\\lesssim \\| u_0 \\|_{L^2} + T^{\\varepsilon } \\| u \\|_{X_T^{0, \\frac 12 + \\varepsilon}}^2.\n\\end{align*}\n\n\\noindent\n Similarly, we  obtain the following difference estimate:\n\\begin{align*}\n\\| \\Gamma(u) - \\Gamma(v) \\|_{X_T^{0, \\frac 12 + \\varepsilon}}\n\\lesssim T^{ \\varepsilon}\\Big( \\| u \\|_{X_T^{0, \\frac 12 + \\varepsilon}} + \\| v \\|_{X_T^{0, \\frac 12 + \\varepsilon}} \\Big)\n \\| u-v \\|_{X_T^{0, \\frac 12 + \\varepsilon}} .\n\\end{align*}\n\n\\noindent\nTherefore,  by choosing $T = T(\\| u_0 \\|_{L^2}) > 0$\nsufficiently small, we conclude  that $\\Gamma$ is a contraction on the ball $B_R \\subset X^{0, \\frac 12 + \\varepsilon}_T$ of radius $R \\sim \\| u_0 \\|_{L^2}$.\nThis proves  Theorem~\\ref{THM:LWP}.\n\n\n\n\n\n\n\n\\begin{ackno}\\rm\nR.L.~and T.O.~were supported by the European Research Council (grant no.~864138 ``SingStochDispDyn'').\n They are also grateful to the anonymous referee for the helpful comments.\n\n\\end{ackno}\n\n", "meta": {"timestamp": "2022-08-09T02:14:09", "yymm": "2203", "arxiv_id": "2203.15389", "language": "en", "url": "https://arxiv.org/abs/2203.15389"}}
{"text": "\n", "meta": {"timestamp": "2022-08-08T02:13:53", "yymm": "2203", "arxiv_id": "2203.15475", "language": "en", "url": "https://arxiv.org/abs/2203.15475"}}
{"text": "\\section{Introduction}\n\n\\noindent The classical linear-time / branching-time\nspectrum~\\cite{Glabbeek90} organizes a plethora of notions of\nbehavioural equivalence on labelled transition systems at various\nlevels of granularity ranging from (strong) bisimilarity to trace\nequivalence. Similar spectra appear in other system types, e.g.~on\nprobabilistic systems, again ranging from branching-time equivalence\nsuch as probabilistic bisimilarity to linear-time ones such as\nprobabilistic trace equivalence~\\cite{JouSmolka90}. While the\nvariation in system types (nondeterministic, probabilistic, etc.) is\ncaptured within the framework of \\emph{universal\n  coalgebra}~\\cite{Rutten00}, the variation in the granularity of\nequivalence, which we shall generally refer to as the \\emph{semantics}\nof systems, has been tackled, in coalgebraic generality, in a variety\nof approaches~\\cite{HJS07,KR15,JSS15,JLR18}. One setting that\nmanages to accommodate large portions of the\nlinear-time / branching-time spectrum, notably including also\nintermediate equivalences such as ready similarity, is based on\n\\emph{graded monads}~\\cite{MPS15,DMS19,FMS21a}.\n\nAn important role in the theoretical and algorithmic treatment of a\nbehavioural equivalence is classically played by equivalence\ngames~\\cite{Glabbeek90,Stirling99}, e.g.~in partial-order\ntechniques~\\cite{hnw:por-bisimulation-checking} or in on-the-fly\nequivalence checking~\\cite{h:bisim-verif-journal}. In the present\nwork, we contribute to \\emph{graded semantics} in the sense indicated\nabove by showing that, under mild conditions, we can extract from a\ngiven graded monad a Spoiler-Duplicator game~\\cite{Stirling99} that\ncharacterizes the respective equivalence, i.e.~ensures that two states\nare equivalent under the semantics iff Duplicator wins the game.\n\nAs the name suggests, graded monads provide an \\emph{algebraic} view\non system equivalence; they correspond to \\emph{grad\\-ed theories},\ni.e.~algebraic theories equipped with a notion of \\emph{depth} on\ntheir operations. It has been noticed early on~\\cite{MPS15} that many\ndesirable properties of a semantics depend on this theory being\n\\emph{depth-1}, i.e.~having only equations between terms that are\nuniformly of depth~1. Standard examples include distribution of\nactions over non-deterministic choice (trace semantics) or\nmonotonicity of actions w.r.t.~the choice ordering\n(similarity)~\\cite{DMS19}. Put simply, our generic equivalence game\nplays out an equational proof in a depth-1 equational theory in a\nsomewhat nontraditional manner:\nDuplicator starts a round by playing a set of equational assumptions\nshe claims to hold at the level of successors of the present state,\nand Spoiler then challenges one of these assumptions.\n              \t\t\t\t\t\t\t\t\t\t\nIn many concrete cases, the game can be rearranged in a straightforward\nmanner to let Spoiler move first as usual; in this view, the equational claims\nof Duplicator roughly correspond to a short-term strategy determining the\nresponses she commits to playing after Spoiler's next move. In particular,\nthe game instantiates, after such rearrangement, to the standard pebble\ngame for bisimilarity. We analyse additional cases, including similarity\nand trace equivalence, in more detail. In the latter case, several natural\nvariants of the game arise by suitably restricting strategies played by\nDuplicator.\n\nIt turns out that the game is morally played on a form of\npre-determinization of the given coalgebra, which lives in the\nEilenberg-Moore category of the zero-th level of the graded monad, and as\nsuch generalizes a determinization construction that applies in\ncertain instances of coalgebraic language semantics of\nautomata~\\cite{JSS15}.  Under suitable conditions on the graded monad,\nthis pre-determinization indeed functions as an actual\ndeterminization, i.e.~it turns the graded semantics into standard\ncoalgebraic behavioural equivalence for a functor that we construct on\nthe Eilenberg-Moore category. This construction simultaneously\ngeneralizes, for instance, the standard determinization of serial labelled\ntransition systems for trace equivalence and the identification of\nsimilarity as behavioural equivalence for a suitable functor on\nposets~\\cite{KKV12} (specialized to join\nsemilattices).\n\nWhile graded semantics has so far been constrained to apply only to\nfinite-depth equivalences (finite-depth bisimilarity, finite trace\nequivalence, etc.), we obtain, under the mentioned conditions on the\ngraded monad, a new notion of infinite-depth equivalence induced by a\ngraded semantics, namely via the (pre-)determinization. It turns out\nthe natural infinite version of our equivalence game captures\nprecisely this infinite-depth equivalence. This entails\na fixpoint characterization of graded semantics on finite systems,\ngiving rise to perspectives for a generic algorithmic treatment.\n\n\\paragraph{Related Work.} Game characterizations of process\nequivalences are an established theme in concurrency theory;\nthey tend to be systematic but not generic~\\cite{Glabbeek90,CD08}.\nWork on games for spectra of quantitative equivalences is positioned\nsimilarly~\\cite{FLT11,fl:quantitative-spectrum-journal}. The idea of developing \\mbox{(bi)}simulation\ngames in coalgebraic generality goes back to work on branching-time\nsimulations based on relators~\\cite{Baltag00}. There\nis recent highly general work, conducted in a fibrational setting, on\nso-called codensity games for various notions of\nbisimilarity~\\cite{kkhkh:codensity-games}. The emphasis in this work\nis on generality w.r.t.~the measure of bisimilarity, covering,\ne.g.~two-valued equivalences, metrics, pre-orders, and topologies,\nwhile,\nviewed through the lens of spectra of equivalences, the\nfocus remains on branching time. The style of the codensity game is\ninspired by modal logic, in the spirit of coalgebraic Kantorovich\nliftings~\\cite{BaldanEA18,WildSchroder20};\nSpoiler plays predicates thought of as arguments of modalities.\nWork focused more specifically on games for Kantorovich-style\ncoalgebraic behavioural equivalence and behavioural\nmetrics~\\cite{km:bisim-games-logics-metric} similarly concentrates on\nthe branching-time case. A related game-theoretic characterization is\nimplicit in work on $\\Lambda$-(bi)similarity~\\cite{GorinSchroder13},\nalso effectively limited to branching-time. Comonadic game\nsemantics~\\cite{ADW17, AS18, CD21} proceeds in the opposite way\ncompared to the mentioned work and ours: It takes existing games as\nthe point of departure, and then aims to develop categorical models.\n\nGraded semantics was developed in a line of work mentioned\nabove~\\cite{MPS15,DMS19,FMS21a}. The underlying notion of\ngraded monad stems from algebro-geometric work~\\cite{Smirnov08}\nand was introduced into computer science (in substantially higher generality)\nin work on the semantics of effects~\\cite{Katsumata14}. Our pre-determinization\nconstruction relates to work on coalgebras over algebras~\\cite{BK11}.\n\n\n\n\\paragraph*{Organization.} We discuss preliminaries on categories,\ncoalgebras, graded monads, and games in \\cref{sec:prelims}. We\nrecall the key notions of graded algebra and canonical graded algebra\nin~\\cref{sec:algebras}, and graded semantics in~\\cref{sec:semantics}.\nWe introduce our pre-determinization construction in \\cref{sec:determinization},\nand finite behavioural equivalence games in \\cref{S:games}. In\n\\cref{sec:infinte-depth}, we consider the infinite version of the\ngame, relating it to behavioural equivalence on the pre-determinization. We\nfinally consider specific cases in detail in~\\cref{sec:cases}.\n\n\n\n\n\n\\section{Preliminaries}\\label{sec:prelims}\nWe assume basic familiarity with category theory~\\cite{AHS90}. We\nwill review the necessary background on coalgebra~\\cite{Rutten00},\ngraded monads~\\cite{Smirnov08,MPS15}, and the standard bisimilarity\ngame~\\cite{Stirling99}.\n\n\\paragraph*{The category of sets.}\nUnless explicitly mentioned otherwise, we will\nwork in the category $\\mathbf{Set}$ of sets and functions\n(or \\emph{maps}), which is both complete and\ncocomplete. We fix a terminal object $1=\\{\\star\\}$\nand use $!_{X}$ (or just $!$ if confusion is\nunlikely) for the unique map $X\\to 1$.\n\nIn the subsequent sections, we will mostly draw examples from (slight\nmodifications of) the following (endo-)functors on $\\mathbf{Set}$. The\n\\emph{powerset functor} $\\pow$ sends each set $X$ to its set of\nsubsets $\\pow X$, and acts on a map $f\\colon X\\to Y$ by taking direct\nimages, i.e.~$\\pow f(S):= f[S]$ for $S\\in\\pow X$.\nWe write $\\pow_{\\mathsf f}$ for the \\emph{finitary powerset functor} which sends\neach set to its set of finite subsets; the action of $\\pow_{\\mathsf f}$ on maps is\nagain given by taking direct images. Similarly, $\\pow^+$ denotes the\nnon-empty powerset functor\n($\\pow^+(X)=\\{Y\\in\\pow(X)\\mid Y\\neq\\emptyset\\}$), and $\\pow_{\\mathsf f}^+$ its\nfinitary subfunctor\n($\\pow_{\\mathsf f}^+(X)=\\{Y\\in\\pow_{\\mathsf f}(X)\\mid Y\\neq\\emptyset\\}$).\n\nWe write $\\CalD X$ for the set of \\emph{distributions} on a set $X$:\nmaps $\\mu\\colon X\\to [0,1]$ such that $\\sum_{x\\in X}\\mu(x)=1$.  A\ndistribution $\\mu$ is \\emph{finitely supported} if the set\n$\\{x\\in X~|~\\mu(x)\\neq 0\\}$ is finite. The set of finitely supported\ndistributions on $X$ is denoted $\\CalD_f X$. The assignment\n$X\\mapsto\\CalD X$ is the object-part of a functor: given\n$f\\colon X\\to Y$, the map $\\CalD f\\colon\\CalD X\\to \\CalD Y$ assigns to a\ndistribution $\\mu\\in\\CalD X$ the \\emph{image} distribution\n$\\CalD f(\\mu)\\colon Y\\to [0, 1]$ defined by\n$\\CalD f(\\mu)(y)=\\sum_{x\\in X\\mid f(x)=y} \\mu(x)$.  Then,\n$\\CalD f(\\mu)$ is finitely supported if $\\mu$ is, so $\\CalD_f$ is\nfunctorial as well.\n\n\\takeout{\nWe will also consider the \\emph{contravariant powerset functor}\n$\\Q\\colon\\mathbf{Set}\\to\\mathbf{Set}^{op}$, which acts on sets according to $\\pow$,\ni.e.~$\\Q(X):= \\pow X$, but sends a map $f\\colon X\\to Y$ to the\n\\emph{inverse image map} $\\Q f\\colon\\pow Y\\to \\pow X$ defined by\n$S\\mapsto f^{-1}[S]$. The composite functor\n$\\CalN:=(\\mathbf{Set}\\xra{\\Q}\\mathbf{Set}^{op}\\xra{\\Q^{op}}\\mathbf{Set})$ is called the\n\\emph{neighborhood functor}.  Explicitly, $\\CalN$ is the endofunctor\non $\\mathbf{Set}$ which sends a set $X$ to the set $\\pow(\\pow X)$ of all\n\\emph{neighborhoods} of~$X$, and it sends a map $f\\colon X\\to Y$\nto the map $\\CalN f\\colon\\CalN X\\to \\CalN Y$ defined by the\nassignment\n$\\mathscr S\\mapsto\\{S\\in \\pow Y~|~f^{-1}[S]\\in \\mathscr S\\}$.\n\n\\takeout{\nWe will also work with the \\emph{(finitely supported)\ndistribution functor} $\\CalD\\colon\\mathbf{Set}\\to\\mathbf{Set}$ is the functor\nwhich sends a set $X$ to the set of \\emph{(finitely supported)\nprobability distributions on $X$}. That is, $\\CalD X$ consists of\nall maps $\\varphi\\colon X\\to [0,1]$ (where $[0,1]$ denotes the\nreal unit interval) such that\n\\[\n\\sum_{x\\in X}\\varphi(x) = 1\n\\]\nand such that the \\emph{support}\n$\\Support(\\varphi):=\\{x\\in X\\mid\\varphi(x)\\ne 0\\}$ is finite. It\nis well known (and easy to see) that each $\\varphi\\in\\CalD X$\nis equivalently presented as the formal convex sum\n\\[\n\\sum_{x\\in\\Support(\\varphi)} \\varphi(x)\\cdot x.\n\\]\nIn this notation, the action $\\CalD f\\colon \\CalD X\\to\\CalD Y$\non a map $f\\colon X\\to Y$ is conveniently described via the\nassignment\n$\\sum_{i\\leq k}r_i\\cdot x_i \\mapsto \\sum_{i\\leq k} r_i\\cdot f(x_i).$\n\n\\noindent\n\\paragraph*{Coalgebra.}\n We will review the\nbasic definitions and results of \\emph{universal\ncoalgebra}~\\cite{Rutten00}, a categorical framework for the uniform\ntreatment of a variety of reactive system types.\n\\begin{defn}\n  For an endofunctor $G\\colon\\CatC\\to\\CatC$ on a\n  category $\\CatC$, a \\emph{$G$-coalgebra} (or just\n  \\emph{coalgebra}) is a pair $(X, \\gamma)$ consisting\n  of an object $X$ in $\\CatC$ and a morphism\n  $\\gamma\\colon X\\to GX$.\n  A \\emph{(coalgebra) morphism} from $(X, \\gamma)$ to\n  a coalgebra $(Y, \\delta)$ is a morphism $h\\colon X\\to Y$\n  such that $\\delta\\cdot h = Fh\\cdot\\gamma$.\n \n \n\\end{defn}\n\\noindent Thus, for $\\CatC = \\mathbf{Set}$, a coalgebra consists of a set $X$\nof \\emph{states} and a map $\\gamma\\colon X\\to GX$, which we view as a\ntransition structure that assigns to each state $x\\in X$ a structured\ncollection $\\gamma(x)\\in GX$ of \\emph{successors} in $X$.\n\\begin{expl}\\label{E:coalg}\n  We describe some examples of functors on $\\mathbf{Set}$ and their coalgebras\n  for consideration in the subsequent. Fix a finite set $\\A$ of\n  \\emph{actions}.\n\\begin{enumerate}\n\\item\\label{E:coalg:1} Coalgebras for the functor $G=\\pow(\\A\\times -)$\n  are just\n  \\emph{$\\A$-labelled transition systems (LTS)}: Given such a\n  coalgebra $(X, \\gamma)$, we can view the elements\n  $(a, y)\\in\\gamma(x)$ as the $a$-successors of~$x$. We call\n  $(X, \\gamma)$ \\emph{finitely branching} (resp.~\\emph{serial}\n  if~$\\gamma(x)$ is finite (resp.~non-empty) for all~$x\\in X$.\n Finitely branching (resp.~serial) LTS are coalgebras for\n the functor $G=\\pow_{\\mathsf f}(\\A\\times -)$ (resp. $\\pow^+(\\A\\times -)$).\n\n\n\\takeout{\n\\item A coalgebra for the neighborhood functor $\\CalN$ is a\n  \\emph{neighborhood frame}~\\cite{HKP09}: a map\n  $\\nu\\colon X\\to \\CalN X$ assigning each state $x\\in X$ to its\n  set of \\emph{neighbourhoods} $\\nu(x)\\in\\CalN X$.%\n  \\smnote{I wouldn't write $\\pow(\\pow X)$; this gives the wrong\n    impression that $\\N$ is the double covariant powerset functor.}\n  A morphism\n  of neighborhood frames (also: \\emph{bounded morphism}) from\n  $(X, \\nu)$ to $(X', \\nu')$ is a map $f\\colon X\\to X'$ such that for\n  all $x\\in X$: $f^{-1}[V]\\in\\nu(x)$ iff $V\\in\\nu'(f(x))$ for all\n  $V\\in\\pow X'$.\n\n\n\\item A coalgebra $(X,\\gamma)$ for the functor $G=\\CalD(\\A\\times -)$\n  is a \\emph{(generative) probabilistic transition system} (PTS): The\n  transition structure~$\\gamma$ assigns to each state $x\\in X$ a\n  distribution $\\gamma(x)$ on pairs $(a,y)\\in\\A\\times X$. We think of\n  $\\gamma(x)(a, y)$ as the probability of executing an $a$-transition\n  to state~$y$ while sitting in state~$x$.\n  A PTS $(X, \\gamma)$ is \\emph{finitely branching} if $\\gamma(x)$ is\n  finitely supported for all $x\\in X$; then, finitely branching PTS\n  are coalgebras for $\\CalD_f(\\A\\times -)$.\n\\end{enumerate}\n\\end{expl}\n\nGiven coalgebras $(X, \\gamma)$ and $(Y, \\delta)$ for an endofunctor\n$G$ on $\\mathbf{Set}$, states $x\\in X$ and $y\\in Y$ are\n\\emph{$G$-behaviourally equivalent} if there exist coalgebra morphisms\n\\[\n  (X,\\gamma) \\xra{f} (Z,\\zeta) \\xla{g} (Y,\\delta)\n\\]\nsuch that $f(x)= g(y)$. Behavioural equivalence can be approximated\nvia the (initial $\\omega$-segment of the) \\emph{final chain}\n$(G^n1)_{n\\in\\omega}$, where $G^n$ denotes $n$-fold application of\n$G$.\nThe \\emph{canonical cone} of a coalgebra\n$(X, \\gamma)$ is then the family of maps $\\gamma_n\\colon X\\to G^n1$\ndefined inductively for $n\\in\\omega$ by\n\\begin{align*}\n  \\gamma_0 &= \\big(X\\xra{!} 1\\big), \\text{and} \\\\\n  \\gamma_{n+1} &= \\big(X \\xra{\\gamma} GX \\xra{G\\gamma_n} GG^n 1 = G^{n+1}1\\big).\n\\end{align*}\nStates $x, y\\in X$ are \\emph{finite-depth behaviourally equivalent} if\n$\\gamma_n(x)= \\gamma_n(y)$ for all $n \\in \\omega$.\n\n\\begin{rem}\\label{rem:finite-depth}\n  It follows from results of Worrell~\\cite{Worrell05} that behavioural\n  equivalence and finite-depth behavioural equivalence coincide for\n  finitary functors on $\\mathbf{Set}$, where a functor $G$ on $\\mathbf{Set}$ is\n  \\emph{finitary} if it preserves filtered colimits. Equivalently, for\n  every set $X$ and each $x \\in GX$ there exists a finite subset $Y\n  \\subseteq X$ such that $x = Gi[GY]$, where $i\\colon Y \\hookrightarrow X$ is\n  the inclusion map~\\cite[Cor.~3.3]{amsw19-1}.\n\\end{rem}\n\n\\paragraph*{Bisimilarity games.}\nWe briefly recapitulate the classical \\emph{bisimilarity game}, a\ntwo-player graph game between the players Duplicator (D) and Spoiler\n(S); player~D tries to show that two given states are bisimilar,\nwhile~S tries to refute this. \\emph{Configurations} of the game are\npairs $(x, y)\\in X\\times X$ of states in a LTS $(X, \\gamma)$. The game\nproceeds in rounds, starting from the \\emph{initial configuration},\nwhich is just the contested pair of states. In each round, starting\nfrom a configuration $(x, y)$,~S picks one of the sides, say,~$x$, and\nthen selects an action $a\\in\\A$ and an $a$-successor~$x'$ of~$x$;\nplayer~D then selects a corresponding successor on the other side, in\nthis case an $a$-successor~$y'$ of~$y$. The game then reaches the new\nconfiguration $(x', y')$. If a player gets stuck, the play is\n\\emph{winning} for their opponent, whereas any infinite play is\nwinning for~D.\n\nIt is well known (e.g.~\\cite{Stirling99}) that~D has a\nwinning strategy in the bisimilarity game at a configuration $(x, y)$\niff $(x, y)$ is a pair of bisimilar states. Moreover, for finitely\nbranching LTS, an equivalent formulation may be given in terms of the\n\\emph{$n$-round bisimilarity game}:\nthe rules of the $n$-round game are the same as those above, only\nnow~D wins as soon as at most~$n$ rounds have been played. In fact, a\nconfiguration $(x, y)$ is a bisimilar pair precisely if~D has\na winning strategy in the $n$-round bisimilarity game for all\n$n\\in\\omega$.\n\nWe mention just one obvious variation of this game that characterizes\na different spot on the linear-time/branching-time spectrum: The\n\\emph{mutual-simulation game} is set up just like the bisimulation\ngame, except that~S may only choose his side once, in the first round,\nand then has to move on that side in all subsequent rounds (in the\nbisimulation game, he can switch sides in every round if he\ndesires). It is easily checked that states~$x,y$ are mutually similar\niff~S wins the position $(x,y)$ in the mutual-simulation game. We will\nsee that both these games (and many others) are obtained\nas instances of our generic notion of graded equivalence game.\n\n\n\n\\paragraph*{Graded monads.}\nWe now review some background material on graded monads\n\\cite{Smirnov08, MPS15}:\n\\begin{defn}\\label{D:gradedmonad}\n  A \\emph{graded monad} $\\M$ on a category $\\CatC$ is a triple\n  $(M, \\eta, \\mu)$ where $M$ is a family of functors\n  $M_n\\colon\\CatC\\to\\CatC$ on $\\CatC$ ($n\\in\\omega$),\n  $\\eta\\colon \\mathsf{id} \\to M_0$ is a natural transformation (the\n  \\emph{unit}), and $\\mu$ is a family of natural transformations%\n  \\begin{equation}\n    \\mu^{n,k}\\colon M_nM_k\\to M_{n+k}  \\tag{$n,k\\in\\omega$}\n  \\end{equation}\n  (the \\emph{multiplication}) such that the following diagrams commute\n  for all $n,m,k\\in\\omega$:\n  \\begin{equation}\\label{diagram:unitlaw}\n    \\begin{tikzcd}[column sep=40]\n      &\n      M_n\n      \\arrow[ld, \"M_n\\eta\"'] \\arrow[rd, \"\\eta M_n\"]  \\arrow[d, \"\\Id\"]\n      \\\\\n      M_nM_0\n      \\arrow[r, \"{\\mu^{n,0}}\"]\n      &\n      M_n\n      &\n      M_0M_n \\arrow[l, \"{\\mu^{0,n}}\"']\n    \\end{tikzcd}\n   \n  \\end{equation}\n  \\begin{equation}\\label{diagram:associativelaw}\n    \\begin{tikzcd}[column sep = 60]\n      M_nM_kM_m\n      \\arrow[r, \"{M_n\\mu^{k,m}}\"]\n      \\arrow[d, \"{\\mu^{n,k}M_m}\"']\n      &\n      M_{n}M_{k+m} \\arrow[d, \"{\\mu^{n,k+m}}\"]\n      \\\\\n      M_{n+k}M_m \\arrow[r, \"{\\mu^{n+k,m}}\"]\n      &\n      M_{n+k+m}\n    \\end{tikzcd}\n  \\end{equation}\n  We refer to~\\eqref{diagram:unitlaw} and~\\eqref{diagram:associativelaw}\n  as the \\emph{unit} and \\emph{associative} laws of $\\M$,\n  respectively. We call $\\M$ \\emph{finitary} if all of the\n  functors $M_n\\colon\\CatC\\to\\CatC$ are finitary.\n\\end{defn}\n\\noindent\nThe above notion of graded monad is due to Smirnov \\cite{Smirnov08}.\nKatsumata~\\cite{Katsumata14}, Fujii et al.~\\cite{FKM16}, and\nMellies~\\cite{Mellies17} consider a more general notion of graded (or\n\\emph{parametrized}) monad given as a lax monoidal action of a\nmonoidal category $\\Mon$ (representing the system of grades) on a\ncategory $\\CatC$. Graded monads in the above sense are recovered by\ntaking~$\\Mon$ to be the (discrete category induced by the) monoid\n$(\\N, +, 0)$.\n\nThe graded monad laws imply that the triple $(M_0, \\eta, \\mu^{0,0})$\nis a (plain) monad on the base category $\\CatC$; we use this freely\nwithout further mention.\n\n\\begin{expl}\\label{E:graded-monad}\n  We review some salient constructions~\\cite{MPS15} of graded\n  monads on $\\mathbf{Set}$ for later use.\n  \\begin{enumerate}\n  \\item\\label{E:graded-monad:1} Every endofunctor $G$ on $\\mathbf{Set}$\n    induces a graded monad~$\\M_G$ with underlying endofunctors\n    $M_n:= G^n$ (the $n$-fold composite of~$G$ with itself); the unit\n    $\\eta_X\\colon X\\to G^0X= X$ and multiplication\n    $\\mu_X^{n,k}\\colon G^nG^kX\\to G^{n+k}X$ are all identity maps.\n    We will later see that~$\\M_G$ captures (finite-depth) $G$-behavioural\n    equivalence.\n\n  \\item\\label{item:graded-kleisli}\n  Let $(T, \\eta, \\mu)$ be a monad on $\\mathbf{Set}$, let $F$ be\n  an endofunctor on $\\mathbf{Set}$, and let $\\lambda\\colon FT\\to TF$\n  be a natural transformation such that\n    \\[\n      \\lambda\\cdot F\\eta=\\eta F\n      \\qquad\\text{and}\\qquad\n      \\lambda \\cdot F\\mu = \\mu F \\cdot T\\lambda \\cdot \\lambda T\n    \\]\n    (i.e.~$\\lambda$ is a distributive law of the functor~$F$ over the\n    monad~$T$). For each $n\\in\\omega$, let\n    $\\lambda^n\\colon F^nT\\to TF^n$ denote the natural transformation\n    defined inductively by\n    \\[\n      \\lambda^0:= \\mathsf{id}_T; \\qquad \\lambda^{n+1}:= \\lambda^n F\\cdot F^n\\lambda.\n    \\]\n    We obtain a graded monad with $M_n:= TF^n$, unit $\\eta$, and\n    components $\\mu^{n,k}$ of the multiplication given as the\n    composites\n   \n    \\[\n     \n      TF^nTF^k\\xra{T\\lambda^n F^k}  TTF^nF^k = TTF^{n+k}\\xra{ \\mu F^{n+k}} TF^{n+k}.\n    \\]\n    Such graded monads relate strongly to Kleisli-style coalgebraic\n    trace semantics~\\cite{HJS07}.\n  \\item\\label{item:T-traces} We obtain (by instance of the example above) a graded monad\n    $\\M_T(\\A)$ with $M_n= T(\\A^n\\times -)$ for every monad $T$ on\n    $\\mathbf{Set}$ and every set~$\\A$. Thus, $\\M_T$ is a graded monad for\n    traces under effects specified by~$T$; e.g.~for $T=\\CalD$, we will\n    see that $\\M_T(\\A)$ captures probabilistic trace equivalence on\n    PTS.\n\n  \\item\\label{E:graded-monad:4}Similarly, given a monad $T$, an\n    endofunctor $F$, both on the same category $\\CatC$, and a\n    distributive law $\\lambda\\colon TF \\to FT$ of~$T$ over $F$, we\n    obtain a graded monad with $M_n := F^nT$, unit and\n    multiplication given analogously as in\n    item~\\ref{item:graded-kleisli} above\n    (see~\\cite[Ex.~5.2.6]{MPS15}). Such graded monads relate strongly\n    to Eilenberg-Moore-style coalgebraic language semantics~\\cite{bms13}\n\\end{enumerate}\n\\end{expl}\n\nGraded variants of Kleisli triples have been introduced and proved\nequivalent to graded monads (in a more general setting)\nby Katsumata~\\cite{Katsumata14}:\n\n\\begin{notn}\\label{N:star}\nWe will employ the \\emph{graded Kleisli star} notation:\nfor $n\\in\\omega$ and a morphism $f\\colon X\\to M_k Y$, we\nwrite%\n\\begin{equation}\\label{Eqn:Kleisli-star}\n  f^*_n:=  \\big(M_nX\\xra{M_nf} M_nM_k\\xra{\\mu^{n,k}}M_{n+k}Y\\big).\n\\end{equation}\nIn this way, we obtain a morphism satisfying the following graded\nvariants~\\cite[Def.~2.3]{Katsumata14} of the usual laws of the\nKleisli star operation for ordinary monads: for every $m\\in\\omega$ and\nmorphisms $f\\colon X\\to M_nY$ and $g\\colon Y\\to M_kZ$ we have:%\n\\begin{align}\n  f^*_0\\cdot\\eta_X &= f,\t\\label{item:star-2} \\\\\n  (\\eta_X)^*_n &= \\mathsf{id}_{M_n X}, \\label{item:start-3}\\\\\n  (g^*_{n}\\cdot f)^*_m &= g^*_{m+n}\\cdot f_m^*.    \\label{item:star-1}\n\\end{align}\n\n\\end{notn}\n\n\n\n\\paragraph*{Graded theories.}\nGraded theories, in a generalized form in which arities of operations\nare not restricted to be finite, have been proved equivalent to graded\nmonads on $\\mathbf{Set}$~\\cite{MPS15} (the finitary case was implicitly\ncovered already by Smirnov~\\cite{Smirnov08}).  We work primarily with\nthe finitary theories below; we consider infinitary variants of such\ntheories only when considering infinite-depth equivalences\n(\\cref{sec:infinte-depth}).\n\n\n\\begin{defn}\\label{def:theory}\n\\begin{enumerate}\n\\item A \\emph{graded signature} is a set $\\Sigma$ of \\emph{operations}\n  $f$ equipped with a finite \\emph{arity} $\\mathsf{ar}(f) \\in \\omega$ and a\n  finite \\emph{depth} $d(f)\\in\\omega.$ An operation of arity 0 is\n  called a \\emph{constant}.\n\n\\item\nLet $X$ be a set of \\emph{variables} and let\n$n\\in\\omega$. The set $\\Termsarg{\\Sigma, n}(X)$\nof \\emph{$\\Sigma$-terms of uniform depth $n$ with\nvariables in $X$} is defined inductively as follows:\nevery variable $x\\in X$ is a term of uniform depth\n$0$ and, for $f\\in\\Sigma$ and\n$t_1,\\dots, t_{\\mathsf{ar}(f)}\\in\\Termsarg{\\Sigma, k}(X)$,\n$f(t_1,\\dots, t_{\\mathsf{ar}(f)})$ is a $\\Sigma$-term\nof uniform depth $k+ d(f)$. In particular,\na constant $c$ has uniform depth $k$ for all $k\\geq d(c)$.\n\\item A \\emph{graded $\\Sigma$-theory} is a set $\\E$ of\n  \\emph{uniform-depth equations}: pairs $(s, t)$, written `$s=t$',\n  such that $s,t\\in\\Termsarg{\\Sigma, n}(X)$ for some $n\\in\\omega$;\n  %\n \n  we say that $(s, t)$ is \\emph{depth-$n$}.\n  A theory is \\emph{depth-$n$} if all of its equations and\n  operations have depth at most $n$.\n\\end{enumerate}\n\\end{defn}\n\n\\begin{notn}\\label{N:substitution}\nA \\emph{uniform-depth substitution} is a map\n$\\sigma\\colon X\\to\\Termsarg{\\Sigma, k}(Y)$,\nwhere $k\\in\\omega$ and $X, Y$ are sets.\nThen $\\sigma$ extends to a family of maps\n$\\bar{\\sigma}_n\\colon\\Termsarg{\\Sigma, n}(X)\\to\\Termsarg{\\Sigma, k+n}(Y)$\n($n\\in\\omega$) defined recursively by\n\\[\n\\bar{\\sigma}_n(f(t_1,\\dots, t_{\\mathsf{ar}(f)})) =\nf(\\bar{\\sigma}_m(t_1),\\dots, \\bar{\\sigma}_m(t_{\\mathsf{ar}(f)})),\n\\]\nwhere $t_i\\in\\Termsarg{\\Sigma, m}$ and\n$d(f)+m = n$. For a term $t\\in T_{\\Sigma, k}(X)$,\nwe also write $t\\sigma:= \\bar{\\sigma}_n(t)$ when\nconfusion is unlikely.\n\\end{notn}\n\\noindent\nGiven a graded theory $\\T=(\\Sigma, \\E)$, we have essentially the\nstandard notion of equational derivation (sound and complete over\ngraded algebras, cf.\\ \\cref{sec:algebras}), restricted to\nuniform-depth equations. Specifically, the system includes the\nexpected rules for reflexivity, symmetry, transitivity, and\ncongruence, and moreover allows substituted introduction of axioms: If\n$s=t$ is in~$\\E$ and~$\\sigma$ is a uniform-depth substitution, then\nderive the (uniform-depth) equation $s\\sigma=t\\sigma$. (A substitution\nrule that more generally allows uniform-depth substitution into\nderived equations is then admissible.)  For a set~$Z$ of uniform-depth\nequations, we write\n\\begin{equation*}\nZ\\vdash s=t\n\\end{equation*}\nif the uniform-depth equation $s=t$ is derivable from equations in~$Z$\nin this system; note that unlike the equational axioms in~$\\E$, the\nequations in~$Z$ cannot be substituted into in such a derivation (they\nconstitute assumptions on the variables occurring in~$s,t$).\n\nWe then see that~$\\T$ induces a graded monad~$\\M_{\\T}$ with $M_nX$\nbeing the quotient of $\\Termsarg{\\Sigma, n}(X)$ modulo derivable\nequality under $\\E$; the unit and multiplication of $\\M_{\\T}$ are\ngiven by the inclusion of variables as depth-0 terms and the\ncollapsing of layered terms, respectively. Conversely, every graded\nmonad arises from a graded theory in this way~\\cite{MPS15}.\n\nWe will restrict attention to graded monads presented by depth-$1$\ngraded theories:\n\n\\begin{defn}\n  A \\emph{presentation} of a graded monad $\\M$ is a graded theory $\\T$\n  such that $\\M\\cong\\M_{\\T}$, in the above notation.  A graded monad\n  is \\emph{depth-1} if it has a depth-$1$ presentation.\n\\end{defn}\n\n\n\n\\begin{expl}\\label{E:graded-theory}\n  Fix a set $\\A$ of actions. We describe \\mbox{depth-$1$} graded theories\n  associated (via the induced behavioural equivalence,\n  \\cref{sec:semantics}) to standard process equivalences on LTS and\n  PTS~\\cite{DMS19}.\n\\begin{enumerate}\n\\item\\label{item:jsl-a} The graded theory $\\JSL(\\A)$ of\n  \\emph{$\\A$-labelled join semilattices} has as depth-1 operations all\n  formal sums\n  \\[\n    \\textstyle\\sum_{i=1}^na_i(-),\n    \\quad\n    \\text{for \\mbox{$n\\ge 0$} and $a_1,\\dots,a_n\\in\\A$}\n  \\]\n  (and no depth-$0$ operations); we write~$0$ for\n  the empty formal sum. The axioms of $\\JSL(\\A)$ consist of all\n  depth-1 equations $\\sum_{i=1}^na_i(x_i) = \\sum_{j=1}^m b_j(y_j)$\n  (where the~$x_i$ and~$y_j$ are variables, not necessarily distinct)\n  such that\n  $\\{(a_i, x_i)~|~1\\le i\\leq n\\}=\\{(b_j, y_j)~|~1\\le j\\leq m\\}$.  The\n  graded monad induced by $\\JSL(\\A)$ is $\\M_G$ for\n  $G=\\pow_{\\mathsf f}(\\A\\times(-))$\n  (cf.~\\cref{E:graded-monad}.\\ref{E:graded-monad:1}).\n\n\\item\\label{item:pt-a} The \\emph{graded theory of probabilistic\n    traces}, $\\mathsf{PT}(\\A)$, has a depth-0 convex sum operation\n  \\[\n    \\textstyle\\sum^n_{i = 1} p_i\\cdot(-)\n    \\quad\n    \\text{for all $p_1, \\ldots, p_n \\in [0,1]$ such that\n      $\\sum^n_{i=1} p_i = 1$}\n  \\]\n  and unary depth-1 operations $a(-)$ for all actions $a\\in\\A$. As\n  depth-0 equations, we take the usual equational axiomatisation of\n  convex algebras, which is given by the equation\n  $\\sum^n_{i = 1}\\delta_{ij}\\cdot x_j = x_i$ (where $\\delta_{ij}$\n  denotes the Kronecker delta function) and all instances of the\n  equation scheme\n\\[\n\\sum^n_{i = n} p_i\\cdot \\sum^m_{j=1} q_{ij}\\cdot x_j =\n\\sum_{j=1}^m \\Big(\\sum^{n}_{i=1}p_iq_{ij}\\Big)\\cdot x_j.\n\\]\nWe further impose depth-1 equations\nstating that actions distribute over\nconvex sums:\n\\[\na\\Big(\\sum_{i=1}^np_i\\cdot x_i\\Big) = \\sum_{i=1}^n p_i\\cdot a(x_i).\n\\]\nThe theory $\\mathsf{PT}(\\A)$ presents~$\\M_{\\CalD_f}(\\A)$,\nwhere $\\CalD_f$ is the finitely supported distribution monad\n(cf.~\\cref{E:graded-monad}.\\ref{item:T-traces}).\n\n\\item\\label{item:traces-a} We mention two variations on the graded\n  theory above. First, the \\emph{graded theory of (non-deterministic)\n    traces} presenting $\\M_{\\pow_{\\mathsf f}}(\\A)$ has depth-0\n  operations~$+$,~$0$ and equations for join-semilattices with bottom,\n  and unary depth-1 operations~$a$ for $a\\in\\A$ as in\n  \\ref{item:jsl-a} above; the depth-1 equations now state that\n  actions distribute over joins and preserve bottom. Second, the\n  \\emph{graded theory of serial (non-deterministic) traces} arises by\n  omitting~$0$ and associated axioms from the graded theory of traces,\n  and yields a presentation of $\\M_{\\pow_{\\mathsf f}^+}(\\A)$.\n\n\\item\\label{item:simulation-a} The \\emph{graded theory of simulation}\n  has the same signature and depth-0 equations as the graded theory of\n  traces,\n  along with depth-1 equations stating that actions are monotone:\n\\[\na(x+y) + a(x) = a(x+y).\n\\]\nThe theory of simulation equivalence then\nyields a presentation of the graded monad\nwith $M_nX$ defined inductively along with\na partial ordering as follows: We take\n\\mbox{$M_0X = \\pow_{\\mathsf f}(X)$} ordered by set inclusion.\nWe equip $\\A\\times M_nX$ with the product ordering\nof the discrete order on $\\A$ and the given\nordering on $M_nX$. Then $M_{n+1}X = \\pow_{\\mathsf f}^{\\downarrow}(\\A\\times M_nX)$ is\nthe set  of downwards-closed finite subsets of\n$\\A\\times M_nX$.\n\\end{enumerate}\n\\end{expl}\n\nIn the following lemma, an \\emph{epi-transformation} is a natural\ntransformation $\\alpha$ whose components $\\alpha_X$ are surjective maps.\n\n\\begin{notheorembrackets}\n\\begin{lem}[{\\cite{MPS15}}]\\label{L:depth-1}\n  A graded monad $\\M$ on $\\mathbf{Set}$ is \\emph{depth-1} if and only if\n  all $\\mu^{1,n}$ are epi-transformations and the following is\n  object-wise a coequalizer diagram in the category of\n  Eilenberg-Moore algebras for the monad $M_0$ for all\n  $n\\in\\omega$:\n  \\begin{equation}\n    \\begin{tikzcd}[column sep = 35]\\label{Diagram:depth1}\n      M_1M_0M_n \\arrow[r, \"M_1\\mu^{0,n}\", shift left] \\arrow[r,\n      \"\\mu^{1,0}M_n\"', shift right] & M_1M_n \\arrow[r, \"\\mu^{1,n}\"] &\n      M_{1+n}.\n    \\end{tikzcd}\n  \\end{equation}\n\\end{lem}\n\\end{notheorembrackets}\n\n\\section{Graded Behavioural Equivalences}\\label{sec:semantics}\nWe next recall the notion of a graded semantics~\\cite{MPS15}\non coalgebras for an endofunctor on $\\mathbf{Set}$;\nwe illustrate several instantiations of subsequent interest.\n\n\n\\begin{defn}[Graded semantics]\n  A \\emph{(depth-1) graded semantics} for an endofunctor\n  $G\\colon\\mathbf{Set}\\to\\mathbf{Set}$ is a pair $(\\alpha, \\M)$ consisting of a\n  (depth-1) graded monad $\\M$ on $\\mathbf{Set}$ and a natural transformation\n  $\\alpha\\colon G\\to M_1$ .\n\\end{defn}\n\\noindent\nGiven a $G$-coalgebra $(X,\\gamma)$, the graded\nsemantics $(\\alpha, \\M)$ induces a sequence of\nmaps $\\gamma^{(n)}\\colon X\\to M_n1$ inductively\ndefined by\n\\begin{align*}\n  \\gamma^{(0)} &:= (X\\xra{\\eta_X}M_0X\\xra{M_0!}M_01); \\\\\n  \\gamma^{(n+1)} &:= (X\\xra{\\alpha_X\\cdot\\gamma} M_1X\n                                  \\xra{M_1\\gamma^{(n)}} M_1M_n 1\n\t\t\t\t \\xra{\\mu^{1,n}_1} M_{1+n}1)\n\\end{align*}\n(or, using the graded Kleisli star,\n$\\gamma^{(n+1)} = (\\gamma^{(n)})^*_1\\cdot\\alpha_X\\cdot\\gamma$). We call\n$\\gamma^{(n)}(x)\\in M_n1$ the \\emph{$n$-step $(\\alpha, \\M)$-behaviour}\nof $x\\in X$.\n\n\\begin{defn}[Graded behavioural equivalence]\n  States ${x\\in X, y\\in Y}$ in $G$-coalgebras $(X, \\gamma)$ and\n  $(Y, \\delta)$ are \\emph{depth-n behaviourally equivalent} under\n  $(\\alpha, \\M)$ if ${\\gamma^{(n)}(x) = \\delta^{(n)}(y)}$, and\n  \\emph{$(\\alpha, \\M)$-behaviourally equivalent} if\n  $\\gamma^{(n)}(x) = \\delta^{(n)}(y)$ for all $n\\in\\omega$. We\n  refer to $(\\alpha,\\M)$-behavioural equivalence as a\n  \\emph{graded behavioural equivalence} or just a \\emph{graded\n    equivalence}.\n\\end{defn}\n\n\n\\begin{expl}\\label{E:semantics}\nWe recall~~\\cite[Section 4]{DMS19} several graded\nequivalences, restricting primarily to LTS and PTS.\n\\begin{enumerate}\n\\item\\label{item:sem-beh}\n For an endofunctor $G$ on $\\mathbf{Set}$, finite-depth\n $G$-behavioural equivalence arises as the\n graded equivalence with $\\M = \\M_G$ and\n $\\alpha = \\mathsf{id}$, where $\\M_G$ is the graded\n monad of\n \\cref{E:graded-monad}.\\ref{E:graded-monad:1}.\n By~\\cref{rem:finite-depth}, it follows that\n $(\\mathsf{id}, \\M_G)$ captures full coalgebraic bisimilarity\n in case $G$ is finitary.\n\n\\item\\label{item:sem-trace} Let $(X, \\gamma)$ be an LTS, let\n  ${x\\in X}$, and let ${w\\in\\A^*}$ be a finite word over $\\A$. We\n  write $x\\xra{w} y$ if the state $y$ can be reached on a path whose\n  labels form the word $w$. A \\emph{finite trace} at $x\\in X$ is a\n  word $w\\in\\A^*$ such that $x\\xra{w} y$ for some $y\\in X$; the set of\n  finite traces at $x$ is denoted $\\tau(x)$. States $x,y\\in X$ are\n  trace equivalent if $\\tau(x)=\\tau(y)$. Trace equivalence on finitely\n  branching LTS is captured by the graded equivalence induced by\n  $\\M=\\M_{\\pow_{\\mathsf f}}(\\A)$\n  (cf.~\\cref{E:graded-monad}.\\ref{item:T-traces}), again with\n  $\\alpha=\\mathsf{id}$; replacing $\\pow_{\\mathsf f}$ with $\\pow^+$ (or with $\\pow_{\\mathsf f}^+$)\n  yields trace equivalence on serial (and finitely branching) LTS.\n\n\\item\\label{item:sem-prob} Probabilistic trace equivalence on PTS is\n  the graded equivalence induced by $\\M=\\M_{\\CalD_f}(\\A)$\n  (cf.~\\cref{E:graded-monad}.\\ref{item:T-traces}) and\n  $\\alpha = \\mathsf{id}$: The maps~$\\gamma^{(k)}$ equip states with\n  distributions on length-$k$ action words, and the induced\n  equivalence identifies states $x$ and $y$ whenever these\n  distributions coincide at~$x$ and~$y$ for all~$k$.\n\n\\item\\label{item:sem-sim}\nSimulation equivalence on LTS can also be\nconstrued as a graded equivalence by taking\n$\\M$ to be the graded monad described in\n\\cref{E:graded-theory}.\\ref{item:simulation-a},\nand\n  \\begin{eqnarray*}\n    \\alpha_X\\colon\\pow_{\\mathsf f}(\\A\\times\n    X) & \\to & \\pow^{\\downarrow}_f(\\A\\times\\pow_{\\mathsf f} X) \\\\\n    S\\ & \\mapsto &\n    {\\downarrow}\\{(a,\\{x\\})\\mid (a,x)\\in S\\}\n  \\end{eqnarray*}\nwhere $\\downarrow$ takes downsets.\n\\end{enumerate}\n\\end{expl}\n\n\\begin{rem}\n  It follows from the depth-1 presentations described\n  in~\\cref{E:graded-theory} that the graded semantics mentioned in\n  \\cref{E:semantics} are depth-1.\n\\end{rem}\n\n\\section{Graded Algebras}\\label{sec:algebras}\nGraded monads come equipped with graded analogues\nof both the Eilenberg-Moore and Kleisli constructions for\nordinary monads. In particular, we have a\nnotion of \\emph{graded algebra}~\\cite{FKM16, MPS15}:\n\n\\begin{defn}[Graded algebra]\\label{D:gradedalgebra}\nLet $k\\in\\omega$ and let $\\M$ be a graded\nmonad on a category $\\CatC$. An \\emph{$M_k$-algebra}\n$A$ consists of a family of $\\CatC$-objects $(A_n)_{n\\leq k}$\n(the \\emph{carriers}) and a family of $\\CatC$-morphisms%\n\\begin{equation}\n  a^{n,m}\\colon M_nA_m\\to A_{n+m}\t\t\\tag{$n+m\\le k$}\n\\end{equation}\n(the \\emph{structure}) such that $a^{0,n}\\cdot\\eta_{A_n} = \\mathsf{id}_{A_n}$\n($n\\leq k$) and\n\\begin{equation}\\label{Diagram:gradedalgebralaw}\n  \\begin{tikzcd}[column sep = 35]\n    M_nM_mA_r \\arrow[d, \"\\mu^{n,m}\"'] \\arrow[r, \"M_na^{m,r}\"]\n    &\n    M_nA_{m+r} \\arrow[d, \"a^{n,m+r}\"] \\\\\n    M_{n+m}A_{r} \\arrow[r, \"a^{n+m,r}\"]\n    &\n    A_{n+m+r}\n  \\end{tikzcd}\n\\end{equation}\nfor all $n,m,r\\in\\omega$ such that $n+m+r\\leq k$. The \\emph{i-part} of\nan $M_k$-algebra $A$ is the $M_0$-algebra $(A_i, a^{0,i})$.\n\n\nA \\emph{homomorphism} from $A$ to an $M_k$-algebra\n$B$ is a family of $\\CatC$-morphisms\n$h_n\\colon A_n\\to B_n$ ($n\\leq k$) such that\n\\[\n  h_{n+m} \\cdot a^{n,m} = b^{n,m}\\cdot M_nh_m\n  \\quad\\text{for all $n,m\\in\\omega$ s.th.~$n+m\\le k$.}\n\\]\n  We\nwrite $\\Alg_k(\\M)$ for the category of $M_k$-algebras\nand their homomorphisms.\n\nWe define \\emph{$M_{\\omega}$-algebras} (and their\nhomomorphisms) similarly, by allowing the indices $n,m,r$\nto range over $\\omega$.\n\n\n\n\\end{defn}\n\n\\begin{rem}\n  The above notion of $M_{\\omega}$-algebra corresponds with the\n  concept of graded Eilenberg-Moore algebras introduced by Fujii et\n  al.~\\cite{FKM16}. Intuitively, $M_{\\omega}$-algebras are devices for\n  interpreting terms of unbounded uniform depth. We understand\n  $M_k$-algebras~\\cite{MPS15} as a refinement of $M_{\\omega}$-algebras\n  which allows the interpretation of terms of uniform depth \\emph{at\n  most~$k$}. Thus, $M_k$-algebras serve as a formalism for\n  specifying properties of states exhibited in \\emph{$k$ steps}. For\n  example, $M_1$-algebras are used to interpret one-step modalities of\n  characteristic logics for graded semantics~\\cite{DMS19,FMS21a}.\n  Moreover, for a depth-1 graded monad, its $M_\\omega$-algebras may\n  be understood as compatible chains of $M_1$-algebras~\\cite{MPS15},\n  and \n                               \n  a depth-1 graded monad can be reconstructed from its $M_1$-algebras.\n  %\n \n \n \n\\end{rem}\n\n\\noindent We will be chiefly interested in $M_0$- and $M_1$-algebras:\n\n\\begin{expl}\nLet $\\M$ be a graded monad on $\\mathbf{Set}$.\n\\begin{enumerate}\n\\item An $M_0$-algebra is just an Eilenberg-Moore algebra for the\n  monad $(M_0, \\eta, \\mu^{0,0})$.  It follows that $\\Alg_0(\\M)$ is\n  complete and cocomplete, in particular has coequalizers.\n\n\n\\item An $M_1$-algebra is a pair $((A_0, a^{0,0}), (A_1, a^{0,1}))$ of\n  $M_0$-algebras -- often we just write the carriers $A_i$ to also\n  denote the algebras, by abuse of notation -- equipped with a\n  \\emph{main structure map} $a^{1,0}\\colon M_1A_0\\to A_1$ satisfying\n  two instances of~\\eqref{Diagram:gradedalgebralaw}.  One instance\n  states that $a^{1,0}$ is an $M_0$-algebra homomorphism from\n  $(M_1A_0, \\mu^{0,1}_A)$ to $(A_1, a^{0,1})$ (\\emph{homomorphy}); the\n  other expresses that $a^{1,0}\\cdot \\mu^{1,0}= a^{1,0}\\cdot\n  M_1a^{0,0}$ (\\emph{coequalization}):\n  \\begin{equation}\\label{Diagram:coequalization}\n    \\begin{tikzcd}[column sep = 35]\n      M_1M_0A_0 \\arrow[r, \"\\mu^{1,0}\", shift left] \\arrow[r,\n      \"M_1a^{0,0}\"', shift right] & M_1A_0 \\arrow[r, \"a^{1,0}\"] & A_1.\n    \\end{tikzcd}\n  \\end{equation}\n \n \n\\end{enumerate}\n\\end{expl}\n\\begin{rem}\n  The free $M_n$-algebra on a set~$X$ is formed in the expected way,\n  in particular has\n  carriers~$M_0X,\\dots,M_nX$, see~\\cite[Prop.~6.3]{MPS15}.\n\\end{rem}\n\n\n\n\n\\paragraph*{Canonical algebras.}\nWe are going to review the basic definitions and results on\n\\emph{canonical $M_1$-algebras}~\\cite{DMS19}. Fix a graded\nmonad $\\M$ on $\\mathbf{Set}$.\n\nWe write $(-)_i\\colon\\Alg_1(\\M)\\to\\Alg_0(\\M)$, $i = 0,1$,\nfor the functor which sends an $M_1$-algebra $A$ to its $i$-part\n$A_i$ and sends a homomorphism $h\\colon A\\to B$ to\n$h_i\\colon A_i\\to B_i$.\n\n\\begin{defn}\\label{D:canonical}\nAn $M_1$-algebra~$A$ is \\emph{canonical} if it is free over its\n$0$-part with respect to $(-)_0\\colon\\Alg_1(\\M)\\to\\Alg_0(\\M)$.\n\\end{defn}\n\n\\begin{rem}\\label{rem:canonical}\n  The universal property of a canonical algebra~$A$ is the following:\n  for every $M_1$-algebra $B$ and every $M_0$-algebra homomorphism\n  $h\\colon A_0\\to B_0$, there exists a unique $M_1$-algebra\n  homomorphism $h^\\#\\colon A\\to B$ such that $(h^{\\#})_0 = h_0$.\n\\end{rem}\n\n\\begin{notheorembrackets}\n\\begin{lem}[{\\cite[Lem.~5.3]{DMS19}}]\\label{L:canonical-algebra}\nAn $M_1$-algebra $A$ is canonical if and only if\n  (\\ref{Diagram:coequalization}) is a coequalizer in $\\Alg_0(\\M)$.\n\\end{lem}\n\\end{notheorembrackets}\n\n\\begin{expl}\\label{E:canonical}\n  Let $X$ be a set and let $\\M$ be a depth-1 graded monad on $\\mathbf{Set}.$\n  For each ${k\\in\\omega}$, we may view $M_kX$ as an $M_0$-algebra with\n  structure $\\mu^{0,k}$. For the $M_1$-algebra $(M_kX, M_{k+1}X)$\n  (with main structure map $\\mu^{1,k}$), the instance of\n  Diagram~(\\ref{Diagram:coequalization}) required\n  by~\\cref{L:canonical-algebra} is a coequalizer\n  by~\\cref{L:depth-1}; that is, $(M_kX, M_{k+1}X, \\mu^{1,k})$ is\n  canonical.\n\\end{expl}\n\n\\section{Pre-Determinization in Eilenberg-Moore}\\label{sec:determinization}\nWe describe a generic notion of pre-determinization (the terminology\nwill be explained in \\cref{rem:determinization}) for coalgebras of\nan endofunctor $G$ on $\\mathbf{Set}$ with respect to a given depth-1 graded\nsemantics $(\\alpha,\\M)$, generalizing the Eilenberg-Moore-style\ncoalgebraic determinization construction by Silva et\nal.~\\cite{BBSR13}. The behavioural equivalence game introduced in the\nnext section will effectively be played on the pre-determinization of\nthe given coalgebra. We will occasionally gloss over issues of finite\nbranching in the examples.\n\nWe first note that every $M_0$-algebra $A$ extends (uniquely) to a\ncanonical $M_1$-algebra $EA$ (with $0$-part $A$), whose $1$-part and\nmain structure are obtained by taking the coequalizer of the pair of\nmorphisms in \\eqref{Diagram:coequalization} (canonicity then follows\nby~\\cref{L:canonical-algebra}). This construction forms the object\npart of a functor $\\Alg_0(\\M)\\to\\Alg_1(\\M)$ which sends a homomorphism\n$h\\colon A\\to B$ to its unique extension $Eh:=h^\\sharp\\colon EA\\to EB$\n(cf.~\\cref{rem:canonical}). We write $\\mybar{0.8}{2pt}{M}_1$ for the endofunctor on\n$\\Alg_0(\\M)$ given by\n\\begin{equation}\\label{eq:barM}\n\\mybar{0.8}{2pt}{M}_1:= (\\Alg_0(\\M)\\xra{E}\\Alg_1(\\M)\\xra{(-)_1}\\Alg_0(\\M)),\n\\end{equation}\nwhere $(-)_1$ is the functor taking $1$-parts.  Thus, for an\n$M_0$-algebra~$A_0$, $\\mybar{0.8}{2pt}{M}_1(A_0)$ is the vertex of the coequalizer\n\\eqref{Diagram:coequalization}.\n\nBy \\cref{E:canonical}, we have\n\\begin{equation}\n  \\label{eq:barM-can}\n  \\mybar{0.8}{2pt}{M}_1(M_kX,\\mu^{0,k}_X)=(M_{k+1}X,\\mu^{0,k+1}_X)\n\\end{equation}\nfor every set~$X$ and every $k\\in\\omega$. In particular,\n\\begin{equation}\\label{Eq:determinization}\n  U\\mybar{0.8}{2pt}{M}_1 F = M_1\n\\end{equation}\nwhere $F\\dashv U\\colon \\Alg_0(\\M) \\to \\mathbf{Set}$ is the canonical\nadjunction of the Eilenberg-Moore category of~$M_0$ -- that is,~$U$ is\nthe forgetful functor, and~$F$ takes free $M_0$-algebras, so\n$FX=(M_0X,\\mu^{00}_X)$. For an $M_1$-coalgebra\n$f\\colon X\\to M_1X=U\\mybar{0.8}{2pt}{M}_1 FX$, we therefore obtain a homomorphism\n$f^\\#\\colon FX\\to\\mybar{0.8}{2pt}{M}_1 FX$ (in $\\Alg_0(\\M)$) via adjoint\ntransposition. This leads to the following pre-determinization\nconstruction:\n\n\n\\begin{defn}\\label{D:determinization}\nLet $(\\alpha, \\M)$ be a depth-1 graded semantics on\n$G$-coalgebras.\nThe \\emph{pre-determinization} of a $G$-coalgebra\n$(X, \\gamma)$ under $(\\alpha, \\M)$\nis the $\\mybar{0.8}{2pt}{M}_1$-co\\-al\\-gebra\n\\begin{equation}\\label{eq:det}\n  (\\alpha_X\\cdot\\gamma)^\\#\\colon FX\\to \\mybar{0.8}{2pt}{M}_1 FX.\n\\end{equation}\n\\end{defn}\n\\begin{rem}\\label{rem:determinization}\n  \\begin{enumerate}\n  \\item\\label{item:predet-det} We call this construction a\n    \\emph{pre-}de\\-ter\\-mi\\-ni\\-zat\\-ion because it will serve as a\n    \\emph{determinization} -- in the expected sense that the\n    underlying graded equivalence transforms into behavioural\n    equivalence on the determinization -- only under additional\n    conditions. Notice that given a $G$-coalgebra $(X,\\gamma)$,\n    (finite-depth) behavioural equivalence on the $\\mybar{0.8}{2pt}{M}_1$-coalgebra\n    $(\\alpha_X\\cdot\\gamma)^*_0$ is given by the canonical cone into\n    the final chain\n    \\begin{equation*}\n      1 \\xla{!} \\mybar{0.8}{2pt}{M}_1 1 \\xla{\\overbar M_1 !} \\mybar{0.8}{2pt}{M}_1^{2}1 \\xla{\\overbar\n          M_1^2 !} \\cdots\n    \\end{equation*}\n    while graded behavioural equivalence on $(X,\\gamma)$ is given by\n    the maps $\\gamma^{(k)}$ into the sequence $M_01,M_11,M_21,\\dots$,\n    equivalently given as homomorphisms\n    $(\\gamma^{(k)})^*_0\\colon FX\\to(M_k1,\\mu^{0,k}_1)$, whose domains\n    can, by~\\eqref{eq:barM-can}, be written as the sequence\n    \\begin{equation*}\n      F1,\\quad \\mybar{0.8}{2pt}{M}_1 1,\\quad \\mybar{0.8}{2pt}{M}_1^21,\\quad \\ldots\n    \\end{equation*}\n    of $M_0$-algebras. The two sequences coincide in case $M_01=1$,\n    and indeed one easily verifies that in this case, finite-depth\n    behavioural equivalence on $\\mybar{0.8}{2pt}{M}_1$-coalgebras coincides with\n    $(\\alpha,\\M)$-behavioural equivalence.  For instance, this holds\n    in the case of probabilistic trace equivalence\n    (\\cref{E:semantics}.\\ref{item:sem-prob}), where $M_0=\\CalD$, so\n    $M_01=1$. In the case of trace equivalence\n    (\\cref{E:semantics}.\\ref{item:sem-trace}), $M_01=1$ can be\n    ensured by restricting to serial labelled transition systems,\n    which, as noted in \\cref{E:coalg}.\\ref{E:coalg:1}, are\n    coalgebras for $\\pow^+(\\A\\times -)$ with~$\\pow^+$ denoting\n    non-empty powerset, so that in the corresponding variant of the\n    graded monad for trace semantics, we have $M_0=\\pow^+$ and hence\n    $M_01=1$.\n\n    On the other hand, the condition $M_01=1$ fails for trace\n    equivalence of unrestricted systems where we have~\\mbox{$M_0=\\pow$,}\n   \n    which in fact\n    constitutes a radical example where behavioural equivalence on the\n    pre-determization is strictly coarser than the given graded\n    equivalence. In this case, since the actions preserve the\n    bottom~$0$, we in fact have~$\\mybar{0.8}{2pt}{M}_1 1=1$: it follows that \\emph{all}\n    states in $\\mybar{0.8}{2pt}{M}_1$-coalgebras are behaviourally equivalent (as\n    the unique coalgebra structure on~$1$ is final).\n\n\n\n   \n   \n   \n   \n   \n   \n   \n\n  \\item Using~\\eqref{Eq:determinization}, we see that the underlying\n    map of the pre-determinization of a coalgebra $(X,\\gamma)$ is\n    $(\\alpha_X \\cdot \\gamma)^*_0 \\colon M_0X \\to M_1 X = U_0\\mybar{0.8}{2pt}{M}_1 F_0\n    X$ (written using graded Kleisli star as\n    per~\\cref{N:star}). Indeed, one easily shows that\n    $(\\alpha_X \\cdot\\gamma)^*_0$ is an $M_0$-algebra\n    morphism\n    $(M_0 X, \\mu^{0,0}_X)\\to\\mybar{0.8}{2pt}{M}_1\n    (M_0X,\\mu^{0,0}_X)=(M_1X,\\mu^{0,1}_X)$ satisfying\n    $(\\alpha_X \\cdot \\gamma)^*_0 \\cdot \\eta_X = \\alpha_X \\cdot\n    \\gamma$. Thus, it is the adjoint transpose in~\\eqref{eq:det}.\n\n  \\item As indicated above, pre-de\\-ter\\-mi\\-ni\\-za\\-tion captures the\n    Eilenberg-Moore style generalized determinization by Silva et\n    al.~\\cite{BBSR13} as an instance. Indeed, for a monad $T$ and an\n    endofunctor $F$, both on the category $\\CatC$, one considers a\n    coalgebra $\\gamma\\colon X \\to FTX$. Assuming that $FTX$ carries\n    the structure of an Eilenberg-Moore algebra for $T$ (e.g.~because\n    the functor~$F$ lifts to the category of Eilenberg-Moore algebras\n    for $T$), one obtains an\n    $F$-coalgebra $\\gamma^\\sharp\\colon TX \\to FTX$ by taking the\n    unique homomorphic extension of $\\gamma$. Among the concrete\n    instances of this construction are the well-known powerset\n    construction of non-deterministic automata (take $T = \\pow$ and\n    $F = 2 \\times (-)^A$), the non-determinization of alternating\n    automata and that of Markov decision processes~\\cite{JSS15}.\n\n    To view this as an instance of pre-de\\-ter\\-mi\\-ni\\-za\\-tion, take\n    the graded monad with $M_n = F^nT$\n    (\\cref{E:graded-monad}.\\ref{E:graded-monad:4}), let $G = FT$,\n    and let $\\alpha = \\mathsf{id}_{FT}$. Using~\\eqref{Eq:determinization}, we\n    see that $(\\alpha_X \\cdot \\gamma)^\\#$ in~\\eqref{eq:det} is the\n    generalized determinization $\\gamma^\\sharp$ above.\n  \\item We emphasize that the construction applies completely\n    universally; e.g.~we obtain as one instance a `determinization' of\n    serial labelled transition systems modulo similarity, which\n    transforms a coalgebra $X\\to\\pow^+(\\A\\times X)$ into an\n    $\\mybar{0.8}{2pt}{M}_1$-coalgebra\n    $\\pow^+(X)\\to\\pow^{\\downarrow}(\\A\\times \\pow^+(X))$\n    (\\cref{E:graded-theory}.\\ref{item:simulation-a}); instantiating\n    the observations in item~\\ref{item:predet-det}, we obtain that\n    finite-depth behavioural equivalence of $\\mybar{0.8}{2pt}{M}_1$-coalgebras (see\n    \\cref{expl:barM} for the description of~$\\mybar{0.8}{2pt}{M}_1$) coincides with\n    finite-depth mutual similarity.\n  \\end{enumerate}\n\\end{rem}\n\n\\begin{expl}\\label{expl:barM}\n  We give a description of the functor~$\\mybar{0.8}{2pt}{M}_1$ on $M_0$-algebras\n  constructed above in some of the running examples.\n  \\begin{enumerate}[wide]\n  \\item For graded monads of the form $\\M_G$, which capture\n    finite-depth behavioural equivalence\n    (\\cref{E:graded-monad}.\\ref{E:graded-monad:1}), we have\n    $M_0=\\Id$, so $M_0$-algebras are just sets, and under this\n    correspondence, $\\mybar{0.8}{2pt}{M}_1$ is the original functor~$G$.\n\n  \\item\\label{item:mono-traces} Trace semantics of LTS\n    (\\cref{E:graded-theory}.\\ref{item:traces-a}): Distribution of\n    actions over the join semilattice operations ensures that depth-1\n    terms over a join semilattice~$X$ can be normalized to sums of the\n    form $\\sum_{a\\in \\A}a(x_a)$, with $x_a\\in X$ (possibly\n    $x_a=0$). It follows that $\\mybar{0.8}{2pt}{M}_1$ is simply given by\n    $\\mybar{0.8}{2pt}{M}_1 X=X^\\A$ ($\\A$-th power, where~$\\A$ is the finite set of labels).\n    Other forms of trace semantics are treated similarly.\n   \n   \n   \n   \n   \n   \n  \\item In the graded theory for simulation\n    (\\cref{E:graded-theory}.\\ref{item:simulation-a}), the\n    description of the induced graded monad~\\cite{DMS19} extends\n    analogously to~$\\mybar{0.8}{2pt}{M}_1$, yielding that $\\mybar{0.8}{2pt}{M}_1 B$ is the join\n    semilattice of finitely generated downwards closed subsets of\n    $\\A\\times B$ where, again,~$\\A$ carries the discrete\n    ordering.\n  \\end{enumerate}\n\\end{expl}\n\n\\begin{rem}\n \n \n \n \n  The assignment $\\M \\mapsto \\mybar{0.8}{2pt}{M}_1$ exhibits the category $\\mathscr{K}$ of\n  depth-1 graded monads whose $0$-part is the monad\n  $(M_0, \\eta, \\mu^{0,0})$ as a coreflective subcategory (up to\n  isomorphism) of the category $\\Fun(\\mathbf{Set}^{M_0})$ of all endofunctors\n  on the Eilenberg-Moore category of that monad.\n\n  Indeed, given an endofunctor $H$ on $\\mathbf{Set}^{M_0}$ we form the\n  $6$-tuple\n \n    $(M_0, UHF, \\eta, \\mu^{0,0}, \\mu^{0,1},\\mu^{1,0}),$\n \n  where the latter two natural transformations arise from the counit\n  $\\varepsilon\\colon FU \\to \\Id$ of the canonical adjunction $F\\dashv U\\colon\\Alg_0(\\M_0)\\to\\mathbf{Set}$:\n  \\begin{align*}\n    \\mu^{0,1} &= (M_0UHF = UFUHF \\xra{U\\varepsilon HF} UHF\\big);\\\\\n    \\mu^{1,0} &= (UHFM_0 = UHFUF \\xra{UHF\\varepsilon F} UHF\\big).\n  \\end{align*}\n  It is not difficult to check that this data satisfies all applicable\n  instances of the graded monad laws. Hence, it specifies a\n  depth-1 graded monad $R(H)$~\\cite[Thm.~3.7]{DMS19}; this\n  assignment is the object part of a functor $R\\colon\\Fun(\\mathbf{Set}^{M_0})\\to\\mathscr{K}$.\n\n  In the other direction, we have for each depth-1 graded monad $\\M$\n  with $0$-part $M_0$ the endofunctor $I(\\M) =\n  \\mybar{0.8}{2pt}{M}_1$. By~\\eqref{Eq:determinization}, we have $RI(\\M) = \\M$.\n \n \n \n \n \n \n \n  Now, given a depth-1 graded monad $\\M$ and an endofunctor $H$ on\n  $\\mathbf{Set}^{M_0}$, consider $\\mybar{0.8}{2pt}{M}_1 = IR(H)$ (so that $M_1 = UHF$). We\n  obtain for every algebra $(A,a)$ in $\\mathbf{Set}^{M_0}$ a homomorphism\n  $c_{(A,a)}\\colon \\mybar{0.8}{2pt}{M}_1 (A,a) \\to H(A,a)$ by using the coequalizer\n  defining $\\mybar{0.8}{2pt}{M}_1(A,a)$ (cf.~\\cref{L:canonical-algebra}):\n  \\[\n    \\begin{tikzcd}\n      M_1M_0 A\n      \\ar[yshift=2]{r}{\\mu^{0,1}_A}\n      \\ar[yshift=-2]{r}[swap]{M_1a}\n      &\n      M_1A=HFA\n      \\ar[->>]{r}\n      \\ar{rd}{Ha}\n      &\n      \\mybar{0.8}{2pt}{M}_1 (A,a)\n      \\ar[dashed]{d}{c_{(A,a)}}\n      \\\\\n      &&\n      H(A,a)\n    \\end{tikzcd}\n  \\]\n  Note that $M_1M_0A$ is the carrier of the Eilenberg-Moore algebra\n  $HFA = H(M_0A, \\mu^{0,0}_A)$ and similarly for the middle object (in\n  both cases we have omitted the algebra structures given by\n  $\\mu^{0,1}_{M_0 A}$ and $\\mu^{0,1}_A$ coming from the graded monad\n  $I(H)$).  It is easy to see that the homomorphism $Ha$ merges the\n  parallel pair, and therefore we obtain the dashed morphism such that\n  the triangle commutes, yielding the components of a natural\n  transformation $c\\colon \\mybar{0.8}{2pt}{M}_1 \\to H$ which is couniversal: for each\n  depth-1 graded monad $\\mathbb N$ whose $0$-part is $M_0$ and each\n  natural transformation $h\\colon \\mybar{0.8}{2pt}{M}_1 \\to H$, there is a unique\n  natural transformation $m_1\\colon N_1 \\to M_1 = UHF$ such that\n  $m = (id_{M_0}, m_1)$ is a morphism of graded monads from\n  $\\mathbb N$ to~$\\M$ and $c \\cdot I(m) = h$. This shows that $I\n  \\dashv R$.\n\\end{rem}\n\n\n\n\n\n\\section{Behavioural Equivalence Games}\\label{S:games}\nLet $\\CalS = (\\alpha, \\M)$ be a depth-1 graded semantics for an\nendofunctor $G$ on $\\mathbf{Set}$. We are going to describe a game for playing\nout depth-$n$ behavioural equivalence under $\\CalS$-semantics on\nstates in $G$-coalgebras.\n\nWe first give a description of the game in the syntactic language of\ngraded equational reasoning, and then present a more abstract\ncategorical definition. Given a coalgebra $(X,\\gamma)$, we will see\nthe states in~$X$ as variables, and the map $\\alpha_X\\cdot\\gamma$ as\nassigning to each variable~$x$ a depth-1 term over~$X$; we can regard\nthis assignment as a (uniform-depth) substitution~$\\sigma$. A\nconfiguration of the game is a pair of depth-0 terms over~$X$; to play\nout the equivalence of states $x,y\\in X$, the game is started from the\ninitial configuration $(x,y)$.  Each round of the game then proceeds\nin two steps: First, Duplicator plays a set~$Z$ of equalities between\ndepth-0 terms over~$X$ that she claims to hold under the\nsemantics. This move is admissible in the configuration $(s,t)$\nif~$Z\\vdash s\\sigma=t\\sigma$. Then, Spoiler challenges one of the\nequalities claimed by Duplicator, i.e.~picks an\nelement~$(s',t')\\in Z$, which then becomes the next configuration. Any\nplayer who cannot move, loses.  After~$n$ rounds have been played,\nreaching the final configuration $(s,t)$, Duplicator wins if\n$s\\theta =t\\theta$ is a valid equality, where~$\\theta$ is a\nsubstitution that identifies all variables. We refer to this last\ncheck as \\emph{calling the bluff}. Thus, the game plays out an\nequational proof between terms obtained by unfolding depth-0 terms\naccording to~$\\sigma$, cutting off after~$n$ steps.\n\nWe introduce some technical notation to capture the notion of\nadmissibility of~$Z$ abstractly:\n\\begin{notn}\\label{N:admissible}\n  Let $Z\\subseteq M_0X\\times M_0X$ be a relation, and let\n  $c_Z\\colon M_0X\\to C_Z$ be the coequalizer in $\\Alg_0(\\M)$ of the\n  homomorphisms $ \\ell_0^*, r_0^*\\colon M_0Z\\to M_0X $ given by\n  applying the Kleisli star~\\eqref{Eqn:Kleisli-star} to the\n  projections $\\ell, r\\colon Z\\to M_0X$. We define a homomorphism\n  $\\mybar{0.7}{1.75pt}{Z}\\colon M_0X\\to M_1C_Z$ in $\\Alg_0(\\M)$ by\n  \\begin{equation}\\label{eq:barZ}\n    \\mybar{0.7}{1.75pt}{Z} = \\big(M_0X\\xra{(\\alpha_X\\cdot\\gamma)^*_0} M_1X = \\mybar{0.8}{2pt}{M}_1 M_0X\n    \\xra{\\overbar M_1 c_Z} \\mybar{0.8}{2pt}{M}_1 C_Z\\big)\n  \\end{equation}\n  (omitting algebra structures, and again using the Kleisli star).\n \n \n\\end{notn}\n\n\\begin{rem}\\label{R:coeq}\n  Using designators as in \\cref{N:admissible}, we note:\n \n  \\begin{enumerate}\n  \\item\\label{R:coeq:1}\\label{R:coeq:2} By the universal property of\n    $\\eta_Z\\colon Z \\to M_0Z$, an $M_0$-algebra homomorphism\n    $h\\colon M_0X \\to A$ merges $\\ell, r$ iff it merges\n    $\\ell^*_0, r^*_0$. This implies that the coequalizer $M_0X \\xra{c_Z}C_Z$\n    quotients the free $M_0$-algebra $M_0X$ by the congruence\n    generated by~$Z$. Also, it follows that in case~$Z$ is already an\n    $M_0$-algebra and $\\ell, r\\colon Z \\to M_0X$ are $M_0$-algebra\n    homomorphisms (e.g.~when $Z$ is a congruence), one may take\n    $c_Z\\colon M_0X \\to C_Z$ to be the coequalizer of $\\ell, r$.\n\n  \\item\\label{item:barZ} The map $\\mybar{0.7}{1.75pt}{Z}\\colon M_0X\\to\\mybar{0.8}{2pt}{M}_1 C_Z$\n    associated to the relation $Z$ on $M_0X$ may be understood as\n    follows. As per the discussion above, we view the states of the\n    coalgebra $(X,\\gamma)$ as variables, and the map\n    $X\\xra{\\gamma} GX\\xra{\\alpha_X} M_1X$ as a substitution mapping a\n    state $x \\in X$ to the equivalence class of depth-1 terms encoding\n    the successor structure $\\gamma(x)$. The second factor $\\mybar{0.8}{2pt}{M}_1 c_Z$\n    in~\\eqref{eq:barZ} then essentially applies the relations given by\n    the closure of $Z$ under congruence w.r.t.~depth-0 operations,\n    embodied in~$c_Z$ as per~\\ref{R:coeq:1}, under depth-1 operations\n    in (equivalence classes of) of depth-1 terms in $M_1X$; to sum up,\n    $\\mybar{0.8}{2pt}{M}_1 c_Z$ merges a pair of equivalence classes $[t], [t']$ iff\n    $Z\\vdash t=t'$ in a depth-1 theory presenting $\\M$ (in notation as\n    per \\cref{sec:prelims}).\n   \n   \n   \n   \n   \n   \n  \\end{enumerate}\n\\end{rem}\n\n\\begin{defn}\\label{def:game}\n  For $n\\in\\omega$, the \\emph{$n$-round $\\CalS$-behavioural\n    equivalence game} $\\CalG_n(\\gamma)$ on a $G$-coalgebra\n  $(X, \\gamma)$ is played by Duplicator (D) and Spoiler\n  (S). \\emph{Configurations} of the game are pairs\n  $(s,t)\\in M_0(X)\\times M_0(X)$. Starting from an \\emph{initial\n    configuration} designated as needed, the game is played for~$n$\n  rounds. Each round proceeds in two steps, from the current\n  configuration~$(s,t)$: First, D chooses a relation\n  $Z\\subseteq M_0X\\times M_0X$ such that $\\mybar{0.7}{1.75pt}{Z}(s) = \\mybar{0.7}{1.75pt}{Z}(t)$\n  (for~$\\mybar{0.7}{1.75pt}{Z}$ as per \\cref{N:admissible}). Then,~S\n  picks an element~$(s',t') \\in Z$, which becomes the next configuration. Any\n  player who cannot move at his turn, loses. After~$n$ rounds have\n  been played,~D wins if $M_0!(s_n) = M_0!(t_n)$; otherwise,~S wins.\n \n \n \n \n \n \n \n \n \n\\end{defn}\n\n\\begin{rem}\n  By the description of~$\\mybar{0.7}{1.75pt}{Z}$ given in\n  \\cref{R:coeq}.\\ref{item:barZ}, the categorical definition of the\n  game corresponds to the algebraic one given in the lead-in\n  discussion. The final check whether $M_0!(s_n)=M_0!(t_n)$\n  corresponds to what we termed \\emph{calling the bluff}. The apparent\n  difference between playing either on depth-0 terms or on elements\n  of~$M_0X$, i.e.~depth-0 terms modulo derivable equality, is absorbed\n  by equational reasoning from~$Z$, which may incorporate also the\n  application of depth-0 equations.\n\\end{rem}\n\\begin{rem}\n  A pair of states coming from different coalgebras $(X,\\gamma)$ and\n  $(Y,\\delta)$ can be treated by considering those states as elements\n  of the coproduct of the two coalgebras:\n  \\[\n    X+Y \\xra{\\gamma + \\delta} GX + GY \\xra{[G\\mathsf{inl}, G\\mathsf{inr}]} G(X+Y),\n  \\]\n  where $X \\xra{\\mathsf{inl}} X+Y \\xla{\\mathsf{inr}} Y$ denote the coproduct\n  injections. There is an evident variant of the game played on two\n  different coalgebras $(X,\\gamma)$, $(Y,\\delta)$, where moves of~D\n  are subsets of $M_0X\\times M_0Y$. However, completeness of this\n  version depends on additional assumptions on~$\\M$, to be clarified\n  in future work. For instance, if we instantiate the graded monad for\n  traces with effects specified by~$T$\n  (\\cref{E:graded-monad}.\\ref{item:T-traces}) to~$T$ being the free\n  real vector space monad, and a state~$x\\in X$ has successor\n  structure $2\\cdot x'-2\\cdot x''$, then~D can support equivalence\n  between~$x$ and a deadlock~$y\\in Y$ (with successor structure~$0$)\n  by claiming that $x'=x''$, but not by any equality between terms\n  over~$X$ with terms over~$Y$. That is, in this instance, the variant\n  of the game where~D plays relations on $M_0X\\times M_0Y$ is not\n  complete.\n\\end{rem}\n\n\\noindent Soundness and completeness of the game with respect to\n$\\CalS$-behavioural equivalence is stated as follows.\n\n\n\\begin{thm}\\label{T:sound-complete}\n  Let $(\\alpha, \\M)$ be a depth-1 graded semantics for a functor~$G$\n  such that $\\mybar{0.8}{2pt}{M}_1$ preserves monomorphisms, and let $(X, \\gamma)$ be\n  a $G$-coalgebra. Then, for all $n\\in\\omega$,~D wins $(s, t)$ in\n  $\\CalG_n(\\gamma)$ if and only if\n  $(\\gamma^{(n)})^*_0(s) = (\\gamma^{(n)})^*_0(t)$.\n\\end{thm}\n\n\n\\begin{cor}\n  States $x,y$ in a $G$-coalgebra $(X, \\gamma)$ are\n  $\\CalS$-behaviourally equivalent if and only if~D wins\n  $(\\eta(x), \\eta(y))$ for all $n\\in\\omega$.\n\\end{cor}\n\n\\begin{rem}\\label{rem:monos}\n  In algebraic terms, the condition that~$\\mybar{0.8}{2pt}{M}_1$ preserves\n  monomorphisms amounts to the following: In the derivation of an\n  equality of depth-1 terms~$s,t$ over~$X$ from depth-0 relations\n  over~$X$ (i.e.~from a presentation of an $M_0$-algebra by relations\n  on generators~$X$), if~$X$ is included in a larger set~$Y$ of\n  variables with relations that conservatively extend those on~$X$,\n  i.e.~do not imply additional relations on~$X$, then it does not\n  matter whether the derivation is conducted over~$X$ or more\n  liberally over~$Y$. Intuitively, this property is needed because not\n  all possible $n$-step behaviours, i.e.~elements of~$Y=M_n1$, are\n  realized by states in a given coalgebra on~$X$. Preservation of\n  monos by~$\\mybar{0.8}{2pt}{M}_1$ is automatic for graded monads of the form $\\M_G$\n  (\\cref{E:graded-monad}.\\ref{E:graded-monad:1}), since $M_0=\\Id$\n  in this case. In the other running examples, preservation of monos\n  is by the respective descriptions of~$\\mybar{0.8}{2pt}{M}_1$ given in\n  \\cref{expl:barM}.\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n\\end{rem}\n\\begin{expl}\\label{expl:bisim-instance}\n  We take a brief look at the instance of the generic game for the\n  case of bisimilarity on finitely branching LTS (more extensive\n  examples are in \\cref{sec:cases}), i.e.~we consider the depth-1\n  graded semantics $(\\mathsf{id}, \\M_G)$ for the functor\n  $G=\\pow_{\\mathsf f}(\\A\\times(-))$. In this case, $M_0=\\Id$, so when playing on\n  a coalgebra $(X,\\gamma)$,~D plays relations~$Z\\subseteq X\\times\n  X$. If the successor structures of states~$x,y$ are represented by\n  depth-1 terms $\\sum_{i}a_i(x_i)$ and $\\sum_j b_j(y_j)$,\n  respectively, in the theory $\\JSL(\\A)$\n  (\\cref{E:graded-theory}.\\ref{item:jsl-a}), then~D is allowed to\n  play~$Z$ iff the equality $\\sum_{i}a_i(x_i)=\\sum_j b_j(y_j)$ is\n  entailed by~$Z$ in $\\JSL(\\A)$. This, in turn, holds iff for\n  each~$i$, there is~$j$ such that $a_i=b_j$ and $(x_i,y_j)\\in Z$, and\n  symmetrically. Thus~$Z$ may be seen as a pre-announced\n  non-deterministic winning strategy for~D in the usual bisimilarity\n  game where~S moves first (\\cref{sec:prelims}):~D announces that\n  if~S moves from, say,~$x$ to~$x_i$, then she will respond with\n  some~$y_j$ such that $a_i=b_j$ and $(x_i,y_j)\\in Z$.\n\\end{expl}\n\n\n\\section{Infinite-depth behavioural\n  equivalence}\\label{sec:infinte-depth}\n\n\\sloppypar\n\\noindent We have seen in \\cref{sec:determinization} that in case\n\\mbox{$M_01=1$}, $(\\alpha, \\M)$-behavioural equivalence\non~$G$-coalgebras coincides, via a determization construction, with\nfinite-depth behavioural equivalence on $\\mybar{0.8}{2pt}{M}_1$-coalgebras for a\nfunctor $\\mybar{0.8}{2pt}{M}_1$ on $M_0$-algebras constructed from~$\\M$. If~$G$ is\nfinitary, then finite-depth behavioural equivalence coincides with\nfull behavioural equivalence~(\\cref{rem:finite-depth}), but in\ngeneral, finite-depth behavioural equivalence is strictly\ncoarser. Previous treatments of graded semantics stopped at this\npoint, in the sense that for non-finitary functors (which describe\ninfinitely branching systems), they did not offer a handle on\ninfinite-depth equivalences such as full bisimilarity or\ninfinite-trace equivalence. In case $M_01=1$, a candidate for a notion\nof infinite-depth equivalence induced by a graded semantics arises via\nfull behavioural equivalence of $\\mybar{0.8}{2pt}{M}_1$-coalgebras. We fix this notion\nexplicitly:\n\\begin{defn}\n  States $x,y$ in a $G$-coalgebra $(X,\\gamma)$ are\n  \\emph{in\\-fin\\-ite-depth $(\\alpha,\\M$)-behaviourally equivalent}\n  if~$\\eta(x)$ and~$\\eta(y)$ are behaviourally equivalent in the\n  pre-det\\-er\\-min\\-i\\-za\\-tion of~$(X,\\gamma)$ as described in\n  \\cref{S:games}.\n\\end{defn}\n\\noindent We hasten to re-emphasize that this notion in general only\nmakes sense in case $M_01=1$. We proceed to show that infinite-depth\nequivalence is in fact captured by an infinite variant of the\nbehavioural equivalence game of \\cref{S:games}.\n\nSince infinite depth-equivalences differ from finite-depth ones only\nin settings with infinite branching, we do not assume in this section\nthat~$G$ or~$\\M$ are finitary, and correspondingly work with\ngeneralized graded theories where operations may have infinite\narities~\\cite{MPS15}; we assume arities to be cardinal numbers. We\ncontinue to be interested only in depth-1 graded monads and theories,\nand we fix such a graded monad~$\\M$ and associated graded theory for the\nrest of this section. The notion of derivation is essentially the same\nas in the finitary case, the most notable difference being that the\ncongruence rule is now infinitary, as it has one premise for each\nargument position of a given possibly infinitary operator. We do not\nimpose any cardinal bound on the arity of operations; if all\noperations have arity less than~$\\kappa$ for a regular\ncardinal~$\\kappa$, then we say that the monad is\n\\emph{$\\kappa$-ary}.\n\\begin{rem}\\label{R:final-coalg}\n  One can show using tools from the theory of locally presentable\n  categories that $\\mybar{0.8}{2pt}{M}_1$ has a final coalgebra if~$\\M$ is\n  $\\kappa$-ary in the above sense. To see this, first note that\n  $\\Alg_0(\\M)$ is locally $\\kappa$-presentable if $M_0$ is\n  $\\kappa$-accessible~\\cite[Remark~2.78]{AR94}. Using a somewhat\n  similar argument one can prove that $\\Alg_1(\\M)$ is also locally\n  $\\kappa$-presentable. Moreover, the functor $\\mybar{0.8}{2pt}{M}_1$ is\n  $\\kappa$-accessible, being the composite~\\eqref{eq:barM} of the left\n  adjoint $E\\colon \\Alg_0(\\M) \\to \\Alg_1(\\M)$ (which preserves all\n  colimits) and the $1$-part functor\n  $(-)_1\\colon \\Alg_1(\\M) \\to \\Alg_0(\\M)$, which preserves\n  $\\kappa$-filtered colimits since those are formed componentwise.\n  It follows that $\\mybar{0.8}{2pt}{M}_1$ has a final\n  coalgebra~\\cite[Exercise~2j]{AR94}. Alternatively, existence of a\n  final $\\mybar{0.8}{2pt}{M}_1$-coalgebra will follow from \\cref{thm:fin-coalg}\n  below. %\n\\end{rem}\n\n\\noindent\nLike before, we \\emph{assume that $\\mybar{0.8}{2pt}{M}_1$ preserves monomorphisms}.\n\n\\begin{expl}\n  We continue to use largely the same example theories as in\n  \\cref{E:graded-theory}, except that we allow operations to be\n  infinitary. For instance, the \\emph{graded theory of complete join\n    semilattices over~$\\A$} has as depth-1 operations all formal sums\n  $\\sum_{i\\in I}a_i(-)$ where~$I$ is now some (possibly infinite)\n  index set; the axioms are then given in the same way as in\n  \\cref{E:graded-theory}.\\ref{item:jsl-a}, and all depth-1\n  equations\n  \\[\n    \\textstyle \\sum_{i\\in I}a_i(x) = \\sum_{j\\in J} b_j(y)\n  \\]\n  such that  $\\{(a_i, x_i)\\mid i\\in I\\}=\\{(b_j, y_j)\\mid j\\in  J\\}$.\n  This theory presents the graded monad~$\\M_G$ for\n  $G=\\pow(\\A\\times(-))$.  \n\\end{expl}\n\n\\noindent The infinite game may then be seen as defining a notion of\nderivable equality on infinite-depth terms by playing out a\nnon-standard, infinite-depth equational proof; we will make this view\nexplicit further below. In a less explicitly syntactic version, the\ngame is defined as follows.\n\\begin{defn}[Infinite behavioural equivalence game]\n  The \\emph{infinite $(\\alpha,\\M$)-behavioural equivalence game}\n  $\\CalG_\\infty(\\gamma)$ on a $G$-coalgebra~$(X,\\gamma)$ is played by\n  Spoiler~(S) and Duplicator~(D) in the same way as the finite\n  behavioural equivalence game (\\cref{def:game}) except that the\n  game continues forever unless one of the players cannot move. Any\n  player who cannot move, loses. Infinite matches are won by~D.\n\\end{defn}\n\\noindent As indicated above, this game captures infinite-depth\n$(\\alpha,\\M)$-behavioural equivalence (under the running assumption\nthat~$\\mybar{0.8}{2pt}{M}_1$ preserves monomorphisms):\n\\begin{thm}\\label{thm:infinite-depth-games}\n  Given a $G$-coalgebra~$(X,\\gamma)$, two states~$s,t$ in the\n  pre-determinization of~$\\gamma$ are behaviourally equivalent iff D\n  wins the infinite $(\\alpha,\\M$)-behavioural equivalence game\n  $\\CalG_\\infty(\\gamma)$ from the initial configuration $(s,t)$.\n\\end{thm}\n\\begin{cor}\n  Two states $x,y$ in a $G$-coalgebra $(X,\\gamma)$ are infinite-depth\n  $(\\alpha,\\M)$-behaviourally equivalent iff D wins the infinite\n  $(\\alpha,\\M$)-behavioural equivalence game $\\CalG_\\infty(\\gamma)$\n  from the initial configuration $(\\eta(x),\\eta(y))$.\n\\end{cor}\n\\begin{rem}\n  Like infinite-depth $(\\alpha,\\M)$-behavioural equivalence, the\n  infinite $(\\alpha,\\M$)-behavioural equivalence game is sensible only\n  in case $M_01=1$. For instance, as noted in\n  \\cref{sec:determinization}, in the graded monad for trace\n  semantics (\\cref{E:graded-theory}.\\ref{item:pt-a}), which does not\n  satisfy this condition, behavioural equivalence of\n  $\\mybar{0.8}{2pt}{M}_1$-coalgebras is trivial. In terms of the game,~D wins every\n  position in $\\CalG_\\infty(\\gamma)$ by playing\n  $Z=\\{(t,0)\\mid t\\in M_0X\\}$ -- since the actions preserve the bottom\n  element~$0$, this is always an admissible move. In the terminology\n  introduced at the beginning of \\cref{S:games}, the reason that~D\n  wins in this way is that in the infinite game, her bluff is never\n  called ($M_0!(t)$ will in general not equal $M_0!(0)=0$).  However, see\n  \\cref{expl:inf-depth}.\\ref{item:inf-trace} below.\n\\end{rem}\n\n\\begin{expl}\\label{expl:inf-depth}\n  \\begin{enumerate}\n  \\item\\label{item:inf-trace} As noted in\n    \\cref{rem:determinization}.\\ref{item:predet-det}, the graded\n    monad for trace semantics can be modified to satisfy the\n    condition~$M_01=1$ by restricting to serial labelled transition\n    systems. In this case, infinite-depth $(\\alpha,\\M)$-behavioural\n    equivalence is precisely infinite trace equivalence, and captured\n    by the corresponding instance of the infinite behavioural\n    equivalence game.\n\n  \\item In the case of graded monads $\\M_G$\n    (\\cref{E:graded-monad}.\\ref{E:graded-monad:1}), which so far\n    were used to capture finite-depth behavioural equivalence in the\n    standard (branching-time) sense, we have $M_0=\\Id$; in particular,\n    $M_01=1$. In this case, the infinite-depth behavioural equivalence\n    game instantiates to a game that characterizes full behavioural\n    equivalence of $G$-coalgebras. Effectively, a winning strategy\n    of~D in the infinite game~$\\CalG_\\infty(\\gamma)$ on a\n    $G$-coalgebra $(X,\\gamma)$ amounts to a\n    relation~$R\\subseteq X\\times X$ (the positions of~D actually\n    reachable when~D follows her winning strategy) that is a\n    \\emph{precongruence} on~$(X,\\gamma)$~\\cite{AczelMendler89}.\n  \\end{enumerate}\n\\end{expl}\n\n\\begin{rem}[Fixpoint computation]\n  Via its game characterization (\\cref{thm:infinite-depth-games}),\n  infinite-depth $(\\alpha,\\M)$-behavioural equivalence can be cast as\n  a greatest fixpoint, specifically of the monotone function~$F$ on\n  $\\pow(M_0X\\times M_0X)$ given by\n  \\begin{equation*}\n    F(Z)=\\{(s,t)\\in M_0X\\times M_0X\\mid \\mybar{0.7}{1.75pt}{Z}(s)=\\mybar{0.7}{1.75pt}{Z}(t)\\}.\n  \\end{equation*}\n  If~$M_0$ preserves finite sets, then this fixpoint can be computed\n  on a finite coalgebra $(X,\\gamma)$ by fixpoint iteration; since\n  $F(Z)$ is clearly always an equivalence relation, the iteration\n  converges after at most $|M_0X|$ steps, e.g.~in exponentially many\n  steps in case $M_0=\\pow$. In case $M_0X$ is infinite (e.g.~if\n  $M_0=\\CalD$), then one will need to work with finite representations\n  of subspaces of~$M_0X\\times M_0X$. We leave a more careful analysis\n  of the algorithmics and complexity of solving infinite\n  $(\\alpha,\\M)$-behavioural equivalence games to future work. We do\n  note that on finite coalgebras, we may assume w.l.o.g.~that both the\n  coalgebra functor~$G$ and graded monad~$\\M$ are finitary, as we can\n  replace them with their finitary parts if needed (e.g.~the powerset\n  functor $\\pow$ and the finite powerset functor~$\\pow_{\\mathsf f}$ have\n  essentially the same finite coalgebras). If additionally~$M_01=1$,\n  then~$(\\alpha,\\M)$-behavioural equivalence coincides with\n  infinite-depth $(\\alpha,\\M)$-behavioural equivalence, so that we\n  obtain also an algorithmic treatment of $(\\alpha,\\M)$-behavioural\n  equivalence. By comparison, such a treatment is not immediate from\n  the finite version of the game, in which the number of rounds is\n  effectively chosen by Spoiler in the beginning.\n\\end{rem}\n\\noindent Assume from now on that $\\M$ is $\\kappa$-ary.\nWe note that in this case, we can describe the final $\\mybar{0.8}{2pt}{M}_1$-coalgebra\nin terms of a syntactic variant of the infinite game that is played on\ninfinite-depth terms, defined as follows.\n\\begin{defn}[Infinite-depth terms]\n  Recall that we are assuming a graded signature~$\\Sigma$ with\n  operations of arity less than~$\\kappa$. A \\emph{(uniform)\n    infinite-depth \\mbox{($\\Sigma$-)}term} is an infinite tree with\n  ordered branching where each node is labelled with an\n  operation~$f\\in\\Sigma$, and then has as many children as given\n  by the arity of~$f$; when there is no danger of confusion, we\n  will conflate nodes with (occurrences of) operations. We require\n  moreover that every infinite path in the tree contains infinitely\n  many depth-1 operations (finite full paths necessarily end in\n  constants). We write $\\Termsarg{\\Sigma,\\infty}$ for the set of\n  infinite-depth $\\Sigma$-terms. By cutting off at the top-most\n  depth-1 operations, we obtain for every $t\\in\\Termsarg{\\Sigma,\\infty}$\n  a \\emph{top-level decomposition} $t=t_1\\sigma$ into a depth-1 term\n  $t_1\\in\\Termsarg{\\Sigma,1}(X)$, for some set~$X$, and a substitution\n  $\\sigma\\colon X\\to\\Termsarg{\\Sigma,\\infty}$.\n\\end{defn}\n\n\\begin{defn}\n  The \\emph{syntactic infinite $(\\alpha,\\M$)-behavioural equivalence\n    game} $\\CalG^\\mathsf{syn}_\\infty$ is played by~S and~D. Configurations of\n  the game are pairs $(s,t)$ of infinite-depth $\\Sigma$-terms. For\n  such $(s,t)$, we can assume, by the running assumption that~$\\mybar{0.8}{2pt}{M}_1$\n  preserves monomorphisms, that the top level decompositions\n  $s=s_1\\sigma$, $t=t_1\\sigma$ are such that\n  $s_1,t_1\\in\\Termsarg{\\Sigma,1}(X)$,\n  $\\sigma\\colon X\\to\\Termsarg{\\Sigma,\\infty}$ for the\n  same~$X,\\sigma$. Starting from a designated initial configuration,\n  the game proceeds in rounds. In each round, starting from a current\n  such configuration~$(s,t)$,~D first chooses a\n  relation~$Z\\subseteq\\Termsarg{\\Sigma,0}(X)\\times\\Termsarg{\\Sigma,0}(X)$\n  such that $Z\\vdash s_1 = t_1$ in the graded theory that\n  presents~$\\M$ (cf.~\\cref{sec:prelims}). \\mbox{Then, S} selects an\n  element $(u,v)\\in Z$, upon which the game reaches the new\n  configuration $(u\\sigma,v\\sigma)$. The game proceeds forever unless\n  a player cannot move. Again, any player who cannot move, loses, and\n  infinite matches are won by~D. We write $s\\sim_{\\mathcal{G}} t$ if~D\n  wins~$\\CalG^\\mathsf{syn}_\\infty$ from position $(s,t)$.\n\\end{defn}\n\\noindent We construct an $\\mybar{0.8}{2pt}{M}_1$-coalgebra on the set\n\\(\nU=\\Termsarg{\\Sigma,\\infty}/{\\sim_{\\mathcal{G}}}\n\\)\nof infinite-depth terms modulo the winning region of~D as follows. We\nmake~$U$ into an $M_0$-algebra by letting depth-0 operations act by\nterm formation. We then define the coalgebra\nstructure~$\\zeta\\colon U \\to \\mybar{0.8}{2pt}{M}_1 U$ by\n\\(\n  \\zeta(q(t_1\\sigma)) = \\mybar{0.8}{2pt}{M}_1((q\\cdot\\sigma)^*_0)([t_1])\n\\)\n(using Kleisli star as per \\cref{N:star}) where\n$t_1\\sigma$ is a top-level decomposition of an infinite-depth term,\nwith $t_1\\in\\Termsarg{\\Sigma,1}(X)$;\n\\[\n  [-]\\colon  \\Termsarg{\\Sigma,1}(X)\\to M_1X=\\mybar{0.8}{2pt}{M}_1 M_0 X\n  \\quad\\text{and}\\quad\n  q\\colon \\Termsarg{\\Sigma,\\infty}\\to U\n\\]\ndenote canonical quotient maps.\nThese data are well-defined.%\n\\begin{thm}\\label{thm:fin-coalg}\n  The coalgebra $(U,\\zeta)$ is final.%\n\\end{thm}\n\n\n\n\n\\section{Case studies}\\label{sec:cases}\nWe have already seen (\\cref{expl:bisim-instance}) how the standard\nbisimilation game arises as an instance of our generic game. We\nelaborate on some further examples. %\n\\takeout{\\subsection{Trace equivalence} In this subsection, we\n  illustrate the admissible moves of Duplicator in concrete terms and,\n  subsequently, establishing the game-theoretic characterisation of\n  trace equivalence.\n\\bknote{Example~2.9 does not talk about trace equivalence, we should explain the connection better.}\nTo this end, consider an LTS $\\gamma\\colon X\\to\\pow_{\\mathsf f}(\\A\\times X)$\nand recall the depth-1 graded monad attributed to trace equivalence\nfrom \\cite{MPS15}.\n\n\\takeout{In particular, $\\Sigma=\\{\\bot,\\vee\\} \\cup \\{a.\\_ \\mid a\\in \\A\\}$ with action prefixing as the only depth-1 operation; $\\E$ has all the axioms of join semilattice and the following depth-1 equations.\n\\[\na.\\bot = \\bot \\qquad a.(x\\vee y) = a.x \\vee a.y\n\\]}\n\n\n\\takeout{We begin by stating the coequaliser $C_Z,c_Z$ for a given relation $Z$ (cf.\\thinspace Notation~\\ref{N:admissible}) in more concrete terms. Define $Z' \\subseteq M_1 X \\times M_1 X$ as follows:\n\\[\nt \\mathrel {Z'} t' \\iff \\exists_{s\\in M_1 Z}\\ l_1^*s =t \\land r_1^*s=t'.\n\\]\nThen the object $C_Z$ is given by the quotient set $M_1X/ Z''$, where $Z''$ is the least equivalence which is closed with respect to all the depth-0 operations and which includes $Z'$. In short, we say that $Z''$ is the \\emph{congruence closure} of $Z'$.\n\\begin{propn}\n  The object $C_Z$ together with its quotient map $c_Z$ indeed forms the coequaliser of the arrows $l_1^*,r_1^*$.\n\\end{propn}}\n\\begin{expl}\n  Consider the following process terms (defined in a fragment of CCS\n  signature):\n  \\[\n    p_1\\equiv a.p_1',\\ p_2\\equiv a.p_2' + b.p_2'',\\ p_3\\equiv b.p_3',\n  \\]\n  where $p_1',p_2',p_2'',p_3'$ are all inaction (i.e.~the constant $\\0$) and $\\equiv$ denotes the syntactic equality. Clearly we find that the set $s=\\{p_1,p_2\\}$ and $t=\\{p_2,p_3\\}$ are trace equivalent.\n\n  In particular, $s,t$ have the same traces of length 1, which we argue next through our game. Duplicator plays the relation $Z$ viewed as a set of equations between $M_0$-terms:\n  \\[\n  Z = \\{p_1' + p_2' = p_2',\\ p_3' + p_2'' = p_2'' \\}.\n  \\]\n  We claim that the relation $Z$ is admissible at position $(s,t)$ because the $M_1$-terms $(\\alpha\\cdot\\gamma)^\\# s=a.p_1' + a.p_2' + b.p_2''$ and $(\\alpha\\cdot\\gamma)^\\# t=a.p_2' + b.p_2'' + b.p_3'$ are related in the congruence closure $Z''$ of $Z'$. To establish this first observe that\n  \\[\n  (a.p_1' + a.p_2') \\mathrel {Z'} a.p_2'\\ \\text{and}\\ b.p_2''\\mathrel {Z'} (b.p_3' + b.p_2'').\n  \\]\n  Clearly $(\\alpha\\cdot\\gamma)^\\# s\\mathrel {Z''} (\\alpha\\cdot\\gamma)^\\# t$. Now Spoiler can pick either of the two equations in $Z$. Moreover, both pairs $(\\{p_1',p_2'\\},\\{p_2'\\})$ and $(\\{p_3',p_2''\\},\\{p_2''\\})$ are mapped to a common point (the singleton containing empty trace) by $M_1!$; thus, resulting in two 1-round matches both won by Duplicator.\n\\end{expl}\n\\begin{propn}\n  Every $M_1$-term $t$ is derivably equivalent to a term of the form $\\bigvee_{a}a.t_a$, where $t_a$ is some $M_0$-term.\n\\end{propn}\n\\begin{propn}\n  Suppose the normal form of $M_1$-terms $(\\alpha\\cdot\\gamma)^*_0\\Box$ is $\\bigvee_{a} a.\\Box_a$ (for $\\Box\\in\\{s,t\\}$). Then a relation $Z$ on $M_0X$ satisfying the following conditions for each action $a$ is admissible at $(s,t)$.\n  \\begin{enumerate}\n    \\item $\\forall_{x\\in s_a}\\exists_{t'}\\ t'\\subseteq t_a \\land \\left((t',x\\vee t') \\in Z \\lor (x\\vee t',t')\\in Z \\right)$, and\n    \\item $\\forall_{y\\in t_a}\\exists_{s'}\\ s'\\subseteq s_a \\land \\left((s',y\\vee s') \\in Z \\lor (y\\vee s',s')\\in Z\\right)$.\n  \\end{enumerate}\n\\end{propn}\n\\begin{proof}\n  To show admissibility of $Z$ at $(s,t)$, it suffices to show that $\\T+Z \\vdash s_a=t_a$. Condition 1 ensures that\n  \\[\n  \\T+Z \\vdash s_a \\lor t_a = \\bigvee_{x\\in s_a} x \\vee t_a = t_a.\n  \\]\n  Likewise Condition 2 ensures that $\\T+Z \\vdash s_a \\lor t_a = s_a$. Thus, $\\T +Z \\vdash s_a=t_a$.\n\\end{proof}\nA natural question to ask is whether there exists an algorithm to\ndetermine whether $s,t$\\hbnote{I changed $U,V$ to $s,t$ inline with the notation in Definition~\\ref{def:game}.} are in the congruence closure of $Z$. In fact\nthere are algorithms to do this for the powerset monad $\\pow_{\\mathsf f}$\n\\cite{bp:checking-nfa-equiv} and for certain semiring monads\n\\cite{bkk:up-to-weighted}. The idea behind those algorithms is to\nobtain rewriting rules from the pairs of $Z$ and two elements are in\nthe congruence closure iff they can be rewritten to the same normal\nform.}\n\n\\paragraph*{Simulation equivalence.}\nWe illustrate how the infinite $(\\alpha,\\M$)-behavioural equivalence\ngame can be used to characterise simulation\nequivalence~\\cite{Glabbeek90} on serial LTS. We have described the\ngraded theory of simulation\nin~\\cref{E:semantics}.\\ref{item:sem-sim}.  Recall that it requires\nactions to be monotone, via the depth-1 equation\n\\( a(x + y) = a(x+ y) + a(x).  \\) When trying to show that depth-1\nterms $\\sum_{i\\in I}a_i(t_i)$ and $\\sum_{j\\in J}b_j(s_k)$ are \\mbox{equal, D}\nmay exploit that over join semilattices, inequalities can be expressed\nas equalities ($x\\le y$ iff $x+y=y$), and instead endeavour to show\ninequalities in both directions. By the monotonicity of actions,\n$\\sum_{i\\in I}a_i(t_i)\\le \\sum_{j\\in J}b_j(s_k)$ is implied by~D\nclaiming, for each~$i$, that $t_i\\le s_j$ for some~$j$ such that\n$a_i=b_j$; symmetrically for $\\ge$ (and by the description of the\nrelevant graded monad as per\n\\cref{E:graded-theory}.\\ref{item:simulation-a}, this proof\nprinciple is complete). Once~S challenges either a claim of the form\n$t_i\\le s_j$ or one of the form $t_i\\ge s_j$, the direction of\ninequalities is fixed for the rest of the game; this corresponds to\nthe well-known phenomenon that in the standard pebble game for\nsimilarity,~S cannot switch sides after the first move. Like for\nbisimilarity (\\cref{expl:bisim-instance}), the game can be modified\nto let~S move first:~S first picks, say, one of the terms~$t_i$, and~D\nresponds with an~$s_j$ such that~$a_i=b_j$, for which she claims\n$t_i\\le s_j$. Overall, the game is played on positions in\n$\\pow^+(X)\\times\\pow^+(X)$, but if started on two states~$x,y$ of the\ngiven labelled transition systems, i.e.~in a position of the form\n$(\\{x\\},\\{y\\})$, the game forever remains in positions where both\ncomponents are singletons, and thus is effectively played on pairs of\nstates. Summing up, we recover exactly the usual pebble game for\nmutual similarity. Variants such as complete, failure, or ready\nsimulation are captured by minor modifications of the graded\nsemantics~\\cite{DMS19}.\n\n\n\n\n\\paragraph*{T-structured trace equivalence.}\nFix a set $\\A$ and a finitary monad $T$ on $\\mathbf{Set}$. We are going to\nconsider the $(\\mathsf{id}, \\M_T(\\A))$-behavioural equivalence game on\ncoalgebras for the functor $T(\\A\\times -)$\n(cf.~\\cref{E:graded-monad}.\\ref{item:T-traces}).\n\n\\begin{notn}\n  Fix a presentation $(\\Sigma', E')$ of $T$ (i.e.~an equational theory\n  in the sense of universal algebra).  We generalize the graded trace\n  theory described in \\cref{E:graded-theory}.\\ref{item:traces-a} to\n  a graded theory $\\T=(\\Sigma,\\E)$ for $\\M_T(\\A)$ as follows:\n  $(\\Sigma', E')$ forms the depth-0 part of $\\T$ and, at depth-1, $\\T$\n  has unary actions $a(-)$ which distribute over all operations\n  $f\\in \\Sigma'$:\n\\[\na(f(s_1,\\cdots,s_{\\mathsf{ar}(f)}))= f(a (s_1),\\cdots,a(s_{\\mathsf{ar}(f)}))\n\\]\nThe arising theory $\\T$ presents $\\M_T(\\A)$.\n\\end{notn}\n\n\\noindent Recall from~\\cref{R:coeq}.\\ref{R:coeq:1} that since, in\nthis setting, $M_0=T$, a legal move for~D in position\n$(s,t)\\in TX\\times TX$ is a relation $Z$ on $TX$ such\nthat equality of the respective successors~$(\\alpha_X\\cdot \\gamma)^*_0(s)$\nand $(\\alpha_X\\cdot \\gamma)^*_0(t)$, viewed\nas (equivalence classes of) depth-1 terms, is derivable in the theory\n$\\T$ under assumptions~$Z$.\n\n\\begin{rem}\n  A natural question is whether there exist algorithms for deciding if\n  a pair $(\\alpha_X\\cdot \\gamma)^*_0(s),(\\alpha_X\\cdot \\gamma)^*_0(t)$\n  sits in the congruence closure of $Z$.  In fact, there are\n  algorithms to check congruence closure of depth-0 terms for the\n  powerset monad $T=\\pow_{\\mathsf f}$~\\cite{bp:checking-nfa-equiv} and for\n  certain semiring monads~\\cite{bkk:up-to-weighted}.  The idea behind\n  those algorithms is to obtain rewrite rules from pairs in $Z$, and\n  two elements are in the congruence closure if and only if they can\n  be rewritten to the same \\emph{normal form}. Applying depth-1\n  equations to normal forms could potentially yield a method to check\n  automatically whether a given pair of $M_1$-terms lies in the\n  congruence closure of $Z$.\n\\end{rem}\n\n\\paragraph*{Finite-trace equivalence.}\nMore concretely, we examine the behavioural equivalence game for trace\nequivalence on finitely branching LTS (i.e.\n$(\\mathsf{id}, \\M_{\\pow_{\\mathsf f}}(\\A))$-semantics as\nper~\\cref{E:semantics}.\\ref{item:sem-trace}).\n\n\n\\begin{expl}\n  Consider the following process terms representing a coalgebra\n  $\\gamma$ (in a fragment of CCS):\n  \\[\n  p_1\\equiv a.p_1';\n  \\quad\n  p_2\\equiv a.p_2' + b.p_2'';\n  \\quad\n  p_3\\equiv b.p_3',\n  \\]\n  where $p_1',p_2',p_2'',p_3'$ are deadlocked. It is easy to see that\n  $s=\\{p_1,p_2\\}$ and $t=\\{p_2,p_3\\}$ are trace equivalent: In\n  particular, $s,t$ have the same traces of length 1. We show that~D\n  has a winning strategy in the 1-round\n  $(\\mathsf{id}, \\M_{\\pow_{\\mathsf f}}(\\A))$-behavioural equivalence game at\n  $(s,t)$. Indeed, the relation\n  \\( Z := \\{p_1' + p_2' = p_2',\\ p_3' + p_2'' = p_2'' \\}, \\)\n  is admissible at~$(s, t)$: We must show that equality of\n  $(\\alpha\\cdot\\gamma)^\\#(s)=a(p_1') + a(p_2') + b(p_2'')$\n  and~$(\\alpha\\cdot\\gamma)^\\#(t)=a(p_2') + b(p_2'') + b(p_3')$ is\n  entailed by~$Z$. To see this, note that\n  \\[\n  Z\\vdash a(p_1') + a(p_2') = a(p_2')\\ \\text{ and }\\ Z\\vdash b(p_2'') = b(p_3') + b(p_2'').\n  \\]\n  Moreover, the pairs $(\\{p_1',p_2'\\},\\{p_2'\\})$\n  and~$(\\{p_3',p_2''\\},\\{p_2''\\})$ are both identified by~$M_0!$ (all\n  terms are mapped to $\\{\\epsilon\\}$ when $1=\\{\\epsilon\\}$).  That\n  is,~$Z$ is a winning move for~D.\n\\end{expl}\n\\noindent\nIn general, admissible moves of~D can be described via a normalisation\nof depth-1 terms as follows:\n\n\\begin{propn}\\label{prop:trace-nf}\n  In $\\M_{\\pow_{\\mathsf f}}(\\A)$, every depth-1 term is derivably equal to one of\n  the form $\\sum_{a\\in \\A}a (t_{a})$, with depth-0 terms (i.e.~finite,\n  possibly empty, sets)~$t_{a}$. Over serial LTS (i.e.~$T=\\pow_{\\mathsf f}^+$),\n  every depth-1 term has a normal form of the shape\n  $\\sum_{a\\in B}a (t_{a})$ with $B\\in\\pow_{\\mathsf f}^+\\A$ (where the~$t_a$ are\n  now finite and non-empty).\n\\end{propn}\n\\begin{propn}\\label{prop:WStratTrace}\n  Let $\\rho=\\sum_{a\\in\\A} a(\\rho_a)$ be depth-1 terms over~$X$ in\n  normal form, for $\\rho\\in\\{s,t\\}$. Then a relation\n  $Z\\subseteq \\pow_{\\mathsf f} X\\times\\pow_{\\mathsf f} X$\n \n  is a legal move of D in position $(s,t)$ iff\n  the following conditions hold for all $a\\in\\A$, in the notation\n  of~\\cref{prop:trace-nf}:\n  \\begin{enumerate}\n    \\item $\\forall x\\in s_{a}.\\ \\exists {t'}(t'\\subseteq t_{a} \\land Z\\vdash x\\leq t')$\n    \\item $\\forall {y\\in t_{a}}.\\ \\exists {s'}(s'\\subseteq s_{a} \\land Z\\vdash y\\leq s')$\n  \\end{enumerate}\n  where, again, $s\\leq t$ abbreviates $s+ t=t$. Over serial LTS\n  (i.e.~$T=\\pow_{\\mathsf f}^+$), and for normal forms\n  $\\rho=\\sum_{a\\in B_\\rho} a(\\rho_a)$, a relation\n  $Z\\subseteq \\pow_{\\mathsf f}^+X \\times \\pow_{\\mathsf f}^+X$\n \n  is a legal move of D in position~$(s,t)$\n  iff~$B_s=B_t$ and the above conditions hold for all~$a\\in B_s$.\n\\end{propn}\n\\noindent To explain terminology, we note at this point that by the\nabove, in particular $Z=\\{(x,0)\\mid x\\in X\\}\\cup\\{(0,y)\\mid y\\in X\\}$\nis always admissible. Playing~$Z$,~D is able to move in every\nround, bluffing her way through the game; but this strategy does not\nwin in general, as her bluff is called at the end\n(cf.~\\cref{S:games}). More reasonable strategies work as follows.\n\nOn the one hand,~D can restrict herself to playing the bisimulation\nrelation on the determinised transition system because the term $s'$\n(resp. $t'$) can be taken to be exactly $s_a$ (resp. $t_a$) in\nCondition~2 (resp. Condition~1). This form of the game may be recast\nas follows. Each round consists just of~S playing some~$a\\in\\A$ (or\n$a\\in B_s$ in the serial case), moving to $(s_a,t_a)$ regardless of\nany choice by~D. In the non-serial case, the game runs until the bluff\nis called after the last round. In the serial case, D wins if either\nall rounds are played or as soon as~$B_s=B_t=\\emptyset$, and~S wins as\nsoon as $B_s\\neq B_t$.\n\nOn the other hand, D may choose to play in a more fine-grained manner,\nplaying one inequality $x\\le t'$ for every $x\\in s_a$ and one\ninequality $s'\\ge y$ for every $y\\in t_a$. Like in the case of\nsimulation, the direction of inequalities remains fixed after~S\nchallenges one of them, and the game can be rearranged to let~S move\nfirst, picking, say, $x\\in s_a$ (or symmetrically), which~D answers\nwith $t'\\subseteq t_a$, reaching the new position $x\\le t'$. The game\nthus proceeds like the simulation game, except that~D is allowed to\nplay sets of states.\n\n\n\n\n\n\n\n\\paragraph*{Probabilistic traces} These are treated similarly as traces in\nnon-deterministic LTS: Every depth-1 term can be normalized into one\nof the form\n $ \\sum_{\\A} p_a\\cdot a(t_a)$,\nwhere $\\sum_{\\A}p_a=1$ and the~$t_a$ are depth-0 terms. To show\nequality of two such normal forms $\\sum_{a\\in\\A} p_a\\cdot a(t_a)$ and\n$\\sum_{a\\in\\A} q_a\\cdot a(s_a)$ (arising as successors of the current\nconfiguration),~D needs to have $p_a=q_a$, and then claim~$t_a=s_a$,\nfor all $a\\in\\A$. Thus, the game can be rearranged to proceed like the\nfirst version of the trace game described above:~S selects~$a\\in\\A$,\nand wins if~$p_a\\neq q_a$ (and the game then reaches the next\nconfiguration~$(t_a,s_a)$ without intervention by~D).\n\n\\paragraph*{Failure equivalence.}\nLet $\\gamma\\colon X \\to \\pow_{\\mathsf f}(\\A \\times X)$ be an LTS.\nA tuple $(w,B)\\in \\A^* \\times \\pow_{\\mathsf f}(\\A)$ is a \\emph{failure pair} of a state $x$ if there is a $w$-path from $x$ to a state $x'\\in X$\nsuch that $x'$ \\emph{fails} to perform some action $b\\in B$ (the\n\\emph{failure set}). Two states are failure equivalent iff\nthey have the same set of failure pairs.\n\nThe \\emph{graded theory of failure semantics}~\\cite{DMS19}\nextends the graded theory of traces by de\\-pth-1 constants~$A$ for\neach~$A\\in\\pow_f(\\A)$ (failure sets) and depth-1\nequations~$A+ (A\\cup B) = A$ for each~$A,B\\in\\pow_f(\\A)$\n(failure sets are downwards closed). The resulting graded monad~\\cite{DMS19}\nhas~$M_0X=\\pow_{\\mathsf f} X$ and~${M_1X=\\pow_{\\mathsf f}^\\downarrow(\\A\\times X + \\pow_{\\mathsf f} \\A)}$,\nwhere~$\\pow_{\\mathsf f} \\A$ is ordered by inclusion,~$\\A\\times X$ carries the discrete order, and\n$\\pow_{\\mathsf f}^\\downarrow$ denotes the finitary downwards-closed powerset.\nIt is clear that $\\mybar{0.8}{2pt}{M}_1$ still preserves monos since we have only expanded\nthe theory of traces by constants. The game in general\nis then described similarly as the one for plain traces above; the key\ndifference is that now~S is able to challenge whether a pair of failure\nsets are matched up to downwards closure.\n\n\\begin{expl}\n  Consider the following process terms with $\\A=\\{a,b,c\\}$:\n  $\n  p_1\\equiv a.\\mathbf 0,\\ p_2\\equiv a.\\mathbf 0+b.\\mathbf 0,\\ p_3\\equiv b.\\mathbf 0.\n  $\n\n  Clearly, the states $s=\\{p_1,p_2,p_3\\}$ and $t=\\{p_1,p_3\\}$ in the pre-determinized system are failure equivalent. To reason this through our game $\\CalG_\\infty(\\gamma)$, Duplicator starts with the relation $Z=\\{(\\mathbf 0,\\mathbf 0)\\}$. From~$Z$, we derive\n  \\begin{align*}\n     (\\alpha_X\\cdot \\gamma)^*_0 s &= a(\\mathbf 0) + b(\\mathbf 0) +  {\\downarrow}\\{b,c\\} + {\\downarrow}\\{c\\} + {\\downarrow}\\{a,c\\} \\\\\n    &=  a(\\mathbf 0) + b(\\mathbf 0) +   {\\downarrow}\\{b,c\\} + {\\downarrow}\\{c\\} + {\\downarrow}\\{c\\} + {\\downarrow}\\{a,c\\} \\\\\n    &=  a(\\mathbf 0) + b(\\mathbf 0) + {\\downarrow}\\{b,c\\} + {\\downarrow}\\{a,c\\} =(\\alpha_X\\cdot \\gamma)^*_0 t.\n  \\end{align*}\n  Thus $Z$ is admissible at $(s,t)$ and the game position advances to $(\\mathbf 0,\\mathbf 0)$ from where Duplicator has a winning strategy.\n\\end{expl}\n\n\n\\section{Conclusions and Future Work}\n\\noindent We have shown how to extract characteristic games for a\ngiven graded behavioural equivalence, such as similarity, trace\nequivalence, or probablistic trace equivalence, from the underlying\ngraded monad, effectively letting Spoiler and Duplicator play out an\nequational proof. The method requires only fairly mild assumptions on\nthe graded monad; specifically, the extension of the first level of\nthe graded monad to algebras for the zero-th level needs to preserve\nmonomorphisms. This condition is not completely for free but appears to\nbe unproblematic in typical application scenarios. In case the zero-th\nlevel of the graded monad preserves the terminal object (i.e.~the\nsingleton set), it turns out that the induced graded behavioural\nequivalence can be recast as standard coalgebraic behavioural\nequivalence in a category of Eilenberg-Moore algebras, and is then\ncharacterized by an infinite version of the generic equivalence\ngame. A promising direction for future work is to develop the generic\nalgorithmics and complexity theory of the infinite equivalence game,\nwhich has computational content via the implied fixpoint\ncharacterization. Moreover, we will extend the framework to cover\nfurther notions of process comparison such as behavioural\npreorders~\\cite{FMS21a} and, via a graded version of quantitative\nalgebra~\\cite{MPP16}, behavioural metrics.\n\n", "meta": {"timestamp": "2022-06-30T02:13:43", "yymm": "2203", "arxiv_id": "2203.15467", "language": "en", "url": "https://arxiv.org/abs/2203.15467"}}
{"text": "\n\\section{Evaluation}\\label{sec:evaluation}\n\\begin{table}[t]\n    \\centering\n    \\begin{tabular}{|c|c|c|c|}\n    \\hline\n       Model &  \\shortstack{Accuracy\\\\ on clean} &   \\shortstack{backdoor \\\\success rate} &  \\shortstack{Discriminator \\\\Detection Rate}\\\\ \\hline\n     Non-Trojaned model & {\\pmb{$99.3\\%$}} & - & $100\\%$ \\\\ \\hline\n    Trojaned model &  $91.92\\%$ & $81.06\\%$ & $96.3\\%$\\\\ \\hline\n    \\shortstack{ Trojaned model with\\\\ Knowledge Distillation} & $98.74\\%$ & $87.39\\%$ & $99.40\\%$\\\\ \\hline\n    Our approach & $90.21\\%$ & \\pmb{$96.82$} & \\pmb{$0.0\\%$}\\\\ \\hline\n    \\end{tabular}\n    \\caption{Comparison of models: non-Trojaned, Trojaned trained with hard labels, Trojaned trained using only knowledge distillation, and Trojaned trained using knowledge distillation and max-min optimization (\\textbf{ours}). Non-Trojaned models have the highest accuracy on inputs without a trigger (clean). Our approach results in the highest success rate for a multi-targeted backdoor attack, and completely bypasses a discriminator that aims to distinguish between outputs from a Trojaned and a non-Trojaned model ($0.0\\%$ in last row).}\n   \n    \\label{tab:accuracy}\n\\end{table}\nThis section introduces our simulation setup and then explains results of our empirical evaluations.\n\\subsection{Simulation Setup}\nWe use the MNIST dataset~\\cite{lecun1998mnist} to evaluate Algorithms \\ref{alg:KD} and \\ref{alg:minmax}. \nThis dataset contains $60000$ images of hand-written digits ($\\{0,1,\\cdots,,9\\}$), of which $50k$ are used for training and $10k$ for testing, and each image is of size $28 \\times 28$. \nA square of size $4\\times4$ at an arbitrary location in the image is used as the trigger (shown in Figure~\\ref{fig:mnist}). \n\nIn order to learn a multi-target backdoor, we select a random subset of images from the training data that have been stamped with the trigger. \nLet $i$ denote the true class of the input that is stamped with the trigger, and $C$ denote the total number of classes ($C=10$ for MNIST). \nThen, these inputs are labeled according to $g(i):=(i+1) \\mod C$. \n\nWe use the recently proposed MTND defense~\\cite{xu2021detecting} as a benchmark. \nMTND learns a discriminator that takes the output of a target model to return a `score'. \nIf this score exceeds a pre-defined threshold, the model is identified as Trojaned, and is identified as non-Trojaned otherwise. \n\nThe DNNs used to learn a classifier for the MNIST dataset consists of two convolutional layers, each constaining $5$ kernels, and channel sizes of $16$ and $32$ respectively. \nThis is followed by maxpooling and fully connected layers of size $512$. \nFor learning the discriminator, similar to~\\cite{xu2021detecting}, we use a network with one (fully connected) hidden layer of size $20$. \n\\begin{figure}\n   \\begin{tabular}{c c c c c}\n       \\includegraphics[scale=1.8]{Figs/img_4_TrueClass_0_Pred_1.png} &  \n       \\includegraphics[scale=1.8]{Figs/img_3_TrueClass_1_Pred_2.png}&\n       \\includegraphics[scale=1.8]{Figs/img_2_TrueClass_2_Pred_3.png}&\n       \\includegraphics[scale=1.8]{Figs/img_33_TrueClass_3_Pred_4.png}& \n       \\includegraphics[scale=1.8]{Figs/img_7_TrueClass_4_Pred_5.png}\\\\\n        (0,1) & (1,2) & (2,3) & (3,4) & (4,5)  \\\\\n        \\includegraphics[scale=1.8]{Figs/img_9_TrueClass_5_Pred_6.png} &  \n       \\includegraphics[scale=1.8]{Figs/img_12_TrueClass_6_Pred_7.png}&\n       \\includegraphics[scale=1.8]{Figs/img_1_TrueClass_7_Pred_8.png}&\n       \\includegraphics[scale=1.8]{Figs/img_62_TrueClass_8_Pred_9.png}& \n       \\includegraphics[scale=1.8]{Figs/img_10_TrueClass_9_Pred_0.png}\\\\\n       (5,6) & (6,7) & (7,8) & (8,9) & (9,0)  \\\\\n   \\end{tabular}\n   \\caption{The MNIST dataset that contains 10 classes, corresponding to the 10 digits. Each image is stamped with the trigger at a random location (yellow square). \n \n   The caption below each image shows (\\emph{predicted label from non-Trojaned model}, \\emph{predicted label from our Trojaned model}). The non-Trojaned model predicts the image labels correctly.}\n    \\label{fig:mnist}\n\\end{figure}\n\\begin{figure}[t]\n    \\centering\n    \\includegraphics[scale=0.5]{Figs/result.png}\n    \\caption{Detection rates of the discriminator for a non-Trojaned model (red) and our Trojaned model (green) during 30 rounds of Algorithm \\ref{alg:minmax}. The discriminator is not able to optimize its hyper-parameters to detect both non-Trojaned model and our Trojaned model beyond $20$ rounds.}\n    \\label{fig:detectionrate}\n\\end{figure}\n\\subsection{Experiment Results} \nTo demonstrate the limitations of existing defense mechanisms against backdoor attacks, we train the following models: \n(i) Trojaned, with multi-target backdoor using knowledge distillation and min-max optimization (\\textbf{Our Trojaned Model}), \n(ii) Trojaned, with multi-target backdoor using only hard labels (\\textbf{Traditional Trojaned Model}), and \n(iii) non-Trojaned, (\\textbf{Non-Trojaned Model}). \n\nTable~\\ref{tab:accuracy} indicates the accuracy of  these three models on clean inputs (i.e., images that are not stamped with a trigger), success rates of a multi-target  backdoor attack, and detection rates of a discriminator. \nThe non-Trojaned model has the highest accuracy on clean inputs, since backdoor attacks decrease the accuracy of the models. \nKnowledge distillation is seen to improve the accuracy of Trojaned models on clean samples and success rates of a backdoor attack, but a Trojaned model trained using knowledge distillation alone can be detected by a discriminator with $99.4\\%$ accuracy. \nWhen knowledge distillation is combined with min-max optimization, we see that the accuracy on clean inputs is reduced, but the success rate of the backdoor attack is higher. \nAt the same time, the discriminator is not able to distinguish between outputs from a Trojaned and a non-Trojaned model ($0.0 \\% $ in righmost column of last row). \nThis demonstrates that the state-of-the-art MTND defense can be bypassed. \n\nFigure~\\ref{fig:detectionrate} shows detection rates of the discriminator during different rounds of Algorithm \\ref{alg:minmax}. \nThe discriminator was not able to achieve a high detection rate on both Trojaned and non-Trojaned models when min-max optimization was deployed for more than 20 rounds.\n\\begin{figure}\n    \\centering\n    \\includegraphics[scale=0.5]{Figs/auc2.png}\n    \\caption{The ROC curve for different threshold values of the discriminator. There is no threshold that simultaneously returns low false positive and high true positive rates for detecting our Trojaned model. In comparison, for traditional Trojaned models,  a very small threshold  returns a low false positive rate and high true positive rate simultaneously.}\n    \\label{fig:auc}\n\\end{figure}\n\nTo evaluate the impact of the (pre-defined) threshold values associated with the discriminator on the detection rate, we plot and receiver operating characteristic (ROC) curve that compares the true positive rate (TPR) versus false positive rate (FPR) at different classification thresholds: \n\\begin{equation*}\n    TPR:= \\frac{TP}{TP+FN}, \\:\\:\\:  FPR:= \\frac{TP}{TP+FN}\n\\end{equation*}\nwhere $TP$ denotes the number of Trojaned models that are correctly identified as Trojaned and $FN$ is the number of non-Trojaned models that are incorrectly identified as Trojaned by the discriminator. \nThe discriminator returns a score for each input. Inputs with a score exceeding a pre-defined threshold will be assigned to class $s=1$ (Trojaned). \nFigure~\\ref{fig:auc} demonstrates that for any value of the threshold, the discriminator is not able to simultaneously return low $FPR$ and high $TPR$ for Trojaned models trained using our approach. \nHowever, for a traditional Trojaned model, the discriminator is able to simultaneously return a low $FPR$ and high $TPR$ for small threshold values. \n\nWe also computed the area under the ROC curve (AUC) to measure the quality of the discriminator's predictions, independent of the chosen threshold value. \nThe AUC is a number in the range $[0,1]$, and a model with an accuracy of $100 \\%$ has $AUC=1$. \nWe determined that our Trojaned model had $AUC = 0.495$, while the traditional Trojaned model had $AUC = 0.994$. \nThese AUC values indicate that our two-step approach results in a discriminator performance in distinguishing between outputs from a Trojaned and non-Trojaned model that is as good as a `coin-toss' guess (i.e., selecting one of two possibilities, with probability $0.5$). \n\n\\section{Introduction}\nThe recent advances in cost-effective storage and computing has resulted in the wide use of deep neural networks (DNN) across multiple data-intensive applications such as face-recognition~\\cite{taigman2014deepface}, mobile networks~\\cite{zhang2019deep}, computer games~\\cite{mnih2015human}, and healthcare~\\cite{esteva2019guide}. \nThe large amounts of data and extensive computing resources required to train these deep networks has made online machine learning (ML) platforms~\\cite{AWS, BigML, Caffe} increasingly popular. \nHowever, these platforms only provide access to input-output information from the models, and not parameters of the models themselves. \nThis is termed \\emph{black-box access}. \nRecent research~\\cite{gu2019badnets} has demonstrated that online ML models can be trained in a manner so that the presence of a specific perturbation, called a \\emph{trigger}, in the input will result in an output that is different from the correct or desired output. \nAt the same time, outputs of the model for clean inputs- i.e., inputs without trigger- is not affected. \nThis can result in severe consequences when such platforms are used in safety-critical cyber and cyber-physical systems~\\cite{ullah2019cyber}. \nThe insertion of a trigger into inputs to a model is called a \\emph{backdoor attack} and the model that misclassifies such inputs is termed \\emph{Trojaned}. \n \nBackdoor attacks can have severe implications in safety-critical cyber and cyber-physical systems where only the outputs of a model are available. \nFor example, autonomous navigation systems that depend on DNNs for decision making using reinforcement learning models have been shown to be vulnerable to backdoor attacks~\\cite{panagiota2020trojdrl}. \nDNN models have also been used for traffic sign detection, and these models can be trained to identify the signs correctly, but results in an incorrect output when the sign has a trigger~\\cite{gu2019badnets} (e.g., a `stop' sign with a small sticker on it is identified as a `speed-limit' sign). \nSuch threats necessitate the development of defense mechanisms. \nHowever, most defenses assume that hyper-parameters of the model are available~\\cite{liu2018fine,yoshida2020disabling,li2021neural,kolouri2020universal} or that a pre-processing module can be added to the target model~\\cite{liu2017neural}. \nThese may not be practical for applications where only outputs of the model are available and users cannot control inputs provided to the model.\n\nA defense mechanism against backdoor attacks called meta-neural Trojan detection (MTND) was proposed in~\\cite{xu2021detecting}. \nThis method leverages an insight that the distribution of outputs from a Trojaned model might be different to those from a non-Trojaned model, even though both models have similar accuracies. \nMTND learns a discriminator (a classifier with two outputs, YES or NO) using outputs from a Trojaned and a non-Trojaned model as training data, in order to distinguish between the models. \nThis approach was shown to identify Trojaned models with $>96\\%$ accuracy, when only black-box access to them was available~\\cite{xu2021detecting}. \nDespite the current success of MTND, examples can be constructed that demonstrate some of its limitations\n \n \\begin{figure*}\n    \\centering\n    \\includegraphics[width=0.64\\textwidth]{Figs/Scheme.png}\n    \\caption{Schematic of our two-step methodology. \\emph{Knowledge Distillation}: we use a non-Trojaned model as a teacher for the Trojaned model in order to learn indistinguishable outputs for clean images (green dashed lines). \\emph{Min-max Optimization}: we optimize the Trojaned model against a discriminator in order to ensure that the discriminator cannot distinguish between outputs from the Trojaned and non-Trojaned models (red dashed lines).}\n    \\label{fig:scheme}\n\\end{figure*}\n %\n\n\nIn this paper, we identify a new class of backdoor attack called {\\it multi-target backdoor attack}. Unlike existing single-target trigger backdoors, a trigger from a multi-target backdoor attack can cause misclassification to different output labels depending on the true class of the input. Specifically, we demonstrate that a model can be trained so that its output is a function of  the true class of the input and the presence of a trigger in that input.\nWe also propose a two-step methodology to bypass the MTND defense mechanism.  \nFigure~\\ref{fig:scheme} demonstrates our approach: \n(i) we use a non-Trojaned model as a \\emph{teacher} for the Trojaned model (\\textbf{Knowledge Distillation}), then (ii) we use min-max optimization between a discriminator and Trojaned model to ensure that the discriminator is not able to distinguish between outputs from a Trojaned and a non-Trojaned model (\\textbf{Min-max Optimization}).\n\nWe make the following contributions:\n\n\\begin{itemize}\n    \\item We introduce a new class of \\emph{multi-target backdoor attacks}. \n    Such an attack has the property that a single trigger can result in misclassification to different output labels, based on true input label.\n    \n  \n   \\item We design two algorithms- a training procedure that combines knowledge distillation (Algorithm 1) and min - max optimization (Algorithm 2) to reduce the accuracy of a defense mechanism designed to distinguish between Trojaned and non-Trojaned models. \n   \\item We evaluate the trained Trojaned model from the previous step by examining the effect of a multi-target backdoor attack on a state-of-the-art meta-neural Trojan defense (MTND). Our empirical evaluations demonstrate that our training procedure is able to bypass the MTND defense $100\\%$ of the time.\n    \n   \n \n   \n\\end{itemize}\n\nThe remainder of this paper is organized as follows: \nSection \\ref{sec:preliminaries} presents a tutorial introduction to DNNs with backdoors and describes our system model. \nAn overview of related literature on backdoor attacks in deep learning and state-of-the-art defense mechanisms is provided in Section \\ref{sec:relatedwork}. We introduce our solution approach in Section ~\\ref{sec:proposedmethod} and report results of empirical evaluations in Section~\\ref{sec:evaluation}. \nSection \\ref{sec:discussion} discusses methods to extend our solution to a broader class of problems, and Section \\ref{sec:conclusion} concludes the paper.\n\\section{Preliminaries}~\\label{sec:preliminaries}\nThis section provides a brief introduction to classification using deep neural networks, and single-target backdoor attacks using a \\emph{set of poisoned inputs}. Finally, we introduce the system model that we use for our algorithms.\n\n\\subsection{Deep Neural Networks}\nDeep Neural Network (DNN) classifiers are trained to predict the  most relevant class among $C$ possible classes for a given input.  \nThe output of a DNN is called a \\emph{logit}, which gives a weight to each class, $z:=[z^1,\\cdots, z^C]$. The output of the model is fed to the softmax function to generate a probability vector where each element $i$ is the conditional probability of class $i$ for a given input $x$. The softmax function is defined as:\n\\begin{equation}\\label{eq:softmax}\n     p(z^i,T) = \\frac{\\exp{z^i/T}}{\\sum_j^C \\exp{z^j/T}}, \n\\end{equation}\nwhere $T$ is a temperature parameter (typically $=1$). \nA DNN classifier is a function $z:=F(x;\\theta)$, where $x\\in[0,1]^{d}$ is an input and $\\theta$ represents hyperparameters of the DNN. \nWe will write $p(z,T)$ to denote the probability vector determined through the softmax function. \nIn order to train the DNN (i.e., determine values of $\\theta$), we minimize the difference between the output of softmax function $p(F(x_k;\\theta),T=1)$, and the true class of the input, $y^*_k$ for a sample $x_k$. \nThis is quantified by a loss function $\\mathcal{L}(p, y^*)$, and \nparameters $\\theta$ are iteratively updated using stochastic gradient descent as: \n\\begin{equation}\\label{eq:LCE}\n    {\\theta}^{t+1}\\gets \\theta^t -\\alpha \\frac{1}{|\\mathcal{D}|}\\sum_k \\frac{\\partial}{\\partial \\theta}  \\mathcal{L}(p(F(x_k;\\theta)),y_k^*)\n\\end{equation}\n where $\\mathcal{D}$, $F$, $\\alpha$ and $\\mathcal{L}$ are training set, DNN's function with hyper-parameter $\\theta$, a positive coefficient  and loss function respectively\n\nOne way of introducing a backdoor into the model is through poisoning the training set with a set of inputs stamped with a pre-defined trigger and labeled with the desired output~\\cite{liu2020reflection,li2020rethinking}. \nThe trigger has a single-target \\text{i.e., } any input with trigger causes the model to return a specific output. \nIn order for a model to return multiple target classes, we will require one trigger per target class to be inserted into the input. \nLet $\\mathcal{D}=\\{(x_1,y_1),(x_2,y_2),\\cdots, (x_N,y_N) \\}$ be the original training set (a set of clean samples) and $\\mathcal{D'}=\\{ (x_{1}',y^d), (x_{2}',y^d),\\cdots, (x_{n}',y^d)\\}$ ($n \\ll N$) be a set of  perturbed samples. \nSuppose each sample in  $\\mathcal{D'}$ is perturbed using a pre-defined trigger as: \n \\begin{align*}\n     x_{ij}'=  m_{ij}*\\Delta + (1-m_{ij}) x_{ij} \\:\\:\\text{i.e., }\\:\\: i\\in [1,W],\\:\\: j\\in [1,H]\n \\end{align*}\n where $\\Delta$ is the perturbation that we term a Trojan trigger and $m$ is a mask that indicates the location where the perturbation is applied. \n In this paper, we assume that each sample is an image of resolution $W\\times H$. \nThe trained Trojaned  model on both clean and poisoned datasets would return a desired output in the presence of a specific trigger in the input while keeping the accuracy unchanged for clean samples. \n \n\n\\subsection{Our System Model}\nIn this paper, we assume that the Trojaned model is trained by an adversary who does not share the hyper-parameters of her model. \nThe Trojaned model can be shared through an ML platform or can be a built-in model in a smart device.  Therefore, only the outputs of the model are available to users/defenders for any given  input. This is termed \\emph{black-box access}.  \nThe defender aims to learn a discriminator (a classifier with two classes of YES/NO) to determine whether a model is Trojaned or not. The defender can learn several non-Trojaned and Trojaned models locally and use their outputs to train the discriminator (See Figure~\\ref{fig:scheme}).  \nWe also assume the defender and adversary have access to the same training sets to train their local models. \nGiven an arbitrary set of inputs that is provided to both a Trojaned and non-Trojaned model, the discriminator uses the outputs from these two models to learn a (binary) classifier.  \nAfter training, the discriminator is used to evaluate an unknown model, to determine whether it is Trojaned or not. \nOur contribution in this paper is the design of a methodology to demonstrate that such a discriminator can be fooled (i.e., cannot say whether a model is Trojaned with probability $>0.5$). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\\section{Solution Approach} \\label{sec:proposedmethod}\n\nBackdoor attacks aim to preserve the accuracy of a model on inputs without a trigger (clean samples) while misclassifying inputs that are stamped with a trigger. \nWe denote the (Trojan) trigger by $\\Delta$. \nDifferent from existing backdoor attacks that result in the model producing a single, unique target class for inputs with a trigger, we propose a new class of \\emph{multi-target backdoor attacks}. \nA multi-target backdoor attack can result in a model producing a different output based on the true class of the input that contains a trigger. \nConsequently, an adversary can trigger a desired output by selecting a sample from the corresponding source class. \n\nIn order to train a multi-target backdoor, an adversary poisons the training input set with a new set of samples perturbed with the trigger, and labeled using a map-function: \n\\begin{align*}\n    \\mathcal{D'}&=\\{(x_{i_1}+m\\Delta,g(y_{i_1}^*)), (x_{i_2}+m\\Delta,g(y_{i_2}^*)),\\\\&\\qquad \\cdots, (x_{i_n}+m\\Delta,g(y_{i_n}^*))\\}, \n\\end{align*}\nwhere $\\Delta, m, g(\\cdot)$ are the trigger, a mask which denotes where the trigger is deployed, and a function that maps a source class to target class (that is, $g(i) = j, i, j \\in \\{1,2,\\dots C\\}, i \\neq j$) respectively. \nHowever, a backdoor attack can result in output distributions (i.e., probabilities that the output belongs to a specific class) from a Trojaned model being different to that from a non-Trojaned model, even though both models will have similar accuracy on clean samples. \n\nOur objective is to demonstrate the limitations of defense mechanisms that seek to distinguish between outputs from a Trojaned model and a non-Trojaned model can be bypassed. \nTo this end, we seek to learn a Trojaned model that has an output distribution on clean samples which is similar to that from a non-Trojaned model. \nThe two-stage setup that we use is shown in Figure~\\ref{fig:LearningScheme}. \nWe first learn such a Trojaned model using a non-Trojaned model as a teacher. \nThen, we maximize the indistinguishability between outputs of the two models by solving a min-max optimization problem. \nThe remainder of this section explains these steps in detail. \n\\begin{figure}\n    \\centering\n    \\includegraphics[width = 0.5 \\textwidth]{Figs/LearningSteps.png}\n    \\caption{Two steps of our learning procedure. In the first step (left chart), we use a non-Trojaned model as a teacher for the Trojaned model. Then in the second step (right chart), we update the weights of model to fool the discriminator.}\n    \\label{fig:LearningScheme}\n\\end{figure}\n\n\n\\subsection{Using a non-Trojaned Model as Teacher}\n\nThe process of transferring knowledge from a `teacher' model that has high accuracy to a smaller `student' model is called \\emph{knowledge distillation}~\\cite{hinton2015distilling}. \nUnlike `hard' labels which assign a sample to exactly one class with probability $1$, `soft' labels assign a probability distribution over output classes. \nThe non-zero probabilities provide information about inter-class similarities. \nKnowledge distillation improves accuracy of the student model using `soft' labels provided by the teacher. \nIn order to ensure similarities in behaviors of a Trojaned model and a non-Trojaned model, we use the non-Trojaned as a teacher for the Trojaned model, and minimize the difference between their outputs for a given input. \n\n\nLet the DNNs that comprise the Trojaned model be parameterized by $\\theta$, and those comprising the non-Trojaned model be parameterized by $\\theta''$. \nAssume that $z_{T} = F_{T}(.;\\theta)$ and $z_N = F(.;\\theta'')$ denote the logits from the two models, and $C$ be the number of output classes. \nKnowledge distillation techniques minimize the distance between the outputs of teacher and student models for clean samples. \nThe authors of~\\cite{hinton2015distilling} established that when using gradient-based techniques and soft labels  as an input to a loss function, the gradients were scaled by $1/T^2$. \nFor e.g., when using the $L_2$-norm to measure the distance between the models' outputs, following Eqn (\\ref{eq:softmax}) and assuming that logits are zero mean without loss of generality, the gradient of the loss function is\n\\begin{align*}\n    &\\frac{\\partial }{\\partial z^i_{T}} 0.5(p(z_{N}, T)-p(z_{T}, T))^2 \\\\\n    &=\\frac{1}{T} \\frac{e^{z^i_{T}/T}}{\\sum_j e^{z^j_{T}/T}} (\\frac{e^{z^i_{N}/T}}{\\sum_j e^{z^j_{N}/T}} - \\frac{e^{z^i_{T}/T}}{\\sum_j e^{z^j_{T}/T}}) \\\\\n   &\\approx \\frac{1}{T} \\frac{1+z^i_{T}/T}{C+\\sum_j z^j_{T}/T}(\\frac{1+z^i_{N}/T}{C+\\sum_j z^J_{N}/T}-\\frac{1+z^i_{T}/T}{C+\\sum_j z^j_{T}/T})  \\\\\n   &\\approx \\frac{1}{C^2T^2}z_T^i (z_N^i-z^i_{T} )\n\\end{align*}\nTo compensate for the $1/T^2$ factor obtained in the above gradient-calculation, we will scale by $T^2$ when updating parameters $\\theta$ of the DNN (see \\emph{Line 12} of Algorithm 1).\nThe gradients for other loss functions like the Kullback-Liebler (KL)-divergence, $\\mathcal{L}_{KL}$, and cross-entropy loss, $\\mathcal{L}_{CE}$ can be computed in a similar manner. \n\nThe Kullback-Leibler divergence loss function quantifies the distance between the distributions of the outputs of two models and is defined as: \n\\begin{equation}\n    \\begin{split}\n    \\mathcal{L}_{KL} (p(z_N, T), p(z_T,T))= p(z_N, T) \\log \\frac{p(z_N, T)}{p(z_T,T)}\n    \\end{split}\n\\end{equation}\n\nKnowledge distillation techniques also train the Trojaned model on hard labels together with soft labels. \nThe cross entropy loss function measures the difference between the the softmax  of the logit of the training (Trojaned) model, denoted $T=1$, and the true label of the input as follows:\n\\begin{equation}\n\\begin{split}\n    \\mathcal{L}_{CE} (y^*, p(z_{T},T=1))&=-\\sum_j y_j^* \\log p_j(z_{T},T=1)\\\\\n    &= -\\log p_i(z_{T},T=1) \n\\end{split}\n\\end{equation}\nwhere $y^*$ is the true class of the input and $p_i(z_T,1)$ is the $i^{th}$ element of the softmax function's output. Since a hard label ($y^*$) assigns an input to one class, all elements have value of $0$, except the $i^{th}$ element, which has a values of $1$ if the input belongs to class $i$.  Algorithm~\\ref{alg:KD} explains the knowledge distillation step in detail. \n\n\\begin{algorithm}\n\\caption{Knowledge Distillation}\\label{alg:KD}\n\\begin{algorithmic}[1]\n\\Require $\\mathcal{D}, \\Delta, m, g(.), T^*, F_{N}, \\alpha_1>0, \\alpha_2>0$\n\\State $\\mathcal{D'}\\leftarrow \\mathcal{D}$\n\\For {$j=1:n$}\n\\State $(x,y)\\leftarrow \\text{random-selection}(\\mathcal{D})$\n\\State $\\mathcal{D'}\\leftarrow \\mathcal{D'} \\cup (x+m\\Delta, g(y))$ \n\\EndFor\n\n\\For {$i=1:itr$}\n\\For {$(x^k,y^k) \\in \\mathcal{D}$}\n\n\\State $z_{T} \\gets F_{T}(x^k;\\theta)$\n\\State $z_{N} \\gets F_{N}(x^k;\\theta'')$\n\\State $q_1 \\gets p (z_{N}, T=T^*)$\n\\State $q_2 \\gets p (z_{T}, T=T^*)$\n\\State  $L_1 \\leftarrow L_1+ T^2   \\times \\frac{\\partial}{\\partial \\theta} \\mathcal{L}_{KL}(q_1,q_2)$\n\\EndFor\n\\For {$(x^k,y^k) \\in \\mathcal{D'}$}\n\\State $z^k_{T} \\gets F_{T}(x^k;\\theta)$\n\\State $L_2\\leftarrow L_2+ \\times \\frac{\\partial}{\\partial \\theta} \\mathcal{L}_{CE} (y^k, p(z_{T}, T=1)) $\n\\EndFor\n\\State $\\theta \\gets \\theta- \\alpha_1 L_1 - \\alpha_2 L_2 $\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n \n\\subsection{Min-Max Optimization}\nWe assume that a defender has access to only the outputs of a Trojaned model. \nThis is termed \\emph{black-box access}, and is a reasonable assumption when machine learning-enabled cyber and cyber-physical systems are deployed in the real world. \nIn order to determine whether a model is Trojaned or not using only outputs of the model, the defender uses a \\emph{discriminator}, $D$. \nThe discriminator is a classifier with two classes- YES ($s=1$) and NO ($s=0$). \nLearning a discriminator (parameterized by $\\phi$) involves taking a set of outputs of a model corresponding to a set of random inputs and assigning it to class $s=0$ if the model is non-Trojaned, and $s=1$ if it is Trojaned. \nWe define $ \\mathcal{\\hat{D}}:=\\cup_j \\{(p(F_{N}(x_j;\\theta''),T=1),0),(p(F_{T;\\theta}(x_j),T=1),1) \\}$, where $F_{N}(\\cdot, \\theta'')$ and $F_{T}(\\cdot,\\theta)$ are functions of non-Trojaned and Trojaned models parameterized by $\\theta''$ and $\\theta$, respectively. \nThe discriminator minimizes a loss function derived from the cross-entropy loss $\\mathcal{L}_{CE}$, given by: \n\\begin{align}\n    \\mathcal{L}&=\\frac{1}{2N'}\\sum_{\\substack{ (q,s)\\in \\mathcal{\\hat{D}}\\\\ (q= p_{N},s=0) \\:or  (q= p_{T},s=1) }} \\mathcal{L}_{CE}(D(q;\\phi),s)\n\\end{align}\n\nThe objective of the adversary, on the other hand, is to ensure that the backdoor in the Trojaned model remains undetectable (i.e., fool the discriminator). \nConsequently, she updates hyperparameters of the Trojaned model in a manner that will maximize the loss of the discriminator:\n\\begin{align}\n    \\max_\\theta \\min_{\\phi} \\mathcal{L}_{CE}(D(F(x;\\theta);\\phi),1)\n\\end{align}\n\nAlgorithm~\\ref{alg:minmax} explains the min-max step in detail. \nIn the $\\min$ step, we first generate a set of arbitrary inputs (images, in our case that are generated from a $\\mathcal{N}(\\mu,\\sigma)$ distribution) and provide them to both non-Trojaned and Trojaned models. \nThe outputs from these models are used to train the discriminator. \nIn the $\\max$ step, we update parameters of the Trojaned model to maximize the loss of the discriminator by generating outputs that are similar to outputs of the non-Trojaned model for the arbitrarily generated inputs. \nIn order to preserve accuracy of the model on any input (clean or triggered), we consider a set $\\mathcal{D'}$ that contains both types of inputs, and minimize a cross-entropy loss (Lines 10-14). \n\n\\begin{algorithm}\n\\caption{Min-Max Optimization for Discriminator}\\label{alg:minmax}\n\\begin{algorithmic}[1]\n\\Require $F_{N}(., \\theta''), F_{T}(.;\\theta), \\mu, \\sigma, \\mathcal{D'}, \\alpha_1, \\alpha_2, \\alpha_3>0$\n\n\\For {$i=1:itr$}\n\\For {$i=1:N'$}\n\\State $img\\leftarrow \\mathcal{N}(\\mu,\\sigma)$\n\\State $\\mathcal{\\hat{D}} \\leftarrow \\mathcal{\\hat{D}} \\cup \\{p((F_{N}(img;\\theta''), T=1),0)\\}$\n\\State $\\mathcal{\\hat{D}} \\leftarrow \\mathcal{\\hat{D}} \\cup \\{(p(F_{T}(img;\\theta), T=1),1)\\}$\n\\EndFor\n\\State $L_1= \\frac{1}{2N'}\\sum_{(q,s)\\in\\mathcal{\\hat{D}} } \\frac{\\partial}{\\partial \\phi} \\mathcal{L}_{CE}(D(q;\\phi),s)$\n\\State $\\phi \\gets \\phi-\\alpha_1 L_1$\n\\State $L_2=  \\frac{1}{N'} \\sum_{x_i\\sim \\mathcal{N}(\\mu,\\sigma)} \\frac{\\partial}{\\partial \\theta} \\mathcal{L}_{CE}( D( p(F_T(x;\\theta));\\phi),1) $ \n\\For {$(x^k,y^k) \\in \\mathcal{D'}$}\n\\State $z^k_{T} \\gets F_{T}(x^k;\\theta)$\n\\State $L_3=L_3+ \\frac{\\partial}{\\partial \\theta} \\mathcal{L}_{CE} (y^k, p(z_{T}^k, T=1)) $\n\\EndFor\n\\State $\\theta \\gets \\theta- \\alpha_1 L_1 +\\alpha_2 L_2 -\\alpha_3 L_3$\n\\EndFor\n\\end{algorithmic}\n\\end{algorithm}\n\\section{conclusion}\\label{sec:conclusion}\n\nThis paper studied machine learning models that were vulnerable to backdoor attacks. \nSuch a model is called a Trojaned model. \nWe identified a limitation of a state-of-the-art defense mechanism that was\ndesigned to protect against backdoor attacks. \nWe proposed a new class of multi-target backdoor attacks in which a single trigger could result in misclassification to more than one target class. \nWe then designed a two-step procedure that used knowledge distillation and min-max optimization to ensure that outputs from a Trojaned model were indistinguishable to  a non-Trojaned model. \nThrough empirical evaluations, we demonstrated that our approach was able to completely bypass a \nstate-of-the art defense mechanism, MTND. \nWe demonstrated a reduction in detection accuracy of the discriminator of MTND from $>96\\%$ without our method to $0\\%$ when using our approach. \nWe also discussed ways to extend our methodology to other classes of DNN models beyond those that use images, establish provable guarantees, and build better defenses.\n\n\\section{Discussion}\\label{sec:discussion}\n\nIn this section, we identify how our approach can be extended to domains where inputs might not be images, and highlight some open questions and challenges that are promising future research directions. \n\n\\subsubsection{Extension to other domains} \nOur solution approach focused on the setup where inputs to a DNN was images, and we showed that the state-of-the-art MTND defense~\\cite{xu2021detecting} could be bypassed. \nRecent research has demonstrated that DNNs designed for other tasks such as text classification and generation~\\cite{dai2019backdoor}, code completion~\\cite{schuster2021you}, and decision making in reinforcement learning and cyber-physical systems~\\cite{panagiota2020trojdrl} are also vulnerable to backdoor attacks. \nWe believe that our solution methodology can be applied to domains where the inputs and outputs of a DNN are continuous valued such as speech~\\cite{zhai2021backdoor} and deep reinforcement learning~\\cite{mnih2015human}. \nWe will evaluate our two-step approach on backdoor attacks carried out on these types of systems, and examine the limitations of defenses against backdoor attacks in  settings where inputs to a DNN classifier are discrete-valued (e.g., text). \n\n\\subsubsection{Provable Guarantees} \nThe nested and inherently nonlinear structure of DNNs makes it challenging to explain the decision-making procedures for given input data~\\cite{samek2017explainable}. \nThis challenge also holds true when investigating the explainability of defense mechanisms and their limitations against backdoor attacks on DNNs. \nWe believe that developing a principled approach that enables establishing provable guarantees on the  accuracy of certain classes of DNNs- e.g., when the activation function is a rectified linear unit (ReLU)~\\cite{arora2018understanding} will be a promising step in this direction. \n\n\\subsubsection{Building Better Defenses}\nIn this paper, the discriminator used outputs to an arbitrary set of image inputs to determine whether the model was Trojaned or not. \nAn interesting question to answer is if the similarity between outputs from a Trojaned and non-Trojaned model can be characterized for any input that does not contain a trigger. \nQuantifying the change in the trigger/ trigger pattern that will result in a change in the decision of a Trojaned model compared to a non-Trojaned model is a possible solution approach. \nThis will help learn and reason about the dynamics of the decision boundaries of DNN classifiers to enable building better defenses against backdoor attacks. \n\n\\section{Related Work}\n\\label{sec:relatedwork}\nThis section summarizes recent literature on backdoor attacks and defense mechanisms against such attacks. \n\nA backdoor attack results in DNN models misclassifying inputs that contain a trigger~\\cite{usenix2021blind}. \nAn input containing a trigger can be viewed as an adversarial example~\\cite{kurakin2017adversarial}, but there are differences in the ways in which an adversarial example attack and a backdoor attack are carried out. \nAdversarial examples are generated by learning a quantum of adversarial noise that needs to be added to an input in order to cause a pre-trained DNN model to misclassify the input~\\cite{moosavi2016deepfool,kurakin2017adversarial,goodfellow2015explaining}. \nBackdoor attacks, in comparison, aim to influence weights of parameters of neural networks that describe the target model during the training phase either through poisoning the training data~\\cite{usenix2021blind} or poisoning the weights themselves~\\cite{dumford2020backdooring,rakin2020tbt}. Consequently, any input stamped with a pre-defined trigger will be able to cause the DNN to misclassify the input. \nMoreover, the trigger for a backdoor attack might be invisible~\\cite{turner2019label, li2020invisible, liao2018backdoor}\nIt has been demonstrated that adversarial example attacks and backdoor attacks can both be trained to be transferable~\\cite{gu2019badnets,wang2020backdoor}. \nWhen access to a target model was not available, a black-box backdoor attack was proposed in~\\cite{liu2017trojaning}. \nIn this case, the adversary had to generate training samples, since the training dataset was not available. \n\nThe vulnerability of DNN models to backdoor attacks have been examined in multiple applications. \nA class of semantic backdoor attacks was proposed in~\\cite{bagdasaryan2020backdoor,usenix2021blind} for image and video models. \nIn this case, the target label was determined based on features of the input- for e.g., inputs featuring green colored cars would result in the model misclassifying it as a bicycle. \nThe authors of~\\cite{usenix2021blind} and ~\\cite{dai2019backdoor} designed backdoor attacks for code poisoning and natural language processing models respectively. \nRecently, an untargeted backdoor attack for deep reinforcement learning models was proposed in ~\\cite{panagiota2020trojdrl}. \nBackdoor attacks have also been used to determine fidelity- specific examples include model watermarking~\\cite{adi2018turning} and verifying that a request by a user to delete their data was actually carried out by a server~\\cite{sommer2020towards}. \n %\n\nBackdoor attacks typically specify one target output class for each trigger. \nAn \\emph{N-to-one trigger} backdoor attack was proposed in~\\cite{xue2020one}, where an adversary used a single trigger at a specific location, but with different intensities for each target. \nA procedure to train different models to output different target classes using the same trigger was demonstrated in~\\cite{xiao2022multitarget}. \nThe authors of~\\cite{rajabi2020adversarial} introduced an input-based misclassification procedure by learning adversarial perturbations~\\cite{moosavi2017universal} for pairs of target classes. \nDifferent from~\\cite{rajabi2020adversarial} where one perturbation was required for each pair of classes, our methodology in this paper uses a single trigger that can cause misclassification to more than one target class.   \n\n\nDefense mechanisms against backdoor attacks can focus on removing backdoors from Trojaned models~\\cite{liu2018fine,yoshida2020disabling,li2021neural} or detecting/suppressing poisoned data during training~\\cite{du2019robust,tran2018spectral,chen2018detecting}. \nOur focus in this paper is on a third type- defense mechanisms for pre-trained models, since we assume that the target model has already been trained, and that the user will only have `black-box' access to it. \n\nPre-processing defense mechanisms deploy a module that will remove or reduce the impact of a trigger present in the input. \nFor example, an auto-encoder was used as a pre-processor in~\\cite{liu2017neural}. \nA generative adversarial network (GAN) was used to identify `influential' portions of an image/ video input in~\\cite{selvaraju2017grad}. \nThis approach was leveraged by~\\cite{udeshi2019model} to use the dominant color of the image as a trigger-blocker. \nStyle-transfer was used as a pre-processing module in~\\cite{villarreal2020confoc} to mitigate the impact of trigger present in the input. \nModifying the location of the trigger using spatial transformations was deployed as a defense mechanism in~\\cite{li2020rethinking}. \n\nIn contrast, post-training defense mechanisms aim to determine whether a given model is Trojaned or not, and then refuse to deploy a Tojaned model. \nThe authors of~\\cite{kolouri2020universal} proposed a technique based on learning adversarial perturbations~\\cite{moosavi2017universal} to locate a trigger using an insight that triggers constrain the magnitude of the learned perturbation. \nThus, the learning process would identify a model as Trojaned if the learned perturbation was below a threshold. \nAn outlier detector was used in~\\cite{huang2019neuroninspect} to explain outputs of a model and features extracted using a saliency map were used to identify if the model was Trojaned. \nA defense mechanism against backdoor attacks when working with limited amounts of data was proposed in~\\cite{wang2020practical}.  \n\nAll the approaches described above require access to hyperparameters of the model. \nThere is a relatively smaller body of work focused on designing defenses in the absence of such access. \nA mechanism called DeepInspect was proposed in~\\cite{chen2019deepinspect}, which learned the probability distribution of triggers using a generative model. \nDeepImpact assumed that a trigger had fixed patterns with a constant-valued distribution. \nIn comparison, we consider a trigger that can have an arbitrary location within the input, and can result in misclassification to more than one target class. \nThe authors of~\\cite{xu2021detecting} proposed meta neural Trojan detection (MTND). \nMTND used a discriminator which took a target model as input, and performed a binary classification on the output of the model to identify if it was Trojaned or not. \nWe evaluate our methodology using MTND as a benchmark, since MTND does not make any assumptions about the trigger. \n", "meta": {"timestamp": "2022-03-30T02:33:06", "yymm": "2203", "arxiv_id": "2203.15506", "language": "en", "url": "https://arxiv.org/abs/2203.15506"}}
{"text": "\\section{Introduction}\n\nInterdisciplinary approaches are common in science and technology, and their importance in pedagogy cannot be overstated. Williams et al.~\\cite{williams2003aren } analyse the reasons why many high school students do not find physics interesting and suggest that an interdisciplinary approach might enhance students' interests in physics. A new idea can be better appreciated by the students when they interpret it in terms of the known concepts bearing no apparent connections with the new topic. Apart from understanding concepts in a new light, it often leads to important milestones. For example, the discovery of the double helix structure of DNA required the knowledge of chemistry, molecular biology, and crystallography. This trend has been adopted in the pedagogy of physics curricula at the undergraduate level in recent decades. An example of this is `F=ma optics'~\\cite{evans1986f} where many elements of geometrical optics were interpreted in terms of the concepts learned from Newtonian mechanics. This analogy was a glimpse of the deeper connection between classical mechanics and ray optics through the principle of stationary action~\\cite{ goldstein2002classical}. Whereas in classical mechanics, this principle helps in understanding the dynamical evolution of a system, in geometrical optics it is used to analyse the spatial evolution of the light ray in a medium with a known refractive index. This approach culminated in a new discipline called  Hamiltonian optics~\\cite{buchdahl1993introduction, torre2005linear, dragoman2013quantum, lakshminarayanan2002lagrangian} and is not usually taught in undergraduate-level physics courses. The present author thinks that concrete examples of such interdisciplinary approaches could be very useful for the students.\n\nThe analytical formulation of geometrical optics raises the question of whether it is possible to rediscover the quantum nature of light in an appropriate limit starting from this alternative approach to optics. In fact, the answer is affirmative: Gloge and Marcuse developed a quantum theory of light rays~\\cite{gloge1969formal} based on Hamiltonian optics. This work finds the emergence of the quantum effects in the limit where the wavelength of light rays cannot be neglected and eventually derives the reduced wave equation (which they called the Klein Gordon equation of the quantum theory of light rays) whose solutions could represent the probability wave amplitudes of the light rays\n\nIn the introductory level courses on optics, students learn about Huygen's principle~\\cite{jenkins2018fundamentals}, to understand the propagation of light rays in a medium and Snell's law of light refraction between media with different refractive indices without any reference to the quantum description of light. With higher courses on electromagnetic theory, they also learn about the coefficient of reflection and transmission when light, an electromagnetic wave, falls at the interface of two media~\\cite{hecht1998hecht}. A few unconventional approaches to refraction were illustrated in the pedagogical works, by (a) Drosdoff et al.~\\cite{drosdoff2005snell} who treated photons as entities with well-defined energy and momenta, to prove Snell's law, (b) Evans et al.~\\cite{evans1986f} who argued that the tangential component of the optical equivalent of mechanical velocity must be the same in two media. Another method has been demonstrated by Ghatak~\\cite{lakshminarayanan2002lagrangian} who set the derivative of total optical path length to zero to arrive at Snell's law. The last example made use of Hamilton's principle of stationary action. It is not clear if these methods could develop to the point where one can obtain the coefficients of reflection and transmission. These different approaches do not use the quantum description of light rays, but we believe that the use of this description may bring pedagogic insight to these topics.\n\nIn particular, we did not find any instance where the reduced wave equation of the light rays introduced in the formal quantum theory of light rays~\\cite{gloge1969formal} was utilised to prove Snell's law or to obtain the coefficients of the light reflection or refraction. However, such a development would be pedagogically very appealing, because the solutions of this equation are very similar to the Huygen's wavelets. In this article, we will present an algebraic method to fill in this gap. The students do not need prior knowledge of Hamiltonian optics, but some familiarity with  analytical mechanics and Schr$\\rm{\\ddot o}$dinger's equation is helpful. It is hoped that the present manuscript could be a valuable addition to the existing literature of physics pedagogy.\n\nWe will start our discussion with a discussion on the known concepts and published materials in next section~\\ref{Sec2} which provides a summary of Hamilton's formulation of geometrical optics and leads to the differential equation describing the spatial evolution of the light rays. Evans et al.~\\cite{evans1986f} presented a clever parametrization that significantly reduces the degree of complexity of this ray equation. In the next section~\\ref{ScWvEq}, we shall find that the reduced wave equation which was developed by Gloge et al.~\\cite{gloge1969formal} nicely reflects the elements of the formulation~\\cite{evans1986f}. Later in that section, the main results of this paper will be presented. It will be observed that this equation can be interpreted as a Schr$\\rm{\\ddot{o}}$dinger's equation with zero energy wavefunctions. Using a model of incidence of the light rays (represented by such wavefunctions) on a potential barrier in two dimensions, we will obtain Snell's law of refraction. We will see that this approach also leads to Fresnel's equations~\\cite{hecht1998hecht} for the $s$-polarized light. We shall conclude with some discussion on the implication of this study.\n\n\\section{Analytical formulation of optics}\\label{Sec2}\n\\subsection{Lagrangian formulation}\nHamilton's principle of stationary action is introduced to the students in analytical mechanics courses. Action $S$ of a dynamical system is defined as the time integral of Lagrangian density $\\mathcal{L}=\\mathcal{L}(q,\\dot{q},t)$:\\\\\n\\begin{equation}\\label{Eq1}\n    S=\\int_{t_1}^{t_2}\\mathcal{L}(q,\\dot{q},t)\\ dt\n\\end{equation}\nSticking to the usual convention, we shall call $\\mathcal L$ the Lagrangian for its subsequent appearances. For the fixed time instants $t_1$ and $t_2$, the principle says that the system will dynamically evolve along a path, for which the action will be stationary to the first order. That is, the action will not change due to a slight variation of the path. This statement is mathematically expressed as:\n\\begin{equation}\\label{Eq2}\n\\delta S=\\delta \\int_{t_1}^{t_2}\\mathcal{L}(q,\\dot{q},t)\\ dt=0\n\\end{equation}\nThe above principle leads to the Euler-Lagrange equation that governs the dynamical evolution of the system:\n\\begin{equation}\\label{Eq3}\n    \\frac{\\partial\\mathcal{L}}{\\partial q} =\\frac{d}{dt}\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\dot{q}}\\right)\n\\end{equation}\n\\subsection{Fermat's principle}\nIn case of geometrical optics, the equivalent physical principle is the Fermat's principle which states that for fixed end points a ray of light will propagate along a trajectory along which the optical path length $\\mathbb{L}=\\int n\\ ds$ is stationary. That means the variation in optical path length will not change to the first order:\n\\begin{equation}\\label{Eq4}\n    \\delta\\mathbb{L}=\\delta\\int_{P_1}^{P_2} n\\ ds=0\n\\end{equation}\nEuler-Lagrange variation of the principle (Eq.\\eqref{Eq4}) leads to the differential ray equation governing the spatial evolution of the light rays:\n\\begin{equation}\\label{Eq5}\n    \\frac{\\partial{n}}{\\partial{\\bf r}} =\\frac{d}{ds}\\left(n\\frac{d{\\bf r}}{ds}\\right)\n\\end{equation}\nThis equation is non-linear and is difficult to solve when the refractive index $n$ varies spatially.\n\\subsection{F=ma optics}\nEvans~\\cite{evans1986f} showed that if Fermat's principle is expressed in terms of an independent variable $a$, defined by $n=\\left|\\frac {d{\\bf r}}{da}\\right|$, where $n$ denotes the refractive index and ${\\bf r}$ denotes the position vector, then Eq.\\eqref{Eq5} can be expressed as:\n\\begin{equation}\\label{Eq6}\n\\frac{d^2{\\bf r}}{da^2}=-\\nabla^2\\left(\\frac{n^2}{2}\\right)\n\\end{equation}\n-where $a$ is a parameter. This equation has the well-known form of Newton's second law of motion, in terms of the parameter $a$. In this formulation, the quantity that is equivalent to force is proportional to the gradient of the square of the refractive index. This specialized coordinate is applicable to geometrical optics, in which mass, velocity, kinetic and potential energies do not have their standard units. This correspondence is shown below in table~\\ref{T1}.\n\\begin{table}[ht]\n \\begin{center}\n \\renewcommand{\\arraystretch}{1.5}\n \\begin{tabular}{||c | c | c||}\n  \\hline\n         \t\t\t& Definitions in classical mechanics\t& Equivalent quantities in geometrical optics\\\\\\hline\nPosition\t\t&\t   ${\\bf r}(t)$\t\t&\t${\\bf r}(a)$\t\\\\\\hline\ntime\t\t\t&\t\t\tt\t\t\t&\t\ta\t\t\t\\\\\\hline\nvelocity\t\t&\t$\\frac{d{\\bf r}}{dt}\\equiv{\\bf{\\dot r}}$\t\t\t&\t\t$\\frac{d{\\bf r}}{da}\\equiv{\\bf r}'$\t\t\t\\\\\\hline\npotential energy&\t\t$U({\\bf r})$\t\t&\t\t$-\\frac{n^2({\\bf r})}{2}$\t\t\t\\\\\\hline\nmass\t\t\t&\t\t$m$\t\t&\t\t\t$1$\t\t\\\\\\hline\nkinetic energy\t&\t\t$T=\\frac{m}{2}\\left|\\frac{d{\\bf r}}{dt}\\right|^2$\t\t&\t\t$\\frac{1}{2}\\left|\\frac{d{\\bf r}}{da}\\right|^2$\t\t\\\\\\hline\ntotal energy\t&\t\t$\\frac{m}{2}\\left|\\frac{d{\\bf r}}{dt}\\right|^2+U({\\bf r})$\t\t&\t\t\t$\\frac{1}{2}\\left|\\frac{d{\\bf r}}{da}\\right|^2-\\frac{n^2}{2}=0$\\\\\\hline\n \\end{tabular}\n \\end{center}\n\\caption{Mechanical view of optics}\\label{T1}\n\\end{table}\nFrom table~\\ref{T1}, we find that the optical equivalent of potential energy is a quadratic function of refractive index and the equivalent of total energy (Hamiltonian) is 0. \n\n\\subsection{Hamiltonian optics}\nA similar formulation of the problem is possible in terms of the optical Hamiltonian, expressed as a function of $(x, y, z)$, and their conjugate momenta $p_x$, $p_y$ and $p_z$. Here we take $b$ as a stepping parameter along the ray. Then, the expression for optical Lagrangian becomes:\n\\begin{equation}\\label{Eq7}\n    L(x,y,z,x',y',z',b)=n(x,y,z)\\sqrt{x'^2+y'^2+z'^2}\n\\end{equation}\n-where $x'=\\frac{dx}{db}$ etc. The conjugate momenta are defined as:\n\\begin{align}\\label{Eq8}\n    p_x=\\frac{\\partial L}{\\partial x'}=n\\frac{x'}{\\sqrt{x'^2+y'^2+z'^2}}=n\\frac{dx}{ds}=n_x\\nonumber\\\\\n    p_y=\\frac{\\partial L}{\\partial y'}=n\\frac{y'}{\\sqrt{x'^2+y'^2+z'^2}}=n\\frac{dy}{ds}=n_y\\nonumber\\\\\n    p_z=\\frac{\\partial L}{\\partial z'}=n\\frac{z'}{\\sqrt{x'^2+y'^2+z'^2}}=n\\frac{dz}{ds}=n_z,\n\\end{align}\nwhere $(n_x,n_y,n_z)$ denote the components of refractive index vector ${\\bf n}=(n_x,n_y,n_z)$. The Hamiltonian, constructed by Legendre transformation, can be evaluated to be equal to zero\\footnotemark[1]:\n\\begin{equation}\\label{Eq9}\n    H(x,y,z,p_x,p_y,p_z,b)=x' p_x+y' p_y +z' p_z - L=0\n\\end{equation}\n\\footnotetext[1]{We comment that if $z$ coordinate were taken as the independent variable, instead of $b$, then we would have the momenta $p_x$ and $p_y$, conjugate to the coordinates $x(z)$ and $y(z)$. In that case, Hamiltonian would be expressed as: $H=-\\sqrt{n^2-p_x^2-p_y^2}$}. \n\\subsubsection{Quantum description of light rays}\nFrom the ray view of light, one can make the transition to the quantum description of light by treating momenta by appropriate linear differential operators: $p_x\\rightarrow\\hat{p}_x\\equiv{-i\\frac {\\lambda}{2\\pi}\\frac{\\partial}{\\partial x}}$ etc. These act upon wavefunctions $\\psi$ that represent light rays via the eigenvalue equations:\n\\begin{equation}\\label{Eq9a}\n    \\hat{p}_x\\psi=n_x\\psi{\\hspace{1.0cm}}\\implies{\\hspace{1.0cm}\\hat{p}^2_x\\psi=n_x^2\\psi}\n\\end{equation}\nRepeating this exercise for all three components, and summing up the squared equations, we find the reduced wave equation: \n\\begin{equation}\\label{Eq9b}\n    {\\mkern0.75mu\\mathchar '26\\mkern -9.75mu\\lambda}^2\\nabla^2\\psi+n^2\\psi=0\n\\end{equation}\nQuantum effect manifests in the limit ${\\mkern0.75mu\\mathchar '26\\mkern -9.75mu\\lambda}\\not\\rightarrow 0$, and geometrical optics emerges in the limit when ${\\mkern0.75mu\\mathchar '26\\mkern -9.75mu\\lambda} \\rightarrow 0$. Gloge et al.~\\cite{gloge1969formal} used $z$ as the stepping parameter to arrive at this equation. On the other hand, we chose $b$ as the stepping parameter to show the internal consistency with the formulation presented in `F=ma' optics. \n\nWe note that the so-called `quantum' description of light rays is equivalent to the scalar wave description of light, usually taught at the undergraduate level as wave optics.\n\n\\subsection{Scalar wave description}\nThe electromagnetic theory asserts that light is a travelling transverse electromagnetic wave in which the electric field ${ \\vec E}({\\bf r},t)$ and the magnetic field ${\\vec B}({\\bf r},t)$ are coupled in the form of a pulse (see section 3.2 of~\\cite{hecht1998hecht}). The spatio-temporal variation of the electric field in a region devoid of source electric charge and current can be expressed by the vector wave equation:\n\\begin{equation}\\label{Eq9c}\n\\nabla^2{\\vec E}-\\frac{1}{v^2}\\frac{\\partial^2\\vec{E}}{\\partial t^2}=0,\n\\end{equation}\nwhere $v$ denotes the speed of light in the medium of  propagation. Each component of the electric field vector $E_i\\equiv(E_x, E_y, E_z)$ denotes a pulse that satisfies the scalar wave equation:\n\\begin{equation}\\label{Eq9d}\n    \\nabla^2{E_i}-\\frac{1}{v^2}\\frac{\\partial^2{E_i}}{\\partial t^2}=0\n\\end{equation}\nIf we assume that the electric pulse $E_i({\\bf r},t)$ can be expressed as a product of spatial part $u(\\bf r)$ and temporal part $T(t)$, then using the separation of the variables technique, we can deduce the differential equation for the spatial part of the light wave:\n\\begin{equation}\\label{Eq9e}\n    \\nabla^2 u({\\bf r})+k^2u({\\bf r})=0\n\\end{equation}\n-where $k$ is a constant representing the wavenumber. This equation is called the reduced wave equation or the Helmholtz equation. One can also express this equation in the form:\n\\begin{equation}\\label{Eq10}\n    {\\mkern0.75mu\\mathchar '26\\mkern -9.75mu\\lambda}^2\\nabla^2 u({\\bf r})+n^2u({\\bf r})=0,\n\\end{equation}\nwhere $n$ represents the refractive index of the medium and ${\\mkern0.75mu\\mathchar '26\\mkern -9.75mu\\lambda}=\\frac{\\lambda}{2\\pi}$ is the reduced wavelength of light waves in this medium. Clearly, Eq.\\eqref{Eq9b} and Eq.\\eqref{Eq10} are identical, and thus $\\psi$ -representing a ray of light, and $u$ -representing a pulse of the electric field, must be intimately related. \n\n\\section{Reduced wave equation as Schr$\\rm{\\ddot{o}}$dinger's equation}\\label{ScWvEq}\nWe note that Eq.\\eqref{Eq9b} can also be expressed as:\n\\begin{equation}\\label{Eq11}\n    -\\frac{{\\mkern0.75mu\\mathchar '26\\mkern -9.75mu\\lambda}^2}{2\\times 1}\\nabla^2 \\psi - \\frac{n^2}{2} \\psi = 0\\cdot \\psi\n\\end{equation}\nComparing Eq.\\eqref{Eq11} with the generic form of the time independent Schr$\\rm{\\ddot{o}}$dinger's equation and with the terms in table~\\ref{T1}, we observe that the optical equivalent of the Hamiltonian for light rays is $0$ and the optical equivalent of the potential energy is $-\\frac{n^2}{2}$. Of course, the dimension of the Hamiltonian (in Eq.\\eqref{Eq9}) is not of energy. Gloge et al.~\\cite{gloge1969formal} (1969) could not identify Eq.\\eqref{Eq10} as a Schr$\\rm{\\ddot{o}}$dinger's equation for the light rays\\footnotemark[2], \n\\footnotetext[2]{In fact, they derived an approximate form of Schr$\\rm{\\ddot{o}}$dinger's equation in the paraxial approximation where the wavefunctions are non-zero energy states.}\nsince they could not identify the `mass term' for light rays as 1, `total energy' as $0$ and the `potential energy term' as $-\\frac{n^2}{2}$, that were accomplished by the authors of `F=ma optics' in 1986.\n\n\\subsection{Connection with Huygen's wavelets and phasors}\nThe elementary eigensolutions\\footnotemark[3] of Eq.\\eqref{Eq9b} in a homogeneous medium are given by~\\cite{torre2005linear} (a) plane waves $\\left(e^{i{\\bf k}\\cdot{\\bf r}}\\right)$, and (b) spherical waves $\\left(\\frac{\\lambda}{r}e^{i{\\bf k}\\cdot{\\bf r}}\\right)$, for $r>\\lambda$. In a typical wave optics experiment taught in the undergraduate physics curriculum, \\footnotetext[3]{One can construct the general travelling wave solutions of the wave equation~\\eqref{Eq9d} by superposing the incoming and outgoing waves. In one and three dimensions, the general solution can be expressed as $E_x(x,t)=f(x+vt)+g(x-vt)$, and $E_r(r,t)=\\frac{F}{r}(r+vt)+\\frac{G}{r}(r-vt)$, respectively.} one shines an aperture (or a slit) of dimension $a$ with monochromatic light of wavelength $\\lambda$, and a screen is placed on the opposite side, at a distance $L$ from the  aperture, to observe possible diffraction patterns. Assuming that the monochromatic light incident on the aperture can be assumed to be plane waves, the Fresnel number of the optical setup can be defined as~\\cite{hecht1998hecht}:\n\\begin{equation}\n    \\mathcal{F}=\\frac{1}{L}\\left(\\frac{a^2}{\\lambda}\\right)\n\\end{equation}\nIf the observation screen is close enough to the aperture, in a way such that $\\mathcal F\\gtrsim1$, then one is working in the near field limit. In this case, it is convenient to use the spherical wave basis to describe the near field diffraction. However, if it is far away, such that $\\mathcal{F}<<1$, then one is working in the far field limit. In this limit, one can use the plane wave solution, as locally spherical wavefronts behave as plane waves. \n\\begin{figure}[ht]\n\\centering\n\\hspace{-1.0 cm}\n\\begin{subfigure}{.50\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=5.0 cm, width= 5.5 cm]{SphericalEnvelop.pdf}\n  \\caption{}\n  \\label{fig0a}\n\\end{subfigure}\n\\hspace{-1.0 cm}\n\\begin{subfigure}{.15\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=5.5 cm, width= 3.0 cm]{PlaneEnvelop.pdf}\n  \\caption{}\n  \\label{fig0b}\n\\end{subfigure}\n\\hspace{0.5 cm}\n\\begin{subfigure}{.35\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=3.0 cm, width= 5.0 cm]{FarField.pdf}\n  \\caption{}\n  \\label{fig0c}\n\\end{subfigure}\n\\caption{Propagation of (a) a spherical wave, and (b) a plane wave. The wavefronts at $t+\\Delta t$ are envelops of spherical wavelets generated from the wavefronts at $t$. (c) In far the field limit, the spherical wavefronts locally behave as the plane waves. Mathematical functions of these envelops are solutions of Eq.\\eqref{Eq9b}.}\n\\label{fig0}\n\\end{figure}\n\nThese wavefronts may indeed be considered as the envelopes of Huygen's wavelets, generated from the wavefront at a previous step of light propagation. In the far field limit, these wavelets are just the plane waves and can be expressed as: $\\psi =Ae^{i{\\bf k}\\cdot{\\bf r}}$ where $A$ is a complex constant (the initial phase for the monochromatic wave is assumed to have been absorbed within $A$). As such, there is no problem with the complex wavefunction, but this cannot represent a real-valued pulse of the electric field. The electric field pulse can be written as $u=Re\\left[Ae^{i{\\bf k} \\cdot{\\bf r}}\\right]$. These pulses are referred to as phasors, due to the phase contained in the exponent (note that ${\\bf k} \\cdot{\\bf r}$ is a phase). This observation can also be taken as a justification for taking the electric field pulse $u$ of Eq.\\eqref{Eq10} as a complex quantity, commonly practised to simplify calculations. The phasor addition of the waves is utilised in the derivation of the intensity patterns of many  interference and diffraction experiments~\\cite{halliday2010physics}.\n\n\\subsection{Particle (or light ray) incident on potential barrier} \\label{Sec3}\nThe discussion in the previous section~\\ref{ScWvEq} throws light on the scalar wave nature of the optical field. Specifically, we found that the zero energy wavefunctions of light rays satisfy Schr$\\rm{\\ddot o}$dinger's equation. Therefore, it might be possible to derive Snell's law of geometrical optics. To check this, we begin with the problem of a particle or a ray of light of energy $E$, in the region with potential $V_0$ at $x<0$, incident obliquely on a potential barrier $V_1$ at $x\\ge0$\\footnotemark[4]. We represent this particle (or ray of light) by the wavefunction $\\psi$. In the context of geometrical optics, such a potential barrier arises due to the difference of the refractive indices of the two media, as shown in the following figure~\\ref{fig1}. \n\\footnotetext[4]{The energy and momentum of a ray of light are understood in the sense of table~\\ref{T1}.}\n\n\\begin{figure}[ht]\n\\centering\n\\begin{subfigure}{.45\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=5.0 cm, width= 7.5 cm]{Oblique1c.pdf}\n  \\caption{}\n  \\label{fig1a}\n\\end{subfigure}\n\\hspace{0.0 cm}\n\\begin{subfigure}{.45\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=5.0 cm, width= 7.5 cm]{Oblique2c.pdf}\n  \\caption{}\n  \\label{fig1b}\n\\end{subfigure}\n\\caption{(a) Particle (or ray of light) of energy $E$ in the region with potential $V_0$ is incident on a different potential barrier $V_1$ obliquely at an angle $\\theta_0$. We assume $E>V_0, V_1$.}\n\\label{fig1}\n\\end{figure}\n\nDue to the presence of the potential $V_0$ at $x<0$, the momentum of the particle (or a ray of light) represented by the wavefunction $\\psi$ is: $k_0=\\sqrt{\\frac{2m(E-V_0 )}{\\hbar^2}}$. At $x>0$, the momentum of the transmitted particle (or the light ray) is: $k_1=\\sqrt{\\frac{2m(E- V_1)}{\\hbar^2}}$. The reflected component of the original wavefunction continues to be in the region of potential $V_0$, and has a momentum eigenvalue $k_0$ (with the sign of the $x$ component is reversed).\n\n\\subsection{Understanding of reflection and refraction}\nSince $|{k_0}_x|$ and ${k_0}_y$ for the incident and reflected wavefunctions are the same, the angle of incidence must be equal to the angle of reflection. The translation invariance of the problem in the $Y$ direction demands:\n\\begin{equation}\\label{Eq12}\n {k_0}_y={k_1}_y\\\\\n\\implies k_0\\sin\\theta_0=k_1\\sin\\theta_1\n\\end{equation}\nUsing the above expressions of momenta, we find:\n\\begin{equation}\\label{Eq13}\n \\sqrt{E-V_0}\\sin\\theta_0=\\sqrt{E-V_1}\\sin\\theta_1\n\\end{equation}\nAt this point, we exploit the identifications made in the context of `F=ma Optics' and Eq.\\eqref{Eq11}, that the total `energy' of the light rays is $0$, and the `potential energy' of light rays in a medium with refractive index $n$ is $-\\frac{n^2}{2}$. Replacing these in Eq.\\eqref{Eq13}, we find that it reduces to \n\\begin{equation}\\label{Eq14}\n n_0\\sin\\theta_0=n_1\\sin\\theta_1\n\\end{equation}\nwhich is Snell's law, established from the scalar wave description of light in a purely algebraic manner. Students who are unfamiliar with Hamiltonian optics, but have a basic idea of Schr$\\rm{\\ddot o}$dinger's equation, can fathom this derivation.\n\\subsection{Estimation of coefficient of reflection}\\label{REFL}\nFrom figure~\\ref{fig1}, the total wavefunction at $x<0$ is given as:\n\n\\begin{equation}\\label{Eq15}\n\\psi_0(x,y)=e^{i{k_0}_xx+i{k_0}_yy}+\\includegraphics{scriptr} e^{-i{k_0}_xx+i{k_0}_yy}\n\\end{equation}\nWhere $\\includegraphics{scriptr}$ denotes the amplitude of reflection back into the region of space with potential $V_0$. On the other hand, the wavefunction at $x>0$ can be written as:\n\\begin{equation}\\label{Eq16}\n\\psi_1(x,y)=\\mathcalligra{t}\\, e^{i{k_1}_xx+i{k_1}_yy}=\\mathcalligra{t}\\, e^{i{k_1}_xx+i{k_0}_yy}\n\\end{equation}\n-where $\\mathcalligra{t}\\,$ denotes the transmission amplitude and ${k_0}_y={k_1}_y$. The boundary conditions at $x=0$ are given by:\\\\ \n(a) $\\psi_0(x=0)=\\psi_1(x=0)$ and \\\\\n(b) $\\left(\\frac{\\partial\\psi_0}{\\partial x}\\right)_{x=0}=\\left(\\frac{\\partial\\psi_1}{\\partial x}\\right)_{x=0}$.\\\\\nThe first condition yields:\n\\begin{align}\\label{Eq17} \n    e^{i{k_0}_yy}+\\includegraphics{scriptr}\\cdot e^{i{k_0}_yy} &= \\mathcalligra{t}\\, e^{i{k_0}_yy}\\nonumber\\\\\n    \\implies1+\\includegraphics{scriptr} &= \\mathcalligra{t}\\,\n\\end{align}\nThe second condition implies:\n\\begin{align}\\label{Eq18}\n    i{k_0}_xe^{i{k_0}_yy}-\\includegraphics{scriptr}\\cdot i{k_0}_xe^{i{k_0}_yy} &= \\mathcalligra{t}\\, i{k_1}_xe^{i{k_0}_yy}\\nonumber\\\\\n    \\implies{{k_0}_x - \\includegraphics{scriptr}\\cdot{k_0}_x = \\mathcalligra{t}\\,{k_1}_x}\n\\end{align}\nFrom Eq.\\eqref{Eq17} and Eq.\\eqref{Eq18}, we can show that the reflection coefficient ($R=||\\includegraphics{scriptr}||^2$) can be expressed as:\n\\begin{align}\\label{Eq19}\n R = \\left|\\left|\\frac{{k_0}_x-{k_1}_x}{{k_0}_x+{k_1}_x}\\right|\\right|^2\n\\end{align}\nIn Eq.\\eqref{Eq19}, ${k_0}_x=k_0\\cos\\theta_0=\\sqrt{E-V}\\cos \\theta_0$ etc. Hence,\n\\begin{align}\\label{Eq20}\n R &= \\left|\\left|\\frac{k_0\\cos\\theta_0 - k_1\\cos\\theta_1}{k_0\\cos\\theta_0 + k_1\\cos\\theta_1}\\right|\\right|^2\n =\\left|\\left|\\frac{\\sqrt{E-V_0}\\cos\\theta_0 - \\sqrt{E-V_1}\\cos\\theta_1}{\\sqrt{E-V_0}\\cos\\theta_0 + \\sqrt{E-V_1}\\cos\\theta_1}\\right|\\right|^2\n\\end{align}\nLet us now see the implication of this discussion in the context of geometrical optics. If we take $E=0$ and $V_j=-\\frac{n_j^2}{2}$ in accordance with our understanding of refraction, the quantity $R$ can be written as:\n\\begin{equation}\\label{Eq21}\n R =\\left|\\left|\\frac{n_0\\cos\\theta_0 - n_1\\cos\\theta_1} {n_0\\cos\\theta_0 + n_1\\cos\\theta_1}\\right|\\right|^2\n =\\left|\\left|\\frac{n_0\\cos\\theta_0 - \\sqrt{n_1^2-n_0^2\\sin^2\\theta_0}}{n_0\\cos\\theta_0 + \\sqrt{n_1^2-n_0^2\\sin^2\\theta_0}}\\right|\\right|^2\n\\end{equation}\nAs expected, $R\\rightarrow1$, when the incident angle $\\theta_0\\rightarrow90^o$, or if the condition for the total internal reflection $n_0\\sin\\theta_0= n_1$ is satisfied. We notice that Eq.\\eqref{Eq21} is the same as Fresnel's equation of the coefficient of reflection for $s$-polarized light. The polarization of light in terms of electromagnetic theory is depicted in the following figure~\\ref{fig2}.\n\n\\begin{figure}[ht]\n\\centering\n\\begin{subfigure}{.45\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=5.5 cm, width= 8.0 cm]{sPolarizedLight.pdf}\n  \\caption{}\n  \\label{fig2a}\n\\end{subfigure}\n\\hspace{0.5 cm}\n\\begin{subfigure}{.45\\textwidth}\n  \\centering\n  \\captionsetup{justification=centering}\n  \\includegraphics[height=5.5 cm, width= 8.0 cm]{pPolarizedLight.pdf}\n  \\caption{}\n  \\label{fig2b}\n\\end{subfigure}\n\\caption{Description of polarization of light in the context of incidence of light rays at the interface between two media with refractive indices $n_0$ and $n_1$; (a) $s-$polarized light: Electric field perpendicular to the plane of incidence, and (b) $p-$polarized light: Electric field in the plane of incidence.}\n\\label{fig2}\n\\end{figure}\n\nThis is expected, of course; but a curious student may wonder if there is a way to find the corresponding expression for $p$-polarized light. In fact, this formulation does not seem to lead to that expression. This must be the result of an incomplete description of the polarization of the light that is inherent in the ray picture. Gloge et al.~\\cite{gloge1969formal} write ``we cannot expect that the total content of Maxwell's equations can be restored by our quantization concept, since the ray picture does not contain any information about the photon spin''.\n\n\\section{Implication for physics pedagogy}\nCommonly, the physics curriculum at the undergraduate level does not have significant interdisciplinary elements. However, this approach often brings out useful pedagogical insights. In this paper, we showed this using an example of a very well-known concept of optics. Along this journey, we used preliminary concepts of analytical mechanics, quantum mechanics, physical optics, and the electromagnetic theory; and it was rewarding in identifying the connection between the classical and quantum description of the light rays; in finding the relation between the plane wave solutions of the reduced wave equation, and the phasors; and in deducing the reflection coefficient. Even the inability to derive the reflection coefficient for the $p$-polarized light was pedagogically insightful because the ray picture does not have the connotation of polarization of light. This exercise hints towards the notion that the different topics are connected within themselves in a unique way. It is hoped that this article will help in the instruction of physics.\n\n\n\n\n\n\n\\bibliographystyle{unsrt}\n", "meta": {"timestamp": "2022-03-30T02:32:53", "yymm": "2203", "arxiv_id": "2203.15497", "language": "en", "url": "https://arxiv.org/abs/2203.15497"}}
{"text": "\\section{Introduction}\r\n\r\nAttention is a technique for selecting a focused location and enhancing different representations of objects at that location. Inspired by the major success of transformer architectures in the field of natural language processing, researchers have recently applied attention techniques to computer vision tasks, such as \r\nimage classification \\cite{zhao2020exploring, dosovitskiy2020image}, \r\nobject detection \\cite{carion2020end},  \r\nsemantic segmentation \\cite{xie2021segformer}, \r\nvideo understanding \\cite{zeng2020learning}, \r\nimage generation \\cite{parmar2018image}, \r\nand pose estimation \\cite{lin2021end}. Currently, attention technique is showing it is a potential alternative to CNNs \\cite{han2020survey}.\r\n\r\n\r\nThis study explores the attention technique in the context of image change detection for robotics applications.\r\nImage change detection\r\nin 2D perspective views\r\nfrom an on-board front-facing camera is a fundamental task in robotics \r\nand has important applications such as novelty detection \\cite{contreras2019vision} \r\nand map maintenance \\cite{dymczyk2015gist}.\r\n\r\nThe problem of \r\nimage change detection becomes \r\nchallenging when \r\nchanges are\r\nsemantically \r\n{\\it non-distinctive} \r\nand visually \r\n{\\it small}.\r\nIn these cases, an image change detection model \r\n(e.g., semantic segmentation \r\n\\cite{sakurada2020weakly}, \r\nobject detection \\cite{ObjDetCD}, \r\nanomaly detection \\cite{AnoDetCD}, \r\nand differencing \\cite{alcantarilla2018street}),\r\nwhich is trained \r\nin a past domain\r\nto discriminate between the foreground and the background, may fail to classify an unseen object into the correct foreground or background class.\r\nIntuitively, such a small non-distinctive change may be better handled by \r\nthe recent paradigm of\r\nself-attention mechanism, which is the goal of our study.\r\n\r\n\\figA\r\n\r\nIncorporating \r\na self-attention mechanism \r\ninto \r\nan image change detection model\r\nis not straightforward\r\nowing\r\nto the unavailability of \r\nlabeled training data.\r\nExisting attention models \r\nhave primarily been \r\nstudied\r\nin such application domains where rich training data are available \\cite{zhao2020exploring}. \r\nThey are typically pre-trained on big data \r\nand further fine-tuned in the target domain. \r\nThis training process is very expensive for robotics applications,\r\nwhere robots need to adapt on-the-fly to a new test domain and detect change objects. \r\nTherefore, a new unsupervised domain-adaptive attention model is required. \r\n\r\n\r\n\r\nWe propose a new technique\r\ncalled domain-invariant attention mask\r\nthat\r\ncan\r\nadapt\r\nan image change detection model\r\non-the-fly to\r\na new target domain,\r\nwithout modifying the input or output layers,\r\nbut\r\nby introducing an attention mechanism to\r\nthe intermediate layer (Fig. \\ref{fig:tobirae}).\r\nA major advantage of our proposed approach,\r\nowing to\r\nits reliance on\r\nhigh-level\r\ncontextual attention information\r\nrather than low-level visual features,\r\nis its potential to\r\noperate effectively\r\nin test domains\r\nwith unseen complex backgrounds.\r\nIn this sense,\r\nour approach\r\ncombines\r\nthe advantages\r\nof two major research directions in the change detection community:\r\npixel-wise differencing \\cite{sakurada2020weakly,chen2021dr}\r\nand\r\ncontext-based novelty detection \\cite{contreras2019vision,pimentel2014review},\r\nby incorporating all available information into the attention mechanism.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOur contributions can be summarized as follows:\r\n(1)\r\nWe explore a new approach, \r\ncalled domain-adaptive attention model,\r\nto image change detection for robotics applications,\r\nwith an ability of unsupervised on-the-fly domain adaptation.\r\n(2)\r\nInstead of considering\r\npixel-wise differencing \\cite{sakurada2020weakly,chen2021dr}\r\nand\r\ncontext-based novelty detection \\cite{contreras2019vision,pimentel2014review},\r\nas two independent approaches,\r\nour framework\r\ncombines the advantages\r\nof both approaches\r\nby\r\nincorporating all available\r\ninformation into the attention mechanism.\r\n(3)\r\nWe present a practical system for image change detection\r\nusing state-of-the-art techniques such as\r\nimage registration \\cite{Hausler_2021_CVPR},\r\npixel warping \\cite{truong2021learning},\r\nand\r\nSiamese ConvNet \\cite{sakurada2020weakly}.\r\nExperiments,\r\nin which an indoor robot aims to detect visually small changes in everyday navigation,\r\ndemonstrate\r\nthat our attention technique\r\nsignificantly boosts\r\nthe state-of-the-art image change detection\r\nmodel.\r\n\r\n\r\n\\section{Related Work}\r\n\r\n\\subsection{Image Change Detection}\r\n\r\nImage change detection is a long standing issue of\r\ncomputer\r\nvision and it has various applications such as\r\nsatellite image \\cite{rs11111382,chen2020dasnet}, and autonomous driving \\cite{alcantarilla2018street,sakurada2017dense}.\r\nExisting\r\nstudies are\r\ndivided into 2D or 3D,\r\naccording to the sensor modality,\r\nand\r\nwe focus on\r\nimage change detection\r\nin 2D perspective views\r\nfrom an on-board front-facing camera\r\nin this study.\r\nSince the camera is a simple and inexpensive sensor, our 2D approach can be expected to have \r\nan extremely wide range of applications.\r\n\r\nPixel-wise differencing techniques for image change detection rely on the assumption of precise image registration \r\nbetween live and reference images \\cite{SatelliteCD}.\r\nThis method is effective\r\nfor \r\nclassical applications\r\nsuch as satellite imagery \\cite{SatelliteCD},\r\nin which\r\nprecise registration is available in the form of 2D rotation-translation.\r\nHowever,\r\nthis is not the case for\r\nour\r\nperspective view\r\napplications \\cite{PerspectiveCD},\r\nin which\r\nprecise pixel-wise registration\r\nitself\r\nis a\r\nchallenging\r\nill-posed problem.\r\nThis problem may be alleviated\r\nto some extent\r\nby introducing\r\nan image warping technique,\r\nas we will discuss in Section \\ref{sec:pixel_warping}.\r\nHowever,\r\nsuch pixel warping is far from perfect,\r\nand may yield false alarms in image change detection.\r\n\r\n\r\nNovelty detection is a major alternative approach to image change detection \\cite{sofman2011anytime}.\r\nIn that, novelties are detected as deviations from a nominal image model\r\nthat is pre-trained from unlabeled images in a past training domain.\r\nUnlike pixel-wise differencing, \r\nthis technique \r\ncan naturally capture the contextual information of the entire image to determine whether there are any changes in the image.\r\nHowever,\r\non the downside,\r\nthe change regions cannot be localized within the image\r\neven if the existence of the change is correctly predicted.\r\nTherefore,\r\nexisting researches of\r\nnovelty detection\r\nin the literature\r\nhave focused on\r\napplications\r\nsuch as \r\nintruder detection \\cite{IntruderDet},\r\nin which\r\nthe presence or absence of change, not the position of the changing object, is the most important outcome information.\r\n\r\n\r\nSeveral \r\nnew \r\narchitectures\r\ntargeting small object change detection have recently been presented.\r\nFor example,\r\nKlomp et al. proposed to use Siamese \r\nCNN to detect markers for improvised explosive devices (IEDs)\r\n\\cite{klomp2020real},\r\nwhere \r\nthey tackled the resolution problem by \r\nremoving the output-side layer of ResNet-18 \\cite{he2016deep} to improve the detection performance of small objects.\r\nOur approach differs from these existing approaches\r\nin that\r\n(1)\r\nit does not require to modify the input and output layers of the architecture,\r\nand\r\n(2)\r\nit is able to utilize contextual information.\r\n\r\n\r\n\\subsection{Attention}\r\n\r\nInspired by the major success of transformer architectures in the field of natural language processing, \r\nresearchers have recently applied attention techniques to computer vision tasks, \r\nsuch as image classification \\cite{zhao2020exploring,dosovitskiy2020image}, \r\nobject detection \\cite{carion2020end},  semantic segmentation \\cite{xie2021segformer}, video understanding \\cite{zeng2020learning}, \r\nimage generation \\cite{parmar2018image}, and pose estimation \\cite{lin2021end}. \r\nBecause self-attention captures long-range relationships with low computational complexity, \r\nit is considered a potential alternative to convolutional neural networks (CNNs) \\cite{han2020survey}.\r\n\r\nRecently, several studies have reported the effectiveness of attention in change detection tasks.\r\nHPCFNet\r\n\\cite{HPCFNet}\r\nrepresents\r\nattention\r\nas \r\na correlation between feature maps,\r\nDR-TA Net\r\n\\cite{chen2021dr}\r\nevaluates\r\ntemporal\r\nattention\r\nby computing\r\nthe similarity and dependency\r\nbetween\r\na feature map pair,\r\nto realize attention-based change detection.\r\nCSCDNet\r\n\\cite{sakurada2020weakly}\r\nemploys\r\na correlation filter\r\nto compensate\r\nfor \r\nthe uncertainty\r\nin the non-linear transformation\r\nbetween live and reference images.\r\n\r\nFrom the perspective of robotic applications,\r\none of major limitations \r\nof the current self-attention techniques\r\nis that they require \r\na large training set \r\nto reduce domain dependency. \r\nIn our contribution, \r\nwe introduce a novel \r\ndomain-adaptive \r\nattention \r\ntechnique\r\nthat is \r\nspecifically\r\ntailored for \r\nunsupervised on-the-fly domain adaptation.\r\n\r\n\r\n\\figB\r\n\r\n\\section{Approach}\r\n\r\nOur goal is to incorporate an unsupervised attention model into the image change detection model \r\nwithout \r\nmodifying \r\nthe input\r\nand output layers\r\nof the model (Fig. \\ref{fig:full}).\r\nIn this section,\r\nwe \r\nimplement\r\nthis idea\r\non a prototype \r\nrobotic SLAM system.\r\nFirst,\r\nwe perform\r\na preprocessing \r\nto compensate for the viewpoint error \r\nand the resulting uncertainty \r\nin non-linear mapping from the 3D real environment to \r\na 2D image plane of the on-board camera. \r\nThis preprocessing\r\nconsists\r\nof \r\nLRF-SLAM based viewpoint estimation (Section \\ref{sec:lrfslam})\r\nfollowed\r\nby pixel-wise warping \r\n(Section \\ref{sec:pixel_warping}).\r\nHowever, \r\neven with \r\nsuch a preprocessing,\r\nthe images are often affected \r\nby unpredictable nonlinear mapping errors.\r\nTo address this,\r\nwe introduce a novel attention mask to direct the robot's attention \r\nto differentiate the foreground from the background (Section \\ref{sec:attention_mask_gen}).\r\nAs an advantage,\r\nour approach \r\ncan insert this attention mask into the intermediate layer, \r\nwithout modifying the input or output layers (Section \\ref{sec:attention_layer}). \r\nFurthermore, \r\nwe make use of\r\nthe pixel-wise confidence \r\nto further \r\nimprove the image change detection performance (Section \\ref{sec:post_processing}). \r\nThe individual modules are detailed as followings.\r\n\r\n\\figC\r\n\r\n\\subsection{Dataset Collection}\\label{sec:lrfslam}\r\n\r\n\r\nFigure \\ref{fig:mobile_robot}\r\nshows \r\nthe indoor robot experimental platform.\r\nWe employ\r\nLRF-SLAM in \\cite{lrfslam}\r\nas a method for aligning live images with the reference images.\r\nAn input live image \r\nis paired with \r\na reference image if its angle deviation from the live image is less than the threshold \r\nof 1 degree.\r\nIf no such \r\nreference image exists, \r\nit \r\nis paired\r\nwith the nearest neighbor viewpoint \r\nto the live image's viewpoint,\r\nwithout considering the angle information.\r\n\r\n\r\n\\figD\r\n\r\n\\subsection{Pixel Warping}\r\n\r\n\\label{sec:pixel_warping}\r\nWe further compensate \r\nfor the viewpoint misalignment in LRF-SLAM by introducing an image warping technique.\r\nA warp is a 2D function, $u(x, y)$, \r\nwhich maps a position $(x, y)$ in the reference image to \r\na position $u=(x', y')$ in the live image.\r\nDense image alignment, which is recently proposed in \\cite{truong2021learning}, is employed to find an appropriate warp, \r\nby minimizing an energy function in the form: \r\n\\begin{equation}\r\n-\\log p(Y | \\Phi(X;\\theta))= \\sum_{ij}\\log p(y_{ij}|\\varphi_{ij}(X;\\theta))\r\n\\end{equation}\r\nwhere \r\n$X$ is input image pair $X = (I^q, I^r)$, \r\n$Y$ is ground-truth flow, $\\Phi$ and $\\varphi$ are predicted parameters.\r\nAn example of pixel warping is shown in Fig. \\ref{fig:pixel_warp}. \r\n\r\n\\figF\r\n\r\n\\subsection{Attention Mask Generation}\r\n\r\n\\label{sec:attention_mask_gen}\r\n\r\n\r\nWe here introduce a novel domain-invariant attention mask (Fig. \\ref{fig:attention_mask}), \r\ninspired by self-attention mechanism \\cite{dosovitskiy2020image}.\r\nRecall that\r\nin\r\nstandard self-attention \\cite{vaswani2017attention},\r\nthe interrelationships of the elements in the sequence\r\nare obtained\r\nby computing a weighted sum over all values ${\\bf v}$ in the sequence \r\nfor each element in an input sequence ${\\bf z} \\in R^{N \\times D}$. \r\nThe attention weights  are based on the pairwise similarity between\r\ntwo elements of the sequence and their respective \r\nquery ${\\bf q}$ and key ${\\bf k}$ representations:\r\n\\begin{equation}\r\n  [{\\bf q}, {\\bf k}, {\\bf v}] = {\\bf zU}_{qkv}  \\hspace{2cm}   {\\bf U}_{qkv} \\in \\mathbb{R}^{D \\times 3D_h}\r\n\\end{equation}\r\n\r\n\r\n\\begin{equation}\r\n  \\label{eq:sa}\r\nSA({\\bf z}) = softmax({\\bf qk}^T / \\sqrt{D_h}){\\bf v} \r\n\\end{equation}\r\n\r\nIn the proposed method, \r\nthis {\\it SA} term is replaced with:\r\n\\begin{equation}\r\nProposed({\\bf q_{p}},{\\bf k_{p}},{\\bf m_{cnn}}) = PatchMatch({\\bf q_{p}},{\\bf k_{p}}) \\odot {\\bf m_{cnn}}.\r\n\\end{equation}\r\nHere, ${\\bf q_{p}} \\in \\mathbb{R}^{h_{np} \\times w_{np} \\times D_p}$ \r\nand \r\n${\\bf k_{p}} \\in \\mathbb{R}^{h_{np} \\times w_{np} \\times D_p}$ \r\nare patches extracted from live and reference images, respectively. \r\n${\\bf m_{cnn}} \\in \\mathbb{R}^{h_{cnn} \\times w_{cnn} \\times D_{cnn}}$\r\nis an intermediate feature of the Siamese CNN. The $PatchMatch$ is \r\nthe function \r\nthat predicts whether \r\nor not a pair of $D_p$-dim vectors of ${\\bf q_{p}}$ and ${\\bf k_{p}}$ match.\r\n\r\nWe generate a binary attention mask by incorporating the attention mechanism.\r\nFirst,\r\nthe image is reshaped into a sequence of 2D patches, each of which is described by a local feature vector.\r\nWe employ the 128-dim deep PatchNetVLAD \\cite{Hausler_2021_CVPR}\r\ndescriptor as the local feature vector.\r\nThe attention score is then computed for each region of interest. \r\nWe \r\nthen \r\nevaluate\r\nthe attention score\r\nas the \r\npatch-wise \r\ndissimilarity (i.e., L2 distance) \r\nbetween \r\nlive and reference image pairs.\r\nThen,\r\nRANSAC geometric verification is performed to filter out\r\nfalse alarms\r\nthat are \r\noriginated from change patches.\r\nFinally,\r\nwe \r\nobtain\r\nthe attention regions\r\nas \r\nscattered discrete regions of\r\nlive patches\r\nwith positive attention score,\r\nwhich\r\nmakes a binary attention mask.\r\n\r\n\r\n\\algA\r\n\r\nAlgorithm \\ref{alg-attention-mask_gen} presents the algorithm for creating the attention mask.\r\nIt aims to \r\ncompute \r\na binary \r\nattention mask \r\n$Mask$\r\nfor \r\nan array of\r\n$W_p\\times H_p$\r\npatches\r\nat time instance $t$\r\nfrom\r\na sequence of live images within \r\nthe time interval $[t-T, t+T]$.\r\nThe algorithm\r\nbegins with \r\nthe initialization of the mask variable $Mask$,\r\nand \r\niterates for each live image,\r\nthe following steps:\r\nFirst,\r\nit extracts \r\nfrom an input image\r\na set of \r\nPatchNetVLAD feature vectors (``$ExtractPF$''),\r\neach of which \r\nbelongs to one of reference patches.\r\nThen,\r\nfor each live feature, \r\nit searches\r\nfor\r\nits mutual nearest neighbor \r\n(``$MNN$\")\r\nreference patch \r\nin terms of the L2 norm of their PatchNetVLAD features.\r\nHere,\r\nthe mutual nearest neighbor search is defined as the process of searching for pairs of matching live and reference elements that \r\nare closest to each other.\r\nOnly feature pairs that have passed the mutual nearest neighbor search are sent to the next RANSAC process.\r\nThen,\r\nit performs \r\ngeometric verification by RANSAC \\cite{ransac} (``$RANSAC$\").\r\nFinally,\r\nit outputs\r\npixel\r\nwith\r\nvalues greater than or equal to threshold,\r\nin the form of the binary attention mask:\r\n\\begin{equation}\r\n\\label{eq:binary_elem}\r\n\\mathbf{b}[i,j] = \\left\\{\r\n\\begin{array}{ll}\r\n1 & \\mbox{If $inliers[i,j]$ passed RANSAC}\\\\\r\n0 & \\mbox{Otherwise}\r\n\\end{array}\r\n\\right.\r\n.\r\n\\end{equation}\r\n\r\n\r\n\r\n\r\n\r\n\r\n\\subsection{Attention Mask Layer}\\label{sec:attention_layer}\r\n\r\nWe \r\nnow \r\ninsert \r\nthe attention mask \r\ninto the standard\r\nimage change detection model \r\nof the Siamese CNN (Section \\ref{sec:attention_mask_gen}). \r\nFor the Siamese CNN, we use the state-of-the-art architecture of CSCDNet \\cite{sakurada2020weakly}. \r\nThe attention mask layer takes the CNN feature map and attention mask as inputs and outputs the CNN features masked in the channel direction. \r\nWe inserted the attention mask before correlation operation (i.e., before concatenating decoded feature).\r\n\r\nWe perform the process of masking the CNN Siamese feature map in the channel direction.\r\nLet\r\n$\\mathbf{fmap_{new}} \\in \\mathbf{R}^{W \\times H}$ \r\ndenote the feature map after attention is applied. \r\nLet\r\n$\\mathbf{fmap_{old}} \\in {R}^{W \\times H \\times C}$ \r\ndenote the feature map obtained from Siamese CNN. \r\nHere, $W$ denote the tensor width, \r\n$H$ denote the tensor height, \r\nand $C$ denote the tensor channel. \r\nLet\r\n$\\mathbf{mask} \\in \\mathbf{R}^{W \\times H}$ \r\ndenote the attention mask. \r\nThen,\r\nthe attention mask element \r\nat the $i$-th row,\r\n$j$-th column\r\nand $k$-th channel is:\r\n\\begin{equation}\r\n\\label{eq:merge}\r\n  \\mathbf{fmap_{new}}[i,j,k] = \\mathbf{fmap_{old}}[i,j,k] \\cdot \\mathbf{mask}[i,j]. \r\n\\end{equation}\r\nThis operation is applied to the both branches of the Siamese CNN.\r\n\r\n\r\n\r\n\r\n\\subsection{Post Processing}\\label{sec:post_processing}\r\n\r\nPost-processing is introduced to eliminate false alarms in the detection results.\r\nWe evaluate\r\nthe uncertainty \r\nin the output layer of \r\nthe dense image alignment model\r\nand use it to evaluate\r\nthe confidence of prediction at each pixel.\r\nIntuitively,\r\na high probability of pixel warping uncertainty \r\nindicates \r\nthat no corresponding pixel exists; \r\ntherefore is a high possibility of change. \r\nConversely, \r\nlow probability\r\nindicates that the corresponding pixel exists; \r\ntherefore is a low possibility of change. \r\nThis masking process can be simply expressed as \r\nan \r\nHadamard product\r\noperation, in the following form:\r\n\\begin{equation}\r\n\\label{eq:unc_merge}\r\n\\mathbf{output_{new}}[i,j] = \\mathbf{output_{old}}[i,j] \\cdot \\mathbf{uncertainty}[i,j].\r\n\\end{equation}\r\nHere, $\\mathbf{output_{old}} \\in \\mathbf{R}^{W \\times H}$ represents the change probability value map of the output of \r\nthe Siamese CNN. \r\n$\\mathbf{uncertainty} \\in \\mathbf{R}^{W \\times H}$ represents the uncertainty of each pixel warp of a live image. \r\n$\\mathbf{output_{new}} \\in \\mathbf{R}^{W \\times H}$  represents the probability of change for each pixel after the merging process. \r\n\r\n\r\n\r\n\\section{Evaluation Experiments}\r\n\r\n\\subsection{Dataset}\r\n\r\nWe collected \r\nfour datasets,\r\n``convenience store,''\r\n``flooring,''\r\n``office room,''\r\nand ``hallway,''\r\nin four distinctive environments.\r\nEight independent image sequences \r\nare collected\r\nfrom the four different environments.\r\nThe number of images\r\nare\r\n534, 491, 378, and 395,\r\nrespectively\r\nfor\r\n``flooring,\" \r\n``convenience store,\" \r\n``office,\" \r\nand\r\n``hallway\".\r\nExamples of these images are shown in Fig. \\ref{fig:dataset}.\r\nThe image size was $640\\times 480$. \r\nThe \r\nground-truth change object regions \r\nin each live image\r\nare manually annotated\r\nusing PaddleSeg \\cite{liu2021paddleseg, paddleseg2019}\r\nas the annotation tool.\r\n\r\n\r\n\r\n\\figE\r\n\r\n\r\n\r\n\\subsection{Settings}\r\n\r\nThe state-of-the-art model,\r\nCSCDNet \\cite{sakurada2020weakly},\r\nis used as our base architecture,\r\nwhich we aim to boost in this study.\r\nIt is also used as a comparing method to verify \r\nwhether the proposed method \r\ncan actually boost the CSCDNet.\r\nThe network is initialized \r\nwith the weight pre-trained on ImageNet \\cite{deng2009imagenet}.\r\nThe pixel-wise binary cross-entropy loss is used as loss function as in the original work of CSCDNet \\cite{sakurada2020weakly}. \r\nPDCNet \\cite{truong2021learning}\r\nis used to align reference images.\r\nAdam optimizer \\cite{kingma2014adam} \r\nis used for the network training.\r\nLearning rate is 0.0001. \r\nThe number of iterations is 20,000.\r\nThe batch size is 32.\r\nA nearest neighbor interpolation\r\nis used \r\nto resize the attention mask\r\nto \r\nfit into the attention mask layer. \r\nThe length of reference image sequence is set to $T=10$ in default.\r\nA single NVIDIA GeForce RTX 3090 GPU \r\nwith PyTorch framework is used. \r\nPixel-wise precision, recall, and F1 score are used as performance index.\r\n\r\n\r\n\\figG\r\n\r\n\r\n\\subsection{Quantitative Results}\r\n\r\nFigure \\ref{fig:Performance}\r\nshows performance results. \r\nAs can be seen,\r\nthe proposed method \r\noutperformed the comparing method\r\nfor almost all combinations \r\nof training and test sets\r\nconsidered here.\r\nNotably, the proposed method \r\nextremely\r\noutperformed the comparing method when it was trained on the ``flooring'' dataset. \r\nThe ``flooring'' is the simplest background scene. \r\nTherefore, the image change detection model trained on that could be generalized to other complex domains as well. \r\nHowever, the proposed method performed almost the same as the comparing method when it was trained on the other complex background scenes. \r\nAs an example, for the convenience store dataset, \r\nthe robot navigates through a narrow and messy passage that makes its visual appearance very different from that of the other two datasets.\r\nThis makes \r\nthe \r\nproposed\r\ntraining\r\nalgorithm\r\nless effective for \r\nthe training set.\r\nIt is noteworthy\r\nthat \r\nsuch an effect\r\nof visual appearance \r\nmight be \r\nmitigated by introducing\r\nview synthesis technique such as \\cite{kupyn2019deblurgan},\r\nwhich is a direction of future research.\r\n\r\n\r\n\r\n\\figH\r\n\r\n\\subsection{Qualitative Results}\r\n\r\nFigure \\ref{fig:examples} shows example results\r\nin which \r\nthe proposed attention mechanism\r\nwas \r\noften\r\nsuccessful \r\nin improving the performance of the CSCDNet.\r\nEspecially,\r\nthe proposed method\r\nwas effective for \r\ncomplex background scenes,\r\nowing to the ability of the proposed attention mechanism\r\nto make use of contextual information.\r\n\r\n\r\n\\figI\r\n\r\n\\subsection{Ablation Study}\r\n\r\nTable \\ref{Table:ablation_warp_unc}\r\npresents the results of \r\na series of ablation studies \r\nby turning off some of the modules in the proposed framework.\r\nFor all ablations, the flooring dataset is used as the training data.\r\nFor the ``convenience store'' dataset,\r\nthe performance is \r\nsignificantly higher\r\nwith than without \r\nthe post-processing technique\r\nin Section \\ref{sec:post_processing}.\r\nBecause of the complex background of this dataset, \r\npixel warping often fails, \r\nand the proposed method was effective in suppressing \r\nsuch effects\r\nthat are originated from complex backgrounds.\r\nFor the ``office'' and ``hallway'' datasets,\r\nthe performance is almost the same \r\nbetween\r\nwith and without\r\nthe technique.\r\nSince the background was less complex in these datasets, \r\npixel warp failures were less common, \r\ntherefore the effect of estimating uncertainty was small. \r\nNext, another ablation with different settings of the length of reference image sequences are conducted. \r\nAs can be seen, the performance is best at $T$=10.\r\nAs expected, higher performance was obtained with longer reference sequences.\r\nFrom the above results,\r\nit could be concluded that \r\nboth of PDCNet and pixel warping \r\nplay important roles\r\nand\r\ncan actually improve the performance\r\nof image change detection.\r\n\r\n\r\n\r\n\r\n\\section{Conclusions}\r\n\r\nIn this research, \r\nwe tackled the challenging problem of small object change \r\ndetection \r\nvia everyday indoor robot navigation.\r\nWe proposed\r\na new self-attention technique with \r\nunsupervised on-the-fly domain adaptation,\r\nby introducing an attention mask into the intermediate layer of \r\nan image change detection model,\r\nwithout modifying the input and output layers\r\nof the model.\r\nExperiments using a novel dataset on small object change detection verified \r\nthat\r\nthe proposed method significantly boosted the state-of-the-art model for image change detection.\r\n\r\n\r\n\r\n\r\n", "meta": {"timestamp": "2022-04-26T02:32:40", "yymm": "2203", "arxiv_id": "2203.15362", "language": "en", "url": "https://arxiv.org/abs/2203.15362"}}
{"text": "\\section{Related Work}\n\\label{sec:related_work}\n\\textbf{Hybrid networks.} The idea originated in research presented in \\cite{oyallon2017firstHybrid} where E. Oyallon et al. experimented with replacing initial layers of ResNet \\cite{he2016resNet} and Wide ResNet \\cite{zagoruyko2016wrn} with a scattering transform. This approach allowed deeper analysis and better interpretation of complex deep learning techniques and demonstrated that predefined filters grant theoretical guarantees required for building better networks and more stable representations. Presented results saw a sizeable improvement as compared to previous scattering-based approaches \\cite{brunaMallat2013invariant_scatSVM}. Despite that, the performance with respect to fully-trained counterparts was still low. However, the promising properties of hybrid networks encouraged research to continue and alternative approaches were introduced. For example, Dual-Tree Complex Wavelet Transform-based network \\cite{singh2017dtcwScat} or a Locally Invariant Convolutional Layer \\cite{cotter2019learnableScat} which pioneered learning in the scattering domain. Although novel approaches including those reviewed in \\cite{minskiy2021scatReview} saw further performance improvement, the issue of a significant performance gap between hybrid and fully-trained networks remained.\n\n\\textbf{Hand-crafted features.} A wide variety of predefined filters has been applied to hybrid designs, most of which are wavelet-based. Researchers employed both discrete and continuous wavelets including Haar \\cite{fujieda2018waveletCNN}, Gabor \\cite{czaja2019gaborHybrid}, and Morlet \\cite{mallat2012scatTransform} wavelets. Later, it was demonstrated that wavelets are most efficient when they are used in a layered CNN-like configuration, such as Scattering Transform \\cite{brunaMallat2013invariant_scatSVM} and Dual-Tree Complex Wavelet Transform \\cite{selesnick2005dtcwt}.\n\nFollowing the approach of E. Oyallon et al. \\cite{oyallon2017firstHybrid}, we employ scattering transform, $S$, as the hand-crafted part for our hybrid network. It is formed by cascading wavelet transforms and modulus nonlinearity functions \\cite{mallat2012scatTransform}. It guarantees translation invariance and linear response to deformations by separating the information along with multiple scales and orientations. As this work focuses on image classification, the wavelets are calculated from a zero-average complex wavelet $\\psi$ as follows \\cite{zarka2019scatDictonary}:\n\n\\begin{equation*}\n    \\psi_{j,\\theta}(u) = 2^{-2j}\\psi(2^{-j}r_{-\\theta}u),\n\\end{equation*}\n\\begin{equation*}\n    \\psi_{j,\\theta,\\alpha} = Real(e^{-i\\alpha}\\psi_{j,\\theta})\n\\end{equation*}\n\nwhere $r_{-\\theta}$ defines rotation, $2^j$ defines dilation, and $\\alpha$ - phase shift.\nIn the proposed design we employ the first-order scattering coefficients which, for a given image $x$, are calculated by averaging rectified wavelet coefficients with a sub-sampling stride of $2^J$:\n\\begin{equation*}\nSx(u,k,\\alpha) = |x\\star\\psi_{j,\\theta,\\alpha}|\\star\\phi_J(2^Ju); k=(j, \\theta)\n\\end{equation*}\nwhere non-linearity is obtained with a complex modulus function and $\\phi_J$ is a Gaussian dilated by $2^J$ \\cite{brunaMallat2013invariant_scatSVM} that eliminates the variations at scales smaller than $2^J$. Further details on the parameter selection are available in section \\ref{sec:taining_details}.\n\n\n\n\n\n\\section{Building an Inductive Hybrid Network}\n\\label{sec:building_an_e-hybrid_network}\n\nTo address the weaknesses of existing hybrids, such as lacking performance and adaptability, we introduce an inductive approach as an alternative to the currently popular stacking paradigm. Conceptually, instead of replacing initial network stages, we enrich the backbone data flow along the entire network path with the output of a scattering transform. By design, this fusion allows scattering features to guide the training process, while the network can adaptively regulate their effect using the deep learning strategy. As a result, our architecture benefits from well-defined and informative hand-crafted features, as well as from an adaptable and flexible deep learning-based approach. The following two sections detail the building blocks of the proposed architecture and explain our design decisions.\n\n\\subsection{Hybrid Fusion Blocks}\n\\label{sec:hybrid_blocks}\nHybrid architecture furnishes a network with two complementary types of representations: fixed, hand-crafted with defined properties and learnt features that are adaptable to the data. The biggest challenge, then, is to fuse those while preserving the necessary information to maximise the classification performance. In our design, Hybrid Fusion Blocks enable that and steer the training procedure more effectively by drawing from the strengths of both types of features.\n\nHybrid Fusion Blocks are designed as encapsulated units and cause no additional disruption to the main network data flow. Figure \\ref{fig:hybrid_blocks} illustrates how two streams of coefficients are embedded into a single tensor identical in shape to the output of the previous network layer. First, we perform feature expansion by concatenating scattering activations with an output of the previous layer, both of which are batch normalised. Then, inspired by the highly efficient MBConv block \\cite{sandler2018mobilenetv2} we apply depth-wise convolution with a 3x3 kernel, followed by the Squeeze and Excitation (SE) stage. SE procedure allows the network to perform a re-adjustment through which it selectively emphasises informative features and suppresses less descriptive ones \\cite{hu2018SEblock} enabling the network to prioritise scattering representations early in the process and regulate their effect as training progresses. Hybrid Fusion Block is concluded with point-wise convolution.\n\nThe above-described is the default Hybrid Fusion architecture (HF-$E$), we also introduce two alternatives to test the effects of batch normalisation during feature expansion and depth-wise convolution. This way, the HF-$H$ block has batch normalisation disabled in the first stage. While HF-$Z$ omits the depth-wise convolution procedure as shown in Figure \\ref{fig:hybrid_blocks}.\n\nAdditionally, to assess the influence of  DropConnect \\cite{wan2013dropConnect} and skip connection features when applied to the HF block, we evaluate three sub-variations for each of the three architectures. Option 0: without DropConnect and no skip connection; (1) no DropConnect with a skip connection; (2) with both DropConnect a skip connection enabled. Hence, the total number of block variations is nine, all of which are listed in Table \\ref{tab:all_results}. \n\n\n\\begin{table}[!b]\n\\newcolumntype{C}[1]{>{\\hsize=#1\\hsize\\centering\\arraybackslash}X}\n    \\begin{tabularx}{\\linewidth}{ l c c }\n      \\hline\n      Operator & Output Res & \\# Channels \\\\\n      \\hline\n       1. Conv3x3 & 112x112 & 32 \\\\\n       2. MBConv1, 3x3 & 56x56 & 16 \\\\\n       \\textbf{3. HF-1, 3x3} & \\textbf{56x56} & \\textbf{24} \\\\\n       4. MBConv6, 3x3 & 28x28 & 24 \\\\\n       \\textbf{5. HF-2, 3x3} & \\textbf{28x28} & \\textbf{40} \\\\\n       6. MBConv6, 5x5 & 14x14 & 40 \\\\\n       7. MBConv6, 3x3 & 14x14 & 80 \\\\\n       8. MBConv6, 5x5 & 7x7 & 112 \\\\\n       9. MBConv6, 5x5 & 7x7 & 192 \\\\\n       10. MBConv6, 3x3 & 7x7 & 320 \\\\\n       11. Conv1x1, Pooling, FC & 7x7 & 1280 \\\\\n      \\hline\n    \\end{tabularx}\n    \\caption{Overview of E-HybridNet-$E$ architecture for an input of resolution 224x224 pixels.}\n    \\label{tab:hybryd_dataflow}\n\\end{table}\n\n\\subsection{Scalable Network Architecture}\n\\label{sec:network_architecture}\n\nOur feature embedding approach requires careful spatial feature resolution management, which restricts a selection of compatible backbone networks. The primary challenge is to align the two streams of features, network activations and scattering transform output, with respect to their spatial resolution.\n\nOne of the networks that meet this requirement is EfficnetNet \\cite{tan2019efficientnet}. Scattering networks can be parameterised (as per section \\ref{sec:taining_details}) to achieve alignment with respect to the resolution of the corresponding spatial features. Although other architectures previously employed with hybrids such as ResNet \\cite{he2016resNet, oyallon2018firdtOrderScat}, Wide-ResNet \\cite{zagoruyko2016wrn, oyallon2018scatAnalsysis} and VGG \\cite{simonyan2014vgg, cotter2019learnableScat} also fulfill our resolution requirement, EfficientNet seem favorable due to its high performance-complexity trade-off, flexibility and wide recognition in applications related to image classification. \n\n Previous research showed \\cite{oyallon2018scatAnalsysis, cotter2019learnableScat} that scattering features are most effective when applied to early CNN stages. Hence, to build E-HybridNet we insert one Hybrid Fusion Block (HF-1) before original stage 3 of EfficientNet and another one (HF-2) just after it. Table \\ref{tab:hybryd_dataflow} portrays the E-HybridNet-B0-$E$ architecture and demonstrates the integration of two Hybrid Fusion Blocks. We apply a two-step process to enhance the effect and form a flow of scattering features. \n \n Importantly, due to the flexibility of the backbone network and adaptable HF block design, E-HybridNet is a highly scalable architecture. Unlike other existing hybrid networks, its complexity can easily be adjusted without re-configuring the predefined part.\n \n\n\n\\begin{table*}[!ht]\n    \\centering \n    \\begin{tabular}{ l| c c c c }\n      \\hline\n       Network Type & Caltech-256, \\% & Flowers-102, \\% & CoronaHack-2, \\% & CoronaHack-3, \\%\\\\\n       \\hline\n        \\textbf{EfficientNet-B0} & 54.23 & 94.63 & 92.58 & 85.66 \\\\\n        E-HybridNet-B0-$E$0 & \\textbf{60.24} & \\textbf{97.75} & 95.10 & 87.73 \\\\\n        E-HybridNet-B0-$E$1 & 60.08 & 96.52 & \\textbf{95.49} & \\textbf{87.82} \\\\\n        E-HybridNet-B0-$E$3 & 58.92 & 97.63 & 95.21 & 86.27 \\\\\n       \\hline\n        E-HybridNet-B0-$Z$0 & 59.50 & 97.12 & 94.09 & 84.24 \\\\\n        E-HybridNet-B0-$Z$1 & \\textbf{60.27} & 96.73 & 93.86 & \\textbf{89.44} \\\\\n        E-HybridNet-B0-$Z$3 & 57.63 & \\textbf{97.63} & \\textbf{94.32} & 85.82 \\\\\n       \\hline\n        E-HybridNet-B0-$H$0 & \\textbf{60.09} & 96.52 & \\textbf{95.15} & 80.49 \\\\\n        E-HybridNet-B0-$H$1 & 58.94 & 96.14 & 94.85 & \\textbf{83.52} \\\\\n        E-HybridNet-B0-$H$3 & 59.93 & \\textbf{97.03} & 92.73 & 81.46 \\\\\n       \\hline\n        \\textbf{EfficientNet-B3} & 49.07 & 96.35 & 92.35 & 80.56 \\\\\n        E-HybridNet-B3-$E$0 & 52.40 & 96.55 & \\textbf{95.19} & 85.39 \\\\\n        E-HybridNet-B3-$E$1 & 54.88 & 96.84 & 93.59 & 86.35 \\\\\n        E-HybridNet-B3-$E$3 & \\textbf{56.78} & \\textbf{97.17} & 94.38 & \\textbf{87.44} \\\\\n       \\hline\n        E-HybridNet-B3-$Z$0 & \\textbf{56.23} & \\textbf{96.75} & \\textbf{94.86} & 85.00 \\\\\n        E-HybridNet-B3-$Z$1 & 52.53 & 96.96 & 93.22 & \\textbf{86.68} \\\\\n        E-HybridNet-B3-$Z$3 & 54.99 & 96.42 & 94.21 & 85.58 \\\\\n       \\hline\n        E-HybridNet-B3-$H$0 & \\textbf{55.83} & 96.95 & \\textbf{93.31} & 83.93 \\\\\n        E-HybridNet-B3-$H$1 & 54.75 & \\textbf{97.53} & 92.63 & \\textbf{85.07} \\\\\n        E-HybridNet-B3-$H$3 & 53.75 & 96.91 & 92.38 & 84.87 \\\\\n       \\hline\n    \\end{tabular}\n    \\caption{Results for EfficientNet B0, B3 and hybrid networks based on those.}\n    \\label{tab:all_results}\n\\end{table*}\n\n\n\\section{Experimental Setup}\nThis section reviews key elements of the experimental setup and justifies the reasons behind certain decisions.\n\n\\subsection{Datasets}\n\\label{sec:datasets}\nDatasets were selected based on the following principles: (i) data is representative of a real-world problem; (ii) to form a variety of tasks with generic and specific data; (iii) data is of relatively high image resolution\n; (iv) the size of the dataset is not too large due to a considerable number of required experiments. Following is a brief introduction to each of the datasets employed. \n\nFlowers-102 \\cite{nilsback2008flowers}: a topic-specific and structure intensive dataset. It consists of 6,652 training and 819 test images non-uniformly split across 102 classes. CoronaHack \\cite{cohen2020coronaHack} is a medical dataset that consists of a mixture of CT scans and X-ray images. It features 5,309 training and 624 test images that can be split either into 2 (disease, no disease) or 3 (virus, bacteria and healthy) categories. We test our networks on both variations. Caltech-256 \\cite{griffin2006caltech256} presents 256 imbalanced categories of real-life objects, it has a training set of 23,824 images and a testing set of 5,956.\n\n\\textbf{Evaluation metric.} As all datasets are imbalanced, the often-used accuracy metric would not be representative as it is highly sensitive to that. Hence, mean Average Precision (mAP) is employed as the main evaluation metric and all results presented in the following discussions will refer to mAP.\n\n\\subsection{Training Details}\n\\label{sec:taining_details}\n\\textbf{Training procedure.} The primary challenge was to create an environment to fairly compare a wide variety of networks. For this, we followed an approach suggested in \\cite{minskiy2021scatReview} and employed a standard training procedure across all experiments. To remove the optimisation bias, we trained all networks from scratch and kept most parameters constant. These include cross-entropy loss function, cosine-annealing scheduler with a step size equal to the number of epochs and SGD optimisation strategy. A minority of parameters varied depending on the dataset, such as the number of epochs, batch size and initial learning rate. More details are available in the project's GitHub directory: https://github.com/dminskiy/EHybridNet-icpr2022.\n\n\\textbf{Scattering features.} To ensure the scattering coefficients align with the EfficientNet backbone in terms of the feature spatial size as per section \\ref{sec:network_architecture} the following scattering configurations were employed. For the first Hybrid Fusion Block, HF-1, $J$ was set to 2, whereas for the second, HF-2, $J=3$. The common parameters for both layers are the scattering order of 1, 8 angles $\\theta$ and 4 phases $\\alpha$. Scattering coefficients are computed with a modified to allow multi-GPU support Kymatio \\cite{andreux2020kymatio} package, please see the project's directory for details.\n\n\n\\begin{figure}[!b]\n\\centerline{\\includegraphics[width=\\linewidth]{media/comparison_of_HF_blocks.png}}\n\\caption{Performance comparison of Hybrid Fusion Block architectures.}\n\\label{fig:nets_outperformed_hybrids_B0&3}\n\\end{figure}\n\n\n\\begin{table*}[!ht]\n    \\centering \n    \\begin{tabular}{ l| c c| c c| c c  }\n      \\toprule\n      \\multirow{2}{*}{Dataset} &\n      \\multicolumn{2}{c|}{All of training data} &\n      \\multicolumn{2}{c|}{50\\% of training data} &\n      \\multicolumn{2}{c}{25\\% of training data} \\\\\n      & {Hybrid, \\%} & {EfficientNet, \\%} & {Hybrid, \\%} & {EfficientNet, \\%} & {Hybrid, \\%} & {EfficientNet, \\%} \\\\\n      \\midrule\n       Caltech-256 & \\textbf{60.24} & 54.23 & \\textbf{54.60} & 46.68 & \\textbf{45.81} & 38.03 \\\\\n       Flowers-102 & \\textbf{97.75} & 94.63 & \\textbf{95.81} & 91.88 & \\textbf{92.14} & 85.52 \\\\\n       CoronaHack-2 & \\textbf{95.10} & 92.58 & \\textbf{94.93} & 90.85 & \\textbf{92.70} & 88.92 \\\\\n       CoronaHack-3 & \\textbf{87.73} & 85.66 & \\textbf{85.79} & 85.26 & \\textbf{79.32} & 78.42 \\\\\n      \\bottomrule\n    \\end{tabular}\n    \\caption{Results with limited training data for E-HybridNet-B0-$E$0 and EfficientNet-B0.}\n    \\label{tab:limited_data_results}\n\\end{table*}\n\n\n\\section{Evaluation and Analysis}\n\nIn this section, we first explore the properties of the E-HybridNet, then evaluate its performance against the baseline EfficientNet. Finally, we assess the effect of scattering features on the network's efficiency in normal and data-limited conditions.  \n\n\\subsection{E-HybridNet Performance}\n\\label{sec:hybrid_performance}\n\n\\textbf{Hybrid Fusion Block architecture}.\nFirst, we evaluate the effects of different design choices on the overall network performance, concentrating on the comparison of three primary design groups (HF-$E$, HF-$Z$ and HF-$H$) as per section \\ref{sec:hybrid_blocks}. The impact of skip connections and DropConnect is considered later in the section.\n\nTo select the best architecture, we count the number of networks each of the candidate hybrid designs has outperformed and accumulate it across our four evaluation datasets and two network complexity configurations (B0 and B3), results are shown in figure \\ref{fig:nets_outperformed_hybrids_B0&3}. To select the best performing design, we group the obtained results based on Hybrid Fusion Block variations ($E$, $Z$ or $H$). Thereby, we observe that HF-$E$ based networks outperformed 44.1\\% of other hybrids, followed by HF-$Z$ design (better in 32.99\\% cases) and HF-$H$ (22.92\\%). These findings indicate the importance of batch normalisation before the features are concatenated, as this is the only difference between the leading HF-$E$ and the least effective HF-$H$ designs. Similarly, the depth-wise convolution stage proved effective as it allowed HF-$E$ architecture to be over 10\\% more efficient than the HF-$Z$ design that does not have this feature.\n\n\\textbf{Hybrid Fusion Block variations.}\nDropConnect and skip connection modules are present within the original EfficientNet architecture and play an important role in its success. Here, we analyse their effect on the E-HybrdNet, for this we apply the same method as before and count the number of peer-hybrids each individual architecture outperforms.\n\nFigure \\ref{fig:nets_outperformed_hybrids_B0&3} presents the aggregate result between B0 and B3-based networks and shows that the effect of these features depends on the fusion block architecture. The skip connection module, on average, did not change the performance of Hf-$E$ networks, for both HF-$E$0 and HF-$E$1 the number of outperformed networks remained the same, at 13.54\\%. HF-$Z$ hybrids, on the other hand, benefited from the addition of the skip connection - the success rate improved by 1.74\\%. Whereas, HF-$H$ architecture saw a drop in performance with the addition of this feature (-1.39\\%). As for the DropConnect, the presence of this module took HF-$E$3 design to 17.01\\%, the highest amongst all networks. However, its effect on other architectures was negative, reducing the performance by 2.78\\% and 2.43\\% for HF-$Z$ and HF-$H$ respectively.\n\nIn summary, we observe that DropConnect and skip connection modules can improve the adaptability of the network and they are most effective with the base E-hybridNet architecture - HF-$E$.\n\n\\textbf{Comparison with EfficientNet.}\nThe major drawback of previous hybrid architectures is their relatively low classification performance as compared to their conventional counterparts. Hence, in this experiment, we compare the performance of the base hybrid architecture (HF-$E$0) against the vanilla EfficientNet in B0 and B3 configurations applied to four diverse datasets.\n\nResults presented in Table \\ref{tab:all_results} evidence the dominance of the hybrid that is on average 3.43\\% more accurate than the reference network with a maximum gap of over 6\\% in B0 setup. When the backbone complexity is increased to B3, the average hybrid's advantage is 2.8\\% with a maximum of 4.83\\% To the best of our knowledge, it is the first time a hybrid network could consistently outperform the baseline network on a variety of tasks when entire training data is available.\n\n\\begin{figure}[!b]\n\\centerline{\\includegraphics[width=\\linewidth]{media/ablation_study.png}}\n\\caption{Results of the ablation study.}\n\\label{fig:deactivation}\n\\end{figure}\n\n\\subsection{Ablation Study}\n\\label{sec:Effect_of_Scattering_Features}\nHere, we evaluate the contribution of scattering features to the performance of our hybrid design. As our HF block fuses scattering coefficients with an output of the corresponding network layer (network features, in short), an idea of the ablation study, therefore, is in disabling parts of the Hybrid Fusion Block. In one case, the scattering features are omitted from the merging stage while, in the other, the network features fed into the block will be ignored. As before, we use HF-$E$0 hybrid configuration for evaluation.\n\nFigure \\ref{fig:deactivation} summarises the results of this experiment across four datasets and following networks: EfficientNet-B0 (Reference), E-HybridNet-B0-$E$0 (Hybrid), E-HybridNet-B0-$E$0 with scattering features omitted (Hybrid Scat Disabled) and E-HybridNet-B0-$E$0 with network features ignored (Hybrid Net Disabled). Across the four datasets, the average loss in performance due to the lack of scattering features is 5.08\\%, while the average performance drop due to the network features not being present is 1.48\\% as compared to the baseline hybrid result. This indicates the significance of scattering features in hybrid architecture and shows that they are, indeed, the driving factor for its success. Additionally, we note that the average performance loss caused by disabling both hybrid and networks features (3.64\\%) is approximately equal to the average gain of the hybrid versus the EfficientNet-B0 (3.76\\%) architecture. This can be interpreted as \"disabling\" both parts of the Hybrid Fusion Block (or effectively removing the hybrid part of the network) is the same as running the standard EfficientNet-B0. Importantly, most of the loss when moving from hybrid to standard design is due to a lack of scattering features.\n\n\\subsection{Generalisation}\n\\label{sec:generalsiation}\nHistorically, hybrids' ability to generalise from small amounts of data have been their strongest advantage over their fully-trained counterparts. To validate that our design preserves this strength, we compare the base hybrid network (HF-$E$0) and its baseline EffcientNet-B0 in data-limited scenarios. We use our four evaluation datasets with 50\\% and 75\\% of the training samples removed randomly.\n\nFrom results presented in Table \\ref{tab:limited_data_results}, we observe that the hybrid network was constantly ahead of EfficientNet-B0. On average, E-HybridNet was 3.43\\%, 4.11\\% and 4.77\\% more accurate than the reference architecture with full, half and quarter of the training set available. It corresponds to approximately 0.7\\% of relative gain for the hybrid as the amount of data is halved. Hence, we conclude that our fusion procedure embeds scattering features effectively, aids CNN generalisation and improves overall performance in data-limited scenarios.\n\n\\section{Conclusions}\n\nIn this work, we introduced the E-Hybrid network, the first scattering based hybrid network that consistently outperformed its conventional counterparts. The evaluation showed that our design improves mAP by up to 6\\% with an average gain of 3.11\\% across tested datasets.\nWe also presented Hybrid Fusion Blocks, the integral part of the novel design. They allow a network to benefit from the adaptability of CNNs while being guided by powerful hand-crafted representations.\nOur analysis showed that scattering features were indeed the driving force behind the success of the proposed hybrid design and constituted a major part of the performance gain. They also allowed E-HybridNets to be superior in data-limited scenarios, considerably improving the network's generalisation.\n\n\n\n\\textbf{Current limitations and further work.} Despite the advantages of the novel architecture, certain aspects of it provide further research opportunities. For instance, currently, each Hybrid Fusion Block requires recalculation of scattering features which slows inference and training speeds by around 60\\%. Another important area that has not been fully addressed yet is training optimisation for E-Hybrid networks, so far they have been tested mostly under a common setup strategy to facilitate the concept verification and enable a fair comparison. There is also a number of scattering and backbone architectures that could be explored within the proposed architectural paradigm. Therefore, we believe that this work is only a promising start in exploring a wide range of research opportunities in hybrid networks.\n\n\\bibliographystyle{latex12}\n", "meta": {"timestamp": "2022-03-30T02:27:56", "yymm": "2203", "arxiv_id": "2203.15392", "language": "en", "url": "https://arxiv.org/abs/2203.15392"}}
{"text": "\\section{INTRODUCTION}\n\nShipping containers revolutionized the transportation of goods across the globe. Before its introduction, the process of loading and unloading a ship was extremely slow and they had to stay for weeks in the port at a time. Thanks to containerization, the productivity and efficiency of ports have been remarkably enhanced. Moreover, an important performance indicator of a port is the speed at which a ship is unloaded and loaded again \\cite{Kreuzer2014,Sakawa1982}.\n\n\nWith this in mind, the efficiency of port operations deeply depends on the performance of the ship-to-shore (STS) cranes. They are responsible for the critical task of loading and safely unloading the ship \\cite{Kreuzer2014,Sakawa1982,Arena2015}. See \\autoref{fig:sts}. Thus making their operations faster is crucial for the performance indicators of the port.\n\nBecause of this, various approaches have been proposed to address the faster loading and unloading of containers. Some authors have focused on minimizing the swing and other oscillations to minimize the time of the loading and unloading process. For instance, in the works done by Kreuzer \\textit{et. al.,} \\cite{Kreuzer2014} and Kim \\textit{et. al.,} \\cite{kim2004anti}, where the main idea is that a good estimation of the sway angle will make it easier to be mitigated and thus lead to faster operations. In the former, an unscented Kalman filter is designed to provide an accurate estimation of the payload swing. Then, it is validated by applying it in different control techniques. In the latter, an anti-sway controller is designed without the use of a vision system, thus making its implementation cheaper. \n\nFollowing this, optimal control has also been used in crane operations as a combination of both anti-sway and trolley motion control. Sakawa and Shindo \\cite{Sakawa1982} divide the payload motion into five sections and derive an optimal speed reference trajectory, and the swing of the payload is then minimized. Furthermore, Maghsoudi \\textit{et al.,} \\cite{Maghsoudi2016} have used a three-dimensional model of the crane and tuned a PID controller by using a cost function with the aim of fast payload positioning and minimum sway.\n\nMoreover, time-optimal control has been used as a natural approach when one aims to make the loading and unloading as fast as possible. In such approaches, the objective is to minimize the final time $t_f$. That is, the total time for the payload to go from an initial position at rest to a desired final position at rest again.\n\nWith this in mind, Auernig and Troger \\cite{Auernig1987} use the Pontryagin maximum principle and obtain the minimum-time solution for moving the payload. Also, based on differential flatness Chen \\textit{et al.,} \\cite{Chen2016} propose a time-optimal offline trajectory planning method and a tracking controller.\n\nFurthermore, time-optimal control has also been combined with anti-sway. Al-Garni \\textit{et al.,} \\cite{AlGarni1995} use a nonlinear dynamic model and constructs a cost function according to a performance index. This performance index is measured under the condition of making the process of loading or unloading in a minimum time as well as reducing the payload sway. Da Cruz and Leonardi \\cite{DaCruz2012} use linear programming to solve the minimum-time anti-sway motion problem. The solution is obtained by solving a sequence of fixed-time maximum-range problems.\n\nThose all are, without doubt, valuable contributions to the field of control of overhead cranes. However, a problem with much of the literature is that they do not address the different heights that the container stacks can have along the path from the shore to a specific position on the ship. As mentioned before, some works focus on estimating the sway in order to reduce it and consequently the transferring time e.g., \\cite{Kreuzer2014} and \\cite{kim2004anti}. Thus, hoisting movements are naturally not addressed. Furthermore, some works that explicitly deal with minimum time do not address hoisting at all. A constant length of the rope is used in \\cite{Maghsoudi2016} and \\cite{Chen2016}, and it is considered given in \\cite{DaCruz2012}. Moreover, works dealing with hoisting do not take stack heights into account e.g., \\cite{Sakawa1982}, \\cite{Auernig1987}  and \\cite{AlGarni1995}.\n\nIn this paper, we propose a time-optimal control approach to address the problem of loading and unloading a ship in the minimum time. Additionally, the different and arbitrary heights that the container stacks can have are taken into account here. Moreover, the novelty of this work is an easy and intuitive way to represent the stack heights into the optimization problem. This is made possible through a change of variable and reformulation of the optimal control problem. After the variable change, time $t$ becomes a state variable and the payload coordinate along the horizontal axis $x_p$ becomes the free variable. Now with the payload position as the free variable instead of time, the container avoidance constraints will turn into bound constraints when the problem is discretized for the numerical solver. This means that no functional representation of the stack heights is required.\n\nThis paper is organized as follows: \\autoref{sec:modelling_and_original} presents the nonlinear model used to describe the crane movements and the original problem formulation, with time as the free variable. Which has been the standard way to solve this problem. \\autoref{sec:reformulation} brings the variable change and the optimal control problem reformulation. \\autoref{sec:geometric-constraints} shows how the stack heights are now represented along the horizontal axis, leading to a form that is suitable for numerical optimization. \\autoref{sec:results} depicts an example to illustrate the proposed idea and subsequently discuss it. Lastly, \\autoref{sec:conclusion} brings the conclusion of the work.\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.17]{images/sts-draw.png}\n\t\\caption{A ship-to-shore crane schematic.}\n\t\\label{fig:sts}\n\\end{figure}\n\n\\section{MODELLING AND ORIGINAL PROBLEM FORMULATION}\\label{sec:modelling_and_original}\n\nIn order to control an STS-crane and make the process of loading and unloading the ship as fast as possible, a model description of its dynamics is obtained and a time-optimal control problem solved. In this section, we first present a lumped mass model, following the single-rope hoisting mechanism. Subsequently, the standard formulation for time-optimal control of this system is presented.\n\n\\subsection{Modelling}\\label{sec:modelling}\n\nA simplified representation of the dynamics of overhead cranes can be made by considering the problem as cart-pendulum with hoisting. To this end, consider the position of the trolley $x(t)$, the sway $\\theta(t)$, the length of the hoisting rope $l(t)$ and, the payload coordinates $x_p(t)$ and $y_p(t)$ as generalized coordinates. The forces $F_t(t)$ and $F_h(t)$ are the control inputs applied to the trolley and for hoisting the payload, respectively. A two-dimensional schematic of this representation showing the general coordinates and the forces involved in the problem is depicted in \\autoref{fig:schematic}.\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.6]{images/schematic.png}\n\t\\caption{Two-dimensional schematic representing the trolley and payload motions.}\n\t\\label{fig:schematic}\n\\end{figure}\n\nSimilarly to the model obtained in \\cite{kim2004anti} and \\cite{Hong2019}, and additionally including the position of the payload in general coordinates, the following equations of motion are obtained\n\\begin{multline}\n\t\\label{eq:dynamics-bridge}\n\t(m_1+m_2)\\ddot{x}(t) + m_2l(t)(\\ddot{\\theta}(t)\\cos(\\theta(t))-\\dot{\\theta}^2(t)\\sin(\\theta(t))) +\\\\\n\tm_2\\ddot{l}(t)\\sin(\\theta(t))+2m_2\\dot{l}(t)\\dot{\\theta}(t)\\cos(\\theta(t)) = F_t(t)\n\\end{multline}\n\\begin{multline}\n\t\\label{eq:dynamics-hoist}\n\tm_2\\ddot{l}(t) - m_2l(t)\\dot{\\theta}^2(t) - m_2g\\cos(\\theta(t)) +\\\\ m_2\\ddot{x}(t)\\sin(\\theta(t)) = -F_h(t)\n\\end{multline}\n\\begin{equation}\n\t\\label{eq:dynamics-sway}\n\tl(t)\\ddot{\\theta}(t) + 2\\dot{l}(t)\\dot{\\theta}(t)+g\\sin(\\theta(t)) + \\ddot{x}(t)\\cos(\\theta(t)) = 0\n\\end{equation}\n\\begin{equation}\n\t\\label{eq:dynamics-xp}\n\tx_p(t) = \\sin(\\theta(t))l(t) + x(t)\n\\end{equation}\n\\begin{equation}\n\t\\label{eq:dynamics-yp}\n\ty_p(t) = \\cos(\\theta(t))l(t).\n\\end{equation}\nHere $g$ is the acceleration of gravity, and $m_1$ and $m_2$  are the trolley and payload masses, respectively. The dynamics of the bridge along the $i\\text{-axis}$ is represented in (\\ref{eq:dynamics-bridge}), the hoist motion of the payload in (\\ref{eq:dynamics-hoist}), the sway motion of the payload in (\\ref{eq:dynamics-sway}), and finally the payload position along $i\\text{ and }j\\text{-axis}$ are represented in (\\ref{eq:dynamics-xp}) and (\\ref{eq:dynamics-yp}), respectively. Note that the dynamics of the actuators themselves are not considered, and this model is formulated considering the dynamics of an overhead crane with a single hoisting rope mechanism. Moreover, the payload is treated as a point mass. \n\nNow, choosing the state variables as $x_1 = x_p$, $x_2 = \\dot{x}_p$, $x_3 = y_p$, $x_4 = \\dot{y}_p$, $x_5 = l$, $x_6 = \\dot{l}$, $x_7 = \\theta$ and $x_8 = \\dot{\\theta}$, the nonlinear state-space representation is derived and can be written as:\n\\begin{equation}\n\\label{eq:original-state-eq}\n\\begin{aligned}\n\t\t\\dot{x}_{1} &= x_{2}\\\\\n\t\t\\dot{x}_{2} &= -(u_{2}\\sin(x_{7}))/m_{2} \\\\\n\t\t\\dot{x}_{3} &= x_{4}\\\\\n\t\t\\dot{x}_{4} &= -(u_{2}\\cos(x_{7}))/m_{2}+g\\\\\n\t\t\\dot{x}_{5} &= x_{6}\\\\\n\t\t\\dot{x}_{6} &= x_{5}x_{8}^{2}+g\\cos(x_{7})-u_{2}/m_{2}-\\sin(x_{7})(u_{1}+u_{2}\\sin(x_{7}))/m_{1}\\\\\n\t\t\\dot{x}_{7} &= x_{8}\\\\\n\t\t\\dot{x}_{8} &= -(2x_{6}x_{8}+g\\sin(x_{7})+\\cos(x_{7})(u_{1}+u_{2}\\sin(x_{7}))/m_{1})/x_{5}\\\\\n\\end{aligned}\n\\end{equation}\nwith the control inputs $u_{1} = F_{t}$ and $u_{2}=F_{h}$.\n\nWith the equations of the motion of the crane written in the state-space form, the time-optimal control problem can be formulated.\n\n\\subsection{Original problem formulation}\\label{sec:orignal_prob}\n\nThe problem of moving the payload from the initial to the final position in the minimum time can be expressed as a time-optimal control problem. This way, we need to minimize a cost function that accounts for the total time spent transporting the payload. This is done by obtaining a control law that satisfies the lower and upper bounds of the decision variables and control inputs.\n\\begin{equation}\n\\label{eq:original-formulation}\n\\begin{aligned}\n& \\underset{}{\\text{minimize}}\n& & T = \\int_{0}^{t_f} \\, dt \\\\\n& \\text{subject to}\n& & \\dot{x}(t) = f(t,x(t),u(t))\\\\\n&&& x(0) = x_{0}\\\\\n&&& x(t_f) = x_{t_f}\\\\\n&&& g(x(t)) \\le x(t) \\le f(x(t))\\\\\n&&& u_{min}(t) \\le u(t) \\le u_{max}(t).\n\\end{aligned}\n\\end{equation}\n\nHowever, as mentioned before, STS-crane operations are usually subject to geometric constraints, corresponding to the height of the container stacks. Such constraints are usually nonlinear and non-smooth functions of space. Because of this, issues arise in formulating and solving the numerical optimization problem starting from (\\ref{eq:original-formulation}).\n\nThe most immediate comes from our necessity of constructing the constraints on the container heights. This means that the geometric constraints in the numerical model need to be constructed from a function $g(x_p(t))$. Due to the discretization of the problem, the required height that the payload needs to be at a certain time instance $t^{k}$ is a function $g(x_p(t^{k}))$. Please note that in this paper we use superscript $k$ to indicate discretization. In this way, the function $g(x_p(t))$ needs to be defined from the current configuration using either continuous approximation or exact integer programming representation. Moreover, any nontrivial configuration of container heights will lead to non-convex constraints.\n\nFurthermore, though the objective function looks simple, it cannot be directly used in a numerical optimization scheme. Once again, the problem here comes due to the discretization. Since the free variable is also the one being optimized, the solution will be influenced by the fixed time sampling rate, or fixed number of control intervals. Thus, it requires a strategy to be used, for instance, time-elastic bands, presented in \\cite{Rosmann2015}.\n\nIn order to circumvent these problems, we reformulate the optimization problem through a change of variable.\n\n\\section{PROBLEM REFORMULATION}\\label{sec:reformulation}\n\nAs mentioned in the previous section, container avoidance constraints are problematic when the independent variable is time $t$. That is, the stack heights are difficult to be represented when using time as the variable to be discretized. However, they can be easily described in the position along the $i\\text{-axis}$.\n\nWith this in mind, the optimization problem is reformulated in an attempt to alleviate the above-mentioned issues. This is done by reparametrizing it in a spatial coordinate. Thus, we redefine the optimization problem with the position of the payload along the $i\\text{-axis}$ $x_{p}$ as the variable that we discretize the dynamics along with. In other words, this means that we perform a change of variable and now the system's dynamics are described using $dx/dx_{p}$.\n\n\\subsection{The variable change}\\label{sec:variable-change}\n\nA change of the integration variable is now performed and the time-optimal problem is reformulated in the same fashion as in \\cite{Verscheure2009} and \\cite{KangShin1985}. First, consider the total time $T$ that the payload takes to go from the initial to the final position i.e., the cost function in (\\ref{eq:original-formulation}) is\n\\begin{equation}\nT = \\int_{0}^{t_f} dt.\n\\end{equation}\nNow, note that the first dynamic equation, used to describe the payload position along the $i\\text{-axis}$, in (\\ref{eq:original-state-eq}) is \n\\begin{equation}\n\\label{eq:change_state}\n\\dot{x}_1=\\frac{dx_1}{dt} = x_2 \\implies \\frac{dt}{dx_1} = \\frac{1}{x_2}.\n\\end{equation}\nIn this way, the cost function to be minimized is rewritten as \n\\begin{equation}\n\t\\label{eq:obj-reformulation}\n\tT = \\int_{0}^{t_f} dt = \\int_{x_{1_0}}^{x_{1_f}} \\frac{dx_1}{x_2} = \\int_{x_{p_0}}^{x_{p_f}} \\frac{dx_p}{x_2}.\n\\end{equation}\n\nThe variable change implies that the optimization problem is now parametrized in the payload position along the $i\\text{-axis}$ ($x_{1}=x_{p}$) and consists of minimizing the integral on the right-hand side of (\\ref{eq:obj-reformulation}). The main benefit of this will be discussed in the next section. Nevertheless, an immediate consequence of (\\ref{eq:obj-reformulation}) is that a new state vector $x=[t,\\dot{x}_p,y_p,\\dot{y}_p,l,\\dot{l},\\theta, \\dot{\\theta}]^T$ is defined for the problem.\n\n\\textit{Remark:} With the variable change, time $t$ is no longer the free variable. Now, all the derivatives will be with respect to $x_p$ and thus, with different notation, $x^\\prime=dx/dx_p$.\n\nAdditionally, since time $t$ is now a state variable, we make the following identification\n\\begin{equation}\n\t\\label{eq:identifications}\n\t\\setlength{\\arraycolsep}{1pt}\n\tx_{1}\\leftarrow{t},~x^\\prime_{j}\\leftarrow{\\frac{dx_{j}}{dx_{p}}},~ j = 1, \\ldots, n,\n\\end{equation}\nand the state equations in (\\ref{eq:original-state-eq}) become a system of differential algebraic equations\n\\begin{equation}\n\\label{eq:reformulated-state-eq}\n\\begin{aligned}\nx_{2}x^\\prime_{1} &= 1\\\\\nx_{2}x^\\prime_{2} &= -(u_{2}\\sin(x_{7}))/m_{2}\\\\\nx_{2}x^\\prime_{3} &= x_{4}\\\\\nx_{2}x^\\prime_{4} &= -(u_{2}\\cos(x_{7}))/m_{2}+g\\\\\nx_{2}x^\\prime_{5} &= x_{6}\\\\\nx_{2}x^\\prime_{6} &= \\resizebox{0.85\\hsize}{!}{$x_{5}x_{8}^{2}+g\\cos(x_{7})-u_{2}/m_{2}-\\sin(x_{7})(u_{1}+u_{2}\\sin(x_{7}))/m_{1}$}\\\\ \nx_{2}x^\\prime_{7} &= x_{8}\\\\\nx_{2}x^\\prime_{8} &= \\resizebox{0.85\\hsize}{!}{$ -(2x_{6}x_{8}+g\\sin(x_{7})+\\cos(x_{7})(u_{1}+u_{2}\\sin(x_{7}))/m_{1})/x_{5},$} \n\\end{aligned}\n\\end{equation}\nwhere $j$ indices the state variables in the new state vector and $n$ is the system's dimension. It is worth noting that in performing the coordinate change, we implicitly make an assumption that the payload is moving monotonically, in one direction along $i\\text{-axis}$. This is a natural limitation that implicitly implies a no-sway condition in the solution.\n\nSubsequently to the change of variable made in (\\ref{eq:obj-reformulation}), a natural choice of the cost function $J$ would be\n\\begin{equation}\n\tJ = \\int_{x_{p_{0}}}^{x_{p_{f}}} \\frac{1}{x_2(x_p)} dx_p,\n\\end{equation}\nwhich leads to convergence issues since $x_{2}(x_{p})=0$ at $x_{2}(0)$ and $x_{2}(x_{2_f})$. However, an interesting property of the reformulation is that the variable $x_1$ in (\\ref{eq:reformulated-state-eq}) corresponds to time $x_{1}(x_{p})=t$. Thus, the optimization problem can now be written as \n\\begin{equation}\n\\label{eq:opt-reformulated}\n\\begin{aligned}\n& \\underset{}{\\text{minimize}}\n& & J = t(x_{p_{f}}) \\\\\n& \\text{subject to}\n& & x_{2}x^\\prime(x_p) = f(x_p,x(x_p),u(x_p))\\\\\n&&& x(0) = x_{0}\\\\\n&&& x(x_{p_{f}}) = x_{f}\\\\\n&&& 0 \\le t(x_p)\\\\\n&&& 0 \\le x_2(x_p)\\\\\n&&& g(x_p) \\le x(x_p) \\le f(x_p)\\\\\n&&& u_{min}(x_p) \\le u(x_p) \\le u_{max}(x_p).\n\\end{aligned}\n\\end{equation}\nAs mentioned before and more evident now, the positions, velocities and accelerations are related to one another through the parametrization of path. This is an interesting feature for representing the geometric constraints as we will see in the next section. Furthermore, though the general dynamics remain nonlinear and non-convex, the cost function is nevertheless still convex, which is an additional outcome of the reformulation.\n\n\\section{GEOMETRIC CONSTRAINTS}\\label{sec:geometric-constraints}\n\nWhen dealing with the constraints that the different heights of the stacks impose on the payload height $y_p(x_p)$, a function $s(x_p)$ that represents the stack profile along the loading site is implicitly required. This way, the constraints on $y_p(x_p)$ are\n\\begin{equation}\n0 \\le y_{p}(x_{p}) \\le h-s(x_{p}),\n\\label{eq:height_constraint_general}\n\\end{equation}\nwhere $h$ is the maximum height, e.g., the distance from the ground to the trolley.\n\nWith the optimization problem in the original formulation as in (\\ref{eq:original-formulation}), after the time discretization,  the constraints imposed on $y_p(x_p)$ in (\\ref{eq:height_constraint_general}) would be\n\\begin{equation}\n0 \\le y_{p}(x_{p}(t^k)) \\le h-s(x_{p}(t^k)).\n\\label{eq:height_constraint_time}\n\\end{equation}\nMoreover, the required explicit function representation $s(x_{p}(t^k))$ will generally be discontinuous, nonlinear and non-convex. \n\nHowever, with the optimization problem (\\ref{eq:opt-reformulated}) now parametrized in $x_{p}$, the geometric constraints can be easily represented. The height of the stacks and their positions along the trajectory are represented by the function $s(x_{p})$, which now addresses the stack height at each position $x_{p}$. This way, when discretizing $x_{p}$, the function $s(x_{p})$ determines the upper bound constraints for $y_{p}(x_{p})$ in (\\ref{eq:opt-reformulated}) as\n\\begin{equation}\n\t0 \\le y_{p}(x_{p}^{k}) \\le h-s(x_{p}^{k}).\n\t\\label{eq:height_constraint_space}\n\\end{equation}\nNote that we no longer need an explicit function $s(x_p)$, but simply function values that can be computed when setting up the numerical model. See \\autoref{fig:schematic-constraint}.\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.39]{images/schematic_constraint.png}\n\t\\caption{Two-dimensional schematic representing the trolley and payload motions and the geometric constraints (stacks).}\n\t\\label{fig:schematic-constraint}\n\\end{figure}\n\nThis way of representing the stack heights and their positions is attractive since it is easier and more intuitive to be constructed and manipulated, after the discretization. This is the main benefit of the variable change in (\\ref{eq:change_state})-(\\ref{eq:obj-reformulation}) and constitutes the main contribution of this paper.\n\n\\section{RESULTS AND DISCUSSION}\\label{sec:results}\n\nIn order to illustrate and validate the idea, a scenario of stack configuration was simulated. For simplicity, a small-scale example was used, where the integration goes from position $x_{p_{0}} = 0$ to $x_{p_{f}} = 1$. In this scenario, the container stacks are particularly high at the end of the loading site.\n\nThe software used to obtain the results was MATLAB. Additionally, CasADi software tool \\cite{Andersson2019} and Yop toolbox \\cite{leek2016optimal} were used for modeling the optimization problem, and IPOPT \\cite{Wchter2005} was used to solve it.\n\nNow, it is important to note that all the distances and lengths used in the results are in \\textit{meters} and the angles in \\textit{radian}. With (\\ref{eq:opt-reformulated}) in mind, the initial and final conditions were set to\n\\begin{equation}\n\\label{eq:initial-final}\n\\begin{aligned}\n&t(0) = 0,~ \t\t\t\t&&\\\\\n&x_{p}(0) = 0, \t    \t\t&&x_{p}(x_{p_{f}}) = 1,\\\\\n&\\dot{x}_p(0) = 0,~\t\t\t&&\\dot{x}_p(x_{p_f}) = 0,\\\\\n&y_p(0) = 3,~\t\t\t\t&&y_p(x_{p_f}) = 3,\\\\\n&\\dot{y}_p(0) = 0,~\t\t\t&&\\dot{y}_p(x_{p_f}) = 0,\\\\\n&l(0) = 3,~\t\t\t\t\t&&l(x_{p_f}) = 3,\\\\\n&\\dot{l}(0) = 0,~\t\t\t&&\\dot{l}(x_{p_f}) = 0,\\\\\n&\\theta(0) = 0,~\t\t\t&&\\theta(x_{p_f}) = 0,\\\\\n&\\dot{\\theta}(0) = 0,~\t\t&&\\dot{\\theta}(x_{p_f}) = 0,\n\\end{aligned}\n\\end{equation}\nthe constraints regarding the minimum and maximum height that the payload can be, given its position $x_p$ along the $i\\text{-axis}$, were defined as\n\\begin{equation}\n0.15 \\le y_p(x_p) \\le h-s(x_p),\n\\end{equation}\nand the other box constraints were\n\\begin{equation}\n\\begin{aligned}\n\t0 \\le &t(x_p) \\\\\n\t0 \\le &\\dot{x}_p(x_p)\\\\\n\t0 \\le &l(x_p) \\le 4.5\\\\\n\t-0.1 \\le &\\theta(x_p) \\le 0.1\\\\\n\t-1 \\le &F_t(x_p) \\le 1\\\\\n\t0 \\le &F_h(x_p) \\le 8.\n\\end{aligned}\n\\end{equation}\nAdditionally, a first-order polynomial degree was used for the collocation method in the solver, the problem was solved in $100$ control intervals, and the trolley and payload masses were respectively set to $m_1=1.2kg$ and $m_2=0.6kg$.\n\nFinally, the position of each stack was assigned as in (\\ref{eq:position}), where $c \\in \\mathbb{R}^{m}$ represents the positions where the stacks are along the $i\\text{-axis}$. In this example, $9$ stack positions were assigned, separated by $10cm$ from each other.\n\\begin{equation}\n\t\\label{eq:position}\n\t\tc = \n\t\\begin{bmatrix}\n\t0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9\n\t\\end{bmatrix}.\n\\end{equation} \nMoreover, the height of each stack was represented as in (\\ref{eq:height})\n\\begin{equation}\n\\label{eq:height}\n\t\\begin{aligned}\n\t\\rho = \n\t\\begin{bmatrix}\n\t0.5 & 1.0 & 1.0 & 1.0 & 2.0 & 2.0 & 2.4 & 2.5 & 1.0\n\t\\end{bmatrix},\n\t\\end{aligned}\n\\end{equation}\nwhere $\\rho \\in \\mathbb{R}^{m}$ represents the height of each of the stacks, corresponding to the assigned position in (\\ref{eq:position}). Also, the width of the stacks was considered, here $0.08m$. From these, the function values in the bound constraints (\\ref{eq:height_constraint_space}) can be constructed.\n\n\\subsection{Results}\n\n\\autoref{fig:ex1-trajectory} shows the path followed by the payload, given the stack heights (\\ref{eq:height}) at the positions (\\ref{eq:position}).\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.18]{images/plots/ex1-trajectory-eps-converted-to.pdf}\n\t\\caption{The path followed by the payload subject to stack constraints.}\n\t\\label{fig:ex1-trajectory}\n\\end{figure}\nFigures \\ref{fig:ex1-states1} and \\ref{fig:ex1-states2} show how the payload positions, hoisting and sway evolve with time. Note that though they correspond to the decision variables in (\\ref{eq:original-state-eq}), they are the result of problem solved as in (\\ref{eq:opt-reformulated}).\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.24]{images/plots/ex1-states1-eps-converted-to.pdf}\n\t\\caption{The payload spatial-coordinates trajectories.}\n\t\\label{fig:ex1-states1}\n\\end{figure}\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.24]{images/plots/ex1-states2-eps-converted-to.pdf}\n\t\\caption{The payload hoisting and sway trajectories.}\n\t\\label{fig:ex1-states2}\n\\end{figure}\nLastly, \\autoref{fig:ex1-extra} shows the velocity $\\dot{x}_p$, time (which now is a decision variable), and control inputs $F_{t}$ and $F_{h}$ evolving along $x_{p}$.\n\\begin{figure}[h]\n\t\\centering\n\t\\includegraphics[scale=0.24]{images/plots/ex1-extra-eps-converted-to.pdf}\n\t\\caption{How $\\dot{x}_{p}$, time and control inputs evolve with the spatial coordinate.}\n\t\\label{fig:ex1-extra}\n\\end{figure}\n\n\\subsection{Discussion} \\label{sec:discussion}\n\nThe presented numerical example shows the simulation results for the optimization control problem, solved off-line after a change of variable. The aim was to illustrate that with the variable change, the description of the stack heights becomes trivial and easier to represent numerically. This is done by defining their positions and corresponding heights in $c$ and $\\rho$, respectively. These are then implicitly defining a function $s(x_p)$, which addresses the stacks' height in each discretization interval. Moreover, Figs. \\ref{fig:ex1-trajectory} to \\ref{fig:ex1-extra} show that the payload was transferred from the initial to the final position avoiding the stacks and not violating the predefined constraints. Hence, we believe that the results have further strengthened our idea that this problem becomes trivial with the discretization of $x_p$.\n\nWe are aware that our work still has some limitations. As mentioned before, a natural one is that the solution enforces no sway condition since the payload is moving monotonically in one direction. Furthermore, the distance traveled by the payload in each discretization interval varies along the loading and unloading process. For instance, as seen in \\autoref{fig:ex1-trajectory}, they were particularly long in the end, after a tall stack. Thus, since the problem is uniformly discretized, most of the dynamics that are not captured might be in those parts. This can be easily addressed by non-uniform discretization. Notice that these regions, where more time is spent, are strongly influenced by the stacks' disposition. \n\nPerhaps the biggest limitation of this work is that, though the objective function and the constraints on the container heights are now convex, the dynamics of the system remain non-convex. This might lead to a solution at a local minimum. However, it is important to note that circumventing these limitations is not in the scope of this work. Moreover, non-convexity is simply a consequence of the high-fidelity model of the system. Additionally, the problem may be extended to 3D, however, we believe that this is not necessary, given the nature of ship-to-shore crane operations.\n\n\\section{CONCLUSIONS AND FUTURE WORK} \\label{sec:conclusion}\n\nWe have proposed a new approach to deal with the different heights that the container stacks can assume during the process of loading and unloading container ships. With this in mind, a variable change has been performed and the representation of the stack heights became trivial in the optimization problem. Subsequently, a small-scale example has led us to confirm it. Thus, this approach has the potential to be successfully applied to real crane operations and enhance their automation. \n\nFuture work will focus on going beyond the point-mass assumption and incorporate more physical and geometric constraints to the setup. Moreover, we will investigate how to deal with it when the initial or final position is in between stacks. Additionally, different densities of discretization will be addressed.\n\n\\addtolength{\\textheight}{-3cm}  \n\n\n\n\\bibliographystyle{IEEEtran}\n", "meta": {"timestamp": "2022-11-21T02:14:24", "yymm": "2203", "arxiv_id": "2203.15360", "language": "en", "url": "https://arxiv.org/abs/2203.15360"}}
{"text": "\\section{Introduction}\n\nThe Potts model is a statistical model, originally invented to study ferromagnetism~\\cite{potts}; it also plays a central role in probability theory, combinatorics and computer science, see e.g.~\\cite{Sokaltutte} for background. \n\nLet $G = (V,E)$ be a finite graph. The anti-ferromagnetic Potts model on the graph $G$ has two parameters, a number of \\emph{states, or colors,} $q\\in \\mathbb{Z}_{\\geq 2}$ and an edge interaction parameter $w=e^{kJ/T}$, with $J<0$ being a coupling constant, $k$ the Boltzmann constant and $T$ the temperature.\nThe case $q=2$ is also known as the zero-field Ising model.\nA \\emph{configuration} is a map $\\sigma:V \\to [q]:=\\{1,\\dots,q\\}$. \nAssociated with such a configuration is the \\emph{weight}\n$w^{m(\\sigma)}$, where $m(\\sigma)$ is the number of edges  $e=\\{u,v\\}\\in E$ for which $\\sigma(u)=\\sigma(v)$.\nThere is a natural probability measure, the \\emph{Gibbs measure} $\\text{Pr}_{G;q,w}[\\cdot]$, on the collection of configurations $\\Omega=\\{\\sigma:V \\to [q]\\}$ in which a configuration is sampled proportionally to its weight.\nFormally, for a given configuration $\\phi:V\\to [q]$ the probability that a random configuration ${\\bf \\Phi}$\\footnote{ We use the convention to denote random variables with capitals in boldface.} is equal to $\\phi$, is given by\n\\begin{equation}\\label{eq:gibbs finite}\n\\text{Pr}_{G;q,w}[{\\mathbf \\Phi}=\\phi] = \\frac{w^{m(\\phi)}}{ \\sum_{ \\sigma:V \\rightarrow [q]} w^{m(\\sigma)}},\n\\end{equation}\nhere the denominator is called \\emph{partition function} of the model and we denote it by $Z(G;q,w)$ (or just $Z(G)$ if $q$ and $w$ are clear form the context).\n\nIn statistical physics the Potts model is most frequently studied on infinite lattices, such as $\\mathbb{Z}^2$. \nAt the cost of introducing some measure theory, the notion of a Gibbs measure can be extended to such infinite graphs, see e.g. \\cite{brigt99, bright02,friedli2017}.\nWhile at any temperature the Gibbs measure on a finite graph is unique, this is no longer the case for all infinite lattices. \nThe transition from having a unique Gibbs measure to multiple Gibbs measures in terms of the temperature is referred to as a \\emph{phase transition} in statistical physics~\\cite{Georgii,friedli2017} and it is an important problem to determine the exact temperature, the \\emph{critical temperature, $P_c$}, at which this happens. \nThere exist predictions for the critical temperature on several lattices in the physics literature by Baxter~\\cite{BaxterZd,Baxterq} (see also~\\cite{SalasSokal} for more details and further references), but it turns out to be hard to prove these rigorously cf.~\\cite{SalasSokal}. \n\nIn the present paper we consider the anti-ferromagnetic Potts model on the infinite $\\Delta$-regular tree, $\\mathbb{T}_\\Delta=(V,E)$, also known as the \\emph{Bethe lattice}, or \\emph{Cayley tree}. \nWe briefly recall the formal definition of a Gibbs measure in this situation. See~\\cite{Roz21} for a survey on this topic in general. \n\nThe sigma algebra is generated by sets of the form $U_{\\sigma}:=\\{\\phi:V\\to [q]\\mid \\phi\\!\\restriction_U=\\sigma\\}$, where $U\\subset V$ is a finite set and $\\sigma:U\\to [q]$.\nLet $w\\in (0,1)$.\nA probability measure $\\mu$ on this sigma algebra is then called a Gibbs measure for the $q$-state anti-ferromagnetic Potts model on $\\mathbb{T}_\\Delta$ at $w$, if for all finite $U\\subset V$ and all $\\phi:V\\to [q]$ the following holds\n\\[\n\\text{Pr}_\\mu[\\mathbf{\\Phi}\\!\\restriction_U=\\phi\\!\\restriction_U\\mid \\mathbf{\\Phi}\\!\\restriction_{V\\setminus U^\\circ}=\\phi\\!\\restriction_{V\\setminus U^\\circ}]\n=\\text{Pr}_{G[U];q,w}[\\mathbf{\\Phi}\\!\\restriction_U=\\phi|_U\\mid \\mathbf{\\Phi}\\!\\restriction_{\\partial U}=\\phi|_{\\partial U}],\n\\]\nwhere $\\partial U$ denotes the collection of vertices in $U$ that have a neighbor in $V\\setminus U$ and $U^\\circ:=U\\setminus \\partial U$.\nNote that for any $w\\in (0,1)$ there is at least one such Gibbs measure.\n\nFor a number of states $q\\geq 3$ define $w_c:=\\max\\{0,1-\\frac{q}{\\Delta}\\}$.\nIt is a longstanding folklore conjecture (cf.\\cite[page 746]{blanca2020}) that the Gibbs measure is unique if and only if $w\\geq w_c$ (where the inequality should be read as strict if $q=\\Delta$.)\nThis was confirmed by Jonasson for the case $w=0$~\\cite{jon02}, by Srivastava, Sinclair and Thurley ~\\cite{sst14} for $q=2$ (see also~\\cite{Georgii}), \nby Galanis, Goldberg and Yang for $q=3$~\\cite{PottsLeslie3} and by three of the authors of the present paper for $q=4$ and $\\Delta\\geq 5$~\\cite{deboer2020uniqueness}. \nOur main result is a confirmation of this conjecture for all $q\\geq 5$ provided the degree of the tree is large enough.\n\n\n\\begin{main}\nFor each integer $q\\geq 5$ there exists $\\Delta_0 \\in \\mathbb{N}$ such that for each $\\Delta \\geq \n\\Delta_0$ and each $w\\in [w_c,1)$ the $q$-state anti-ferromagnetic Potts model with edge interaction parameter $w$ has a unique Gibbs measure on the infinite $\\Delta$-regular tree $\\mathbb{T}_{\\Delta}$.\n\\end{main}\n\nIt has long been known that there are multiple Gibbs measures when $w<w_c$~\\cite{Peruggietalgeneral,Peruggietaldiagrams}, see also~\\cite{gal15}) and~\\cite{Roz1,Roz2,Roz3,Roz4}.\nSo our main results pinpoints the critical temperature for the anti-ferromagnetic Potts model on the infinite regular tree for large enough degree.\nFor later reference we will refer to $w_c$ as the \\emph{uniqueness threshold}.\n\nIn Theorem~\\ref{thm:main} below, we will reformulate our main theorem in terms of the conditional distribution of the color of the root vertex of $\\mathbb{T}_{\\Delta}$ conditioned on a fixed coloring of the vertices at a certain distance from the root, showing that this distribution converges to the uniform distribution as the distance tends to infinity.\nWe in fact show that this convergence is exponentially fast for subcritical $w$ (i.e. $w>w_c$).\n\n\n\n\\subsection{Motivation from computer science}\nThere is a surprising connection between phase transitions on the infinite regular tree and transitions in the computational complexity of approximately computing partition function of $2$-state models (not necessarily the Potts model) on bounded degree graphs. \nFor parameters inside the uniqueness region there is an efficient algorithm for this task~\\cite{weitz06,licor,sst14}, while for parameters for which there are multiple Gibbs measures on the infinite regular tree, the problem is NP-hard~\\cite{Slysun,gal16}. \nIt is conjectured that a similar phenomenon holds for a larger number of states.\n\nWhile the picture for $q$-state models for $q\\geq 3$ is far from clear, some progress has been made on this problem for the anti-ferromagnetic Potts model.\nOn the hardness side, Galanis, \\v{S}tefankvovi\\v{c} and Vigoda~\\cite{gal15} showed that for even numbers $\\Delta\\geq 4$ and any integer $q\\geq 3$, approximating the partition function of the Potts model $Z(G;q,w)$ is NP-hard on the family of graphs of maximum degree $\\Delta$ for any $0\\leq w< q/\\Delta=w_c$, which we now know to be the uniqueness threshold (for $\\Delta$ large enough).\nOn the other side, much less is known about the existence of efficient algorithms for approximating $Z(G;q,w)$ or sampling from the measure $\\text{Pr}_{G;q,w}$ for the class of bounded degree graphs when $w>w_c$. Implicit in~\\cite{bencs} there is an efficient algorithm for this problem whenever $1\\geq w>1- \\alpha q/\\Delta$, with $\\alpha=1/e$, which has been improved to $\\alpha=1/2$ in~\\cite{liu2019correlation}.\n\nFor random regular graphs of large enough degree, our main result implies an efficient randomized algorithm to approximately sample from the Gibbs measure $\\text{Pr}_{G;q,w}$ for any $w_c<w\\leq 1$ by a result of Blanca, Galanis, Goldberg, \\v{S}tefankovi\\v{c}, Vigoda and Yang~\\cite[Theorem 2.7]{blanca2020}. \nIn~\\cite{eft2020}, Efthymiou proved a similar result for Erd\\H{o}s-R\\'enyi random graphs without the assumption that $w_c$ is equal to the uniqueness threshold on the tree.\nAt the very least this indicates that the uniqueness threshold on the infinite regular tree plays an important role in the study of the complexity of approximating the partition function of and sampling from the Potts model on bounded degree graphs.\n\n\n\\subsection{Approach}\nOur approach to prove the main theorem is based on the approach from~\\cite{deboer2020uniqueness} for the  cases $q=3,4$.\nAs is well known, to prove uniqueness it suffices to show that for a given root vertex, say $v$, the probability that $v$ receives a color $i\\in [q]$, conditioned on the event that the vertices at distance $n$ from $v$ receive a fixed coloring, converges to $1/q$ as $n\\to \\infty$ regardless of the fixed coloring of the vertices at distance $n$.\nInstead of looking at these probabilities, we look at ratios of these probabilities.\nIt then suffices to show that these converge to $1$.\nThe ratios at the root vertex $v$ can be expressed as a rational function of the ratios at the neighbors of $v$. \nSee Lemma~\\ref{lem:treerecursion} below.\nThis function is rather difficult to analyze directly and as in~\\cite{deboer2020uniqueness} we analyze a simpler function coupled with a geometric approach.\nA key new ingredient of our approach is to take the limit of $\\Delta$, the degree of the tree, to infinity and analyze the resulting function. \nThis function turns out be even simpler and behaves much better in a geometric sense.\nWith some work we translate the results for the limit case back to the finite case and therefore obtain results for $\\Delta$ large enough.\nThis is inspired by a recent paper~\\cite{bencs2021limit} in which this idea was used to give a precise description of the location of the zeros of the independence polynomial for bounded degree graphs of large degree.\n\n\\subsection*{Organization}\nIn the next section we give a more technical overview of our approach.\nIn particular we recall some results from~\\cite{deboer2020uniqueness} that we will use and set up some terminology.\nWe also gather two results that will be used to prove our main theorem, leaving the proofs of these results to Section~\\ref{sec:convex} and Section~\\ref{sec:forward} respectively. Assuming these results, the main theorem will be proved in Subsection~\\ref{ssec:proof}.\n\n\\section{Preliminaries, setup and proof outline}\\label{sec:technial outline}\n\\subsection{Reformulation of the main result}\nWe will reformulate our main theorem here in terms of the conditional distribution of the color of the root vertex of $\\mathbb{T}_{\\Delta}$ conditioned on a fixed coloring of the vertices at a certain distance from the root.\n\nLet $\\Delta\\geq 2$ be an integer.\nIn what follows it will be convenient to write $d=\\Delta-1$. \nFor a positive integer $n$ we denote by $\\mathbb{T}^n_{d+1}$ the finite tree obtained from $\\mathbb{T}_{d+1}$ by fixing a root vertex $r$, deleting all vertices at distance more than $n$ from the root, deleting one of the neighbors of $r$ and keeping the connected component containing $r$.\nWe denote the set of leaves of $\\mathbb{T}^n_{d+1}$ by $\\Lambda_{n}$, except when $n=0$, in which case we let $\\Lambda_{0}=\\{r\\}$.\nFor a positive integer $q$ we call a map $\\tau:\\Lambda_{n}\\to [q]$ a \\emph{boundary condition at level $n$}.\n\nThe following theorem may be seen as a more precise form of our main result.\n\\begin{theorem}\\label{thm:main}\nLet $q\\geq 3$ be a positive integer. There exist constants $C>0$ and $d_0>0$ such that for all integers $d\\geq d_0$ and all $\\alpha\\in (0,1)$ the following holds for any $i\\in \\{1,\\ldots,q\\}$:\n \\begin{equation}\\label{equation:uniqueness critical}\n        \\lim_{n \\to \\infty} \\max_{\\tau: \\Lambda_{n} \\to [q]} \\bigg\\vert \\text{Pr}_{\\mathbb{T}^n_{d+1},q,w_c} [ {\\bf \\Phi}(r) = i \\ \\vert\\ {\\bf\\Phi}\\!\\restriction_{ \\Lambda_{n}} = \\tau] - \\frac{1}{q}\\bigg\\vert = 0,\n    \\end{equation}\n    for any boundary condition at level $n$, $\\tau$, and edge interaction $w(\\alpha)=1-\\tfrac{\\alpha q}{d+1}$,\n    \\begin{equation}\\label{equation:uniqueness subcritical}\n\\bigg\\vert \\text{Pr}_{\\mathbb{T}^n_{d+1},q,w(\\alpha)} [ {\\bf \\Phi}(r) = i \\ \\vert\\ {\\bf\\Phi}\\!\\restriction_{ \\Lambda_{n}} = \\tau]-\\frac{1}{q}\\bigg\\vert  \\leq C\\alpha^{n/2}.     \n    \\end{equation}\n\\end{theorem}\n\\begin{remark}\\label{rem:strengthen}\nWe can in fact strengthen \\eqref{equation:uniqueness subcritical} in two ways. \nFirst of all, for any $\\alpha<\\hat{\\alpha}<1$ there exists a constant $C_{\\hat{\\alpha}}>0$ such that the right-hand side of~\\eqref{equation:uniqueness subcritical} can be replaced by $C_{\\hat{\\alpha}}\\hat{\\alpha}^n$.\nSecondly, for any fixed $d\\geq d_0$ there exist a constant $C_d>0$ such that the right-hand side of~\\eqref{equation:uniqueness subcritical} can be replaced by $C_{d}\\alpha^n$.\n\\end{remark}\n\nAs is well known (see e.g.~\\cite[Lemma 1.3]{deboer2020uniqueness}\\footnote{The proof of that lemma in the published version of that paper contains an error; this is corrected in a more recent arXiv version: \\texttt{arXiv:2011.05638 v3}.}) Theorem~\\ref{thm:main} directly implies our main theorem. \nTherefore the remainder of the paper is devoted to proving Theorem~\\ref{thm:main}.\n\nIn the next subsections we outline how we do this.\n\n\n\\subsection{Log-ratios of probabilities}\nTheorem~\\ref{thm:main} is formulated in terms of certain conditional probabilities. \nFor our purposes it turns out to be convenient to reformulate this into \\emph{log-ratios} of these probabilities. \nTo introduce these, we recall some relevant definitions from~\\cite{deboer2020uniqueness}.\nThroughout we fix an integer $q\\geq 3$.\n\nGiven a (finite) graph $G=(V,E)$ and a subset $U \\subseteq V$ of vertices, we call $\\tau:U \\to [q]$ a \\emph{boundary condition} on $G$. \nWe say vertices in $U$ are \\emph{fixed} and vertices in $V\\setminus U$ are \\emph{free}.\nThe \\emph{partition function restricted to} $\\tau$ is defined as\n\\[\nZ_{U,\\tau}(G;q,w) = \\sum_{\\substack{ \\sigma:V \\rightarrow [q]\\\\ \\sigma\\restriction_{\\!U} = \\tau}} w^{m(\\sigma)}.\n\\]\nWe just write $Z(G)$ if $U,\\tau$ and $q,w$ are clear from the context. \nGiven a boundary condition $\\tau:U \\to [q]$, a free vertex $v \\in V \\setminus U$ and a state $i \\in [q]$ we define $\\tau_{v,i}$ as the unique boundary condition on $U\\cup\\{v\\}$ that extends $\\tau$ and associates $i$ to $v$.\nWhen $U$ and $\\tau$ are clear from the context, we will denote $Z_{U \\cup \\{v\\},\\tau_{v,i}}(G)$ as $Z^{v}_{i}(G)$. Let $\\tau:U\\to [q]$ be a boundary condition and $v \\in V$ be a free vertex. \nFor any $i \\in [q]$ we define the \\emph{log-ratio} $\\tilde{R}_{G,v,i}$ as\n\\[\n\\tilde{R}_{G,v,i}: =  \\log(Z^{v}_{i}(G))-\\log(Z^{v}_{q}(G)),\n\\]\nwhere $\\log$ denotes the natural logarithm.\nNote that $\\tilde{R}_{G,v,q}=0$.\nWe moreover remark that $\\tilde{R}_{G,v,i}$ can be interpreted as the logarithm of the ratio of the probabilities that the root gets color $i$ (resp. $q$) conditioned on the event that $U$ is colored according to $\\tau$.\n\nFor trees the log-ratios at the root vertex can be recursively computed from the log-ratios of its neighbors.\nTo describe this compactly we introduce some notation that will be used extensively throughout the paper. \nFix $d \\in \\mathbb{R}_{>1}$ and let $\\avar\\in (0,1]$. Define the maps $G_{d,\\avar;i},F_{d,\\avar;i}:\\mathbb{R}^{q-1} \\rightarrow \\mathbb{R}$ for $i \\in \\{1,\\ldots,q-1\\}$ as \n\\begin{equation}\\label{eq:define Gd coordinate}\nG_{d,\\avar;i}(x_1,\\ldots,x_{q-1}) =   \\frac{1-x_i}{\\sum_{j=1}^{q-1}x_j + 1-  \\frac{\\avar \\cdot q}{d+1}}\n\\end{equation}\nand\n\\begin{equation}\\label{eq:define Fd coordinate}\nF_{d,\\avar;i}(x_1,\\ldots,x_q) = d \\log\\left(1+\\frac{\\avar \\cdot q}{d+1} \\cdot G_{d,\\avar;i}(\\exp(x_1),\\ldots,\\exp(x_{q-1}))\\right).\n\\end{equation}\nDefine the map $F_{d,\\avar}: \\mathbb{R}^{q-1} \\rightarrow \\mathbb{R}^{q-1}$ whose $i$th coordinate function is given by $F_{d,\\avar;i}(x_1,\\ldots,x_{q-1})$ and define $G_{d,\\avar}$ similarly.\nTo suppress notation we write $F_d=F_{d,1}$ and $G_d = G_{d,1}$. We also define $\\exp(x_1, \\ldots, x_{q-1}) = (\\exp(x_1), \\ldots, \\exp(x_{q-1}))$ and $\\log(x_1, \\ldots, x_{q-1}) = (\\log(x_1), \\ldots, \\log(x_{q-1}))$.\nWe note that $G_{d,\\alpha}$ and $F_{d,\\alpha}$ are analytic in $1/d$ when viewing $d$ as a variable.\nWe will now use the map $F_{d,\\alpha}$ to give a compact description of the tree recurrence for log-ratios.\n\n\\begin{lemma}\\label{lem:treerecursion}\n    Let $T = (V,E)$ be a tree, $\\tau:U \\to [q]$ a boundary condition on $U \\subsetneq V$. Let $v$ be a free vertex of degree $d\\geq 1$ with neighbors $v_1, \\dots, v_d$.\n   Denote $T_i$ for the tree that is the connected component of $T - v$ containing $v_i$. \n   Restrict $\\tau$ to each $T_i$ in the natural way.  \n   Write $x_{i,j}$ for the log-ratio $\\tilde{R}_{T_i,v_i,j}$.\n    Then for $\\avar$ such that $w=1-\\tfrac{\\avar\\cdot q}{d+1}$,\n    \\begin{equation}\\label{eq:logtreeformula}\n    (\\tilde{R}_{T,v,1},\\ldots, \\tilde{R}_{T,v,q-1}) = \\sum_{i=1}^{d} \\frac{1}{d} F_{d,\\avar}(x_{i,1},\\ldots,x_{i,q-1}),\n\\end{equation}\na convex combination of the images of the map $F_{d,\\alpha}$.\n\\end{lemma}\n\\begin{proof}\nBy focusing on the $j$th entry of the left-hand side and substituting $ R_{T,v,j}:=\\exp(\\tilde{R}_{T,v,j}) $, we see that \\eqref{eq:logtreeformula} follows from the well known recursion for ratios \n\\begin{equation}\\label{eq:treeformula}\n    R_{T,v,i} = \\prod_{s=1}^{d} \\frac{\\sum_{l \\in [q-1]  \\setminus \\{i\\}}  R_{T_s,u_s,l} + w  R_{T_s,u_s,i} + 1}{\\sum_{l \\in [q-1] }  R_{T_s,u_s,l} + w}.\n\\end{equation}\nSee e.g.~\\cite{deboer2020uniqueness} for a proof of this.\n\\end{proof}\n\n\n\nDenote $\\mathbf{0}$ for the zero vector in $\\mathbb{R}^{q-1}$. (Throughout we will denote vectors in boldface.)\nWe define for any $n \\geq 1$ the set of possible log-ratio vectors \n\\[\n\\mathcal{R}_n := \\{(\\tilde{R}_{\\mathbb{T}_{d+1}^n,r_n,1}, \\ldots, \\tilde{R}_{\\mathbb{T}_{d+1}^n,r_n,q-1}) \\in \\mathbb{R}^{q-1}| \\tau:\\Lambda_n \\rightarrow [q] \\}.\n\\]\nThe following lemma shows how the recursion from Lemma \\ref{lem:treerecursion} will be used.\n\n\\begin{lemma}\n    \\label{lem: Tab sequence}\n    Let $q\\geq 3$ and $d\\geq 2$ be integers. If there exists a sequence $\\{\\mathcal{T}_n\\}_{n\\geq 1}$ of convex subsets of $\\mathbb{R}^{q-1}$ with the following properties:\n    \\begin{enumerate}\n        \\item \\label{it:basecase}\n        $\\mathcal{R}_1 \\subseteq \\mathcal{T}_1$,\n        \\item \\label{it:inductionstep}\n        for every $m \\geq 1$, $F_d(\\mathcal{T}_m) \\subseteq \\mathcal{T}_{m+1}$,\n        \\item \\label{it:laststep}\n        for every $\\epsilon > 0$ there is an $M \\geq 1$ such that for all $m\\geq M$ every element of $\\mathcal{T}_m$ has at most distance $\\epsilon$ to the zero vector $\\mathbf{0}$,\n    \\end{enumerate}\nthen     \n\\begin{equation}\n        \\lim_{n \\to \\infty} \\max_{\\tau: \\Lambda_{n,d+1} \\to [q]} \\bigg\\vert \\text{Pr}_{\\mathbb{T}^n_{d},q,w_c} [ {\\bf \\Phi}(r) = i \\ \\vert\\ {\\bf\\Phi}\\!\\restriction_{ \\Lambda_{n,d}} = \\tau] - \\frac{1}{q}\\bigg\\vert = 0.\n    \\end{equation}\n\\end{lemma}\n\\begin{proof}\nThe proof is analogous to the proof of Lemma 2.3 in \\cite{deboer2020uniqueness} and we therefore omit it.\n\\end{proof}\n\nIn the next section we construct a family of convex sets that allows us to form a sequence $\\{\\mathcal{T}_n\\}_{n\\geq 1}$ with the properties required by the lemma. \n\n\\subsection{Construction of suitable convex sets}\nWe need the standard $q-2$-simplex, which we denote as\n\\[\n\\Delta=\\left\\{(t_1,\\ldots,t_{q-2},1-\\sum_{i=1}^{q-2}t_i)\\mid t_i\\geq 0 \\text{ for all $i$, }\\sum_{i=1}^{q-2}t_i\\leq 1\\right\\}.\n\\]\n\nThe symmetric group $S_q$ acts on $\\mathbb{R}^q$ by permuting entries of vectors. \nConsider $\\mathbb{R}^{q-1}\\subset \\mathbb{R}^q$ as the subspace spanned by $\\{\\mathbf{e}_1 - \\mathbf{e}_q, \\ldots, \\mathbf{e}_{q-1} - \\mathbf{e}_q$\\}, where $\\mathbf{e}_i$ denotes the $i$th standard base vector in $\\mathbb{R}^q$. \nThis induces a linear action of $S_q$ on $\\mathbb{R}^{q-1}$, also known as the the standard representation of $S_q$ and denoted by $\\mathbf{x}\\mapsto \\pi\\cdot \\mathbf{x}$ for $\\mathbf{x}\\in \\mathbb{R}^{q-1}$ and $\\pi\\in S_q$.\nThe following lemma shows that the map $F_{d,\\avar}$ is $S_q$-equivariant for any $\\avar \\in (0,1]$, essentially because the action permutes the $q$ colors of the Potts model and no color plays a special role.\n\n\\begin{lemma}\\label{lem:symmetry}\nFor any $\\pi \\in S_q$, any $\\avar \\in (0,1]$, any $\\mathbf{x} \\in \\mathbb{R}^{q-1}$ and any $d$ we have \\[\\pi \\cdot F_{d,\\avar}(\\mathbf{x}) =  F_{d,\\avar}(\\pi \\cdot \\mathbf{x}).\\] \n\\end{lemma}\n\\begin{proof}\nThis follows as in Section 3.1 in \\cite{deboer2020uniqueness}. \n\\end{proof}\n\nDefine for $\\cvar\\geq 0$ the half space \n\\[\nH_{\\geq -\\cvar}:=\\left\\{\\mathbf{x}\\in \\mathbb{R}^{q-1}\\mid \\sum_{i=1}^{q-1} x_i\\geq-\\cvar\\right\\}.\n\\]\nDefine the set\n\\begin{equation}\\label{eq:def P_c}\nP_\\cvar = \\bigcap_{\\pi \\in S_q} \\pi \\cdot H_{\\geq -\\cvar}.\n\\end{equation}\nNote that for each $\\cvar \\geq 0$ the set $P_\\cvar$ equals the convex polytope \n\\[\n\\text{conv} \\big(\\{(-\\cvar,0,\\ldots, 0), \\ldots (0,\\ldots,0,-\\cvar),(\\cvar,\\ldots,\\cvar)\\} \\big) .\n\\]\nDenote $D_\\cvar:= \\text{conv} \\big(\\{(-\\cvar,0,\\ldots, 0), \\ldots (0,\\ldots,0,-\\cvar),(0,\\ldots,0)\\} \\big)$. \nThen we have\n\\begin{equation}\\label{eq:fund domain}\nP_\\cvar = \\bigcup_{\\pi \\in S_q}\\pi \\cdot D_\\cvar.\n\\end{equation}\nWe refer to $D_\\cvar$ as the fundamental domain of the action of $S_q$ on $\\mathbb{R}^{q-1}$.\n\nThe following two propositions capture the image of $P_c$ under applications of the map $F_d$.\n\n\\begin{prop}\\label{prop:convex}\nLet $q\\geq 3$ be an integer. Then there exists $d_1>0$ such that for all $d\\geq d_1$ and $c\\in [0,q+1]$, $F_d(P_c)$ is convex.\n\\end{prop}\n\n\\begin{prop}\\label{prop:2stepforward}\nLet $q\\geq 3$ be an integer. There exists $d_2>0$ such that for all $d\\geq d_2$ the following holds:\nfor any $c\\in(0,q+1]$ there exists $0<c'<c$ such that\n\\[\nF_d^{\\circ 2}(P_c)\\subseteq P_{c'}.\n\\]\n\\end{prop}\n\nWe postpone the proofs of the two results above to the subsequent sections. A crucial ingredient in both proofs will be to analyze the limit $\\lim_{d\\to \\infty} F_d$.\nWe first utilize the two propositions to give a proof of Theorem~\\ref{thm:main}.\n\n\\subsection{A proof of Theorem~\\ref{thm:main}}\\label{ssec:proof}\nFix an integer $q\\geq 3$. Let $d_1,d_2$ be the constants from Proposition~\\ref{prop:convex} and~\\ref{prop:2stepforward} respectively. \nLet $d_0\\geq \\max\\{d_1,d_2\\}$ large enough to be determined below. \nNote that the log-ratios at depth $0$ are of the form $\\infty \\cdot \\mathbf{e}_i$ and $-\\infty\\cdot\\mathbf{1}$, where $\\mathbf{1}$ denotes the all ones vector. \nThis comes from the fact that the probabilities at level $0$ are either $1$ or $0$ and so the ratios are of the form $\\mathbf{1}+\\infty\\mathbf{e}_i$ or $\\mathbf{0}$. \nThis implies that the log-ratios at depth $1$ are convex combinations of $F_d(\\infty\\cdot  \\mathbf{e}_i)=d\\log(1+\\tfrac{-q}{d+1})\\mathbf{e}_i$ and $F_d(-\\infty\\cdot  \\mathbf{1})=d\\log(1+\\tfrac{q}{d+1-q})\\mathbf{1}.$\nSo for $d\\geq d_0$ and $d_0$ large enough they are certainly contained in $P_{q+1}$.\n\n\nWe start with the proof of~\\eqref{equation:uniqueness critical}. We construct a decreasing sequence $\\{c_n\\}_{n\\in \\mathbb N}$ and let $\\mathcal{T}_{2n-1}=P_{c_n}$.\nFor even $n>0$  we set $\\mathcal{T}_n=F_d(P_{c_{n-1}})$, which is convex by Proposition~\\ref{prop:convex}.\nWe set $c_1=q+1$ and for $n\\geq 1$, given $c_n$, we can choose, by Proposition~\\ref{prop:2stepforward}, $c_{n+1}<c_n$ so that $F_d^{\\circ 2}(P_{c_n})\\subseteq P_{c_{n+1}}$. Choose such a $c_{n+1}$ as small as possible.\nWe claim that the sequence $\\{c_n\\}_{n\\in \\mathbb{N}}$ converges to $0$. \nSuppose not then it must have a limit $c>0$. \nChoose $c'<c$ such that $F^{\\circ 2}_d(P_c)\\subseteq P_{c'}.$\nThen for $n$ large enough we must have $F^{\\circ 2}_{d}(P_{c_n})\\subseteq P_{c/2+c'/2}$, contradicting the choice of $c_{n+1}$.\n\nSince $\\{c_n\\}_{n\\in \\mathbb{N}}$ converges to $0$, it follows that the sequence $\\mathcal{T}_n$ converges to $\\{0\\}$.\nWith Lemma~\\ref{lem: Tab sequence} this implies \\eqref{equation:uniqueness critical}.\n\nTo prove the second part let $\\avar\\in (0,1)$. Consider the decreasing sequence $\\{\\cvar_n\\}_{n\\in \\mathbb N}$ with $\\cvar_n=(q+1)\\alpha^{n-1}$. Set $\\mathcal{T}_{2n-1}=P_{\\cvar_n}$ and $\\mathcal{T}_{2n}=F_{d,\\avar}(P_{\\cvar_{n-1}})$.\nWe use the following observation.\n\\begin{lemma}\\label{lem:increasingd and alpha}\nFor any $\\avar \\in (0,1]$, any $\\mathbf{x} \\in \\mathbb{R}^{q-1}$ and any integer $d$ there is $d' \\geq d$ such that $F_{d,\\avar}(\\mathbf{x}) = \\frac{d}{d'} \\cdot F_{d'}(\\mathbf{x})$. Moreover, $\\frac{d}{d'}\\leq \\alpha$.\n\\end{lemma}\n\\begin{proof}\nWhen viewing $\\avar$ and $d$ as variables, $\\tfrac{1}{d}F_{d,\\avar;i}$ only depends on the ratio $\\tfrac{\\avar}{d+1}$. \nTherefore the first statement of the lemma holds with $d'$ defined by $\\tfrac{\\avar}{d+1}=\\tfrac{1}{d'+1}.$\nSince $\\tfrac{d}{d'}=\\tfrac{\\avar d}{d+1-\\avar}$, the second statement also holds.\n\\end{proof}\n\nThe lemma above implies that $F_{d,\\alpha}(P_{\\cvar_n})=\\tfrac{d}{d'}\\cdot F_{d'}(P_{\\cvar_n})$ and hence is convex for each $c_n.$\nIt moreover implies that \n\\[F^{\\circ 2}_{d,\\alpha}(P_{\\cvar_n})\\subset \\alpha F_{d'}(\\alpha F_{d'}( P_{\\cvar_n})))\\subset\\alpha P_{\\cvar_{n}}=P_{\\cvar_{n+1}}.\\] \nBy basic properties of the logarithm, \\eqref{equation:uniqueness subcritical} now quickly follows.\nThis finishes the proof of Theorem~\\ref{thm:main}.\n\nThe strengthening mentioned in Remark~\\ref{rem:strengthen} can be derived from the fact that the derivative of $F_{d,\\alpha}$ at $\\mathbf{0}$ is equal to $\\frac{-\\alpha d}{d+1-\\alpha}\\text{Id}$.\nNote that $\\frac{\\alpha d}{d+1-\\alpha}<\\alpha$ for all $\\alpha\\in (0,1)$ and $d$. \nTherefore on a small enough open ball $B$ around $\\mathbf{0}$ the operator norm of the derivative of $F_{d,\\alpha}$ can be bounded by $\\hat{\\alpha}$ for all $d\\geq d_0$ (and by $\\alpha$ for fixed $d\\geq d_0$). \nThen for any integer $n\\geq 0$, $F^{\\circ n}_{d,\\alpha}(B)\\subset \\hat{\\alpha}^n B$ ($\\alpha^n B$ respectively).\nFor $n_0$ large enough $P_{\\cvar_{n_0}}$ is contained in this ball $B$. \nFor $n>2n_0$ we then set $\\mathcal{T}_n=\\hat{\\alpha}^{n-2n_0} B$ ($\\alpha^{n-2n_0} B$ respectively). \nThe statements in the remark now follow quickly.\n\n\n\\subsection{The \\texorpdfstring{$d\\rightarrow \\infty$ limit map}{The d \u2192 \u221e limit map}}\nAs mentioned above, an important tool in our approach is to analyze the maps $F_d$ as $d\\to \\infty.$ \n    Since $F_d(\\mathbb{R}^{q-1})$ is bounded, it follows that as $d\\to \\infty$, $F_{d}(x_1,\\ldots,x_{q-1})$ converges uniformly to the limit map \n\\begin{equation}\\label{eq:define F infty}\n    F_{\\infty}(x_1,\\ldots,x_{q-1}),\n\\end{equation}\nwith coordinate functions\n\\begin{equation}\\label{eq:def F infty coordinate}\n    F_{\\infty;i}(x_1,\\ldots,x_{q-1}):=q\\frac{1-e^{x_i}}{\\sum_{j=1}^{q-1}e^{x_i}+1}.\n\\end{equation}\nWe write  $G_{\\infty;i}(x_1,\\ldots,x_{q-1}) = q\\frac{1-x_i}{\\sum_{j=1}^{q-1}x_j+1}$ for the $i$th coordinate function of the fractional linear map $G_{\\infty}$. Note that $F_{\\infty} = G_{\\infty} \\circ \\exp$.\n\nBy Lemma \\ref{lem:symmetry} for any $\\pi \\in S_q$, any $\\mathbf{x} \\in \\mathbb{R}^{q-1}$ and any $d$ we have $\\pi \\cdot F_{d}(\\mathbf{x}) =  F_{d}(\\pi \\cdot \\mathbf{x})$. As the action of  $\\pi$ on $\\mathbb{R}^{q-1}$ does not depend on $d$, we immediately see $\\pi \\cdot F_{\\infty}(\\mathbf{x}) =  F_{\\infty}(\\pi \\cdot \\mathbf{x})$ follows.\n\nIn the next two sections we will prove Propositions~\\ref{prop:convex} and~\\ref{prop:2stepforward}. \nThe idea is to first prove a variant of these propositions for the map $F_\\infty$ and then use that $F_d\\to F_\\infty$ uniformly to finally prove the actual statements.\nWe use the description of $P_\\cvar$ as intersection of half spaces $\\pi\\cdot H_{\\geq -\\cvar}$ in Section~\\ref{sec:convex} and the description as the union of the $\\pi \\cdot D_\\cvar$ in Section~\\ref{sec:forward}.\n\n\n\\section{Convexity of the forward image of \\texorpdfstring{$P_c$}{Convexity of the forward image of Pc}}\\label{sec:convex}\n\nThis section is dedicated to proving Proposition \\ref{prop:convex}.\n\nFix an integer $q\\geq 3$.\nRecall for $\\mu\\in \\mathbb{R}$ the definition of the half space \n\\[\nH_{\\geq \\mu}=\\left\\{\\mathbf{x}\\in \\mathbb{R}^{q-1}\\mid \\sum_{i=1}^{q-1} x_i\\geq\\mu\\right\\}.\n\\]\nThe half space $H_{\\leq \\mu}$ is defined similarly. We denote by $H_{\\mu}$ the affine space which is the boundary of $H_{\\leq \\mu}$.\n\n\nIn what follows we will often use that the map $G_\\infty$ is a fractional linear transformation and thus preserves lines and hence maps convex sets to convex sets, see e.g.~\\cite[Section 2.3]{boyd2004convex}.\n\\begin{lemma}\\label{lem:forward convex infinity}\nFor all $\\cvar>0$, the set $\\exp(H_{\\ge -\\cvar}):=\\{\\exp(\\mathbf{x})\\mid \\mathbf{x}\\in H_{\\geq -c}\\}$ is strictly convex, consequently\n\\[\n    G_\\infty(\\exp(H_{\\ge -\\cvar}))\n\\]\nis strictly convex.\n\\end{lemma}\n\\begin{proof}\nSince $G_\\infty$ is a fractional linear transformation, it preserves convex sets. \nIt therefore suffices to show that $\\exp(H_{\\geq -\\cvar})$ is strictly convex. \n\nTo this end take any $\\mathbf{x},\\mathbf{y}\\in \\exp(H_{\\geq -\\cvar})$ and let $\\lambda\\in (0,1)$.\nWe need to show that $\\lambda \\mathbf{x} +(1-\\lambda)\\mathbf{y}\\in \\exp(H_{\\geq -\\cvar})$.\nBy strict concavity of the logarithm we have\n\\[\n\\sum_{i=1}^{q-1}\\log(\\lambda x_i+(1-\\lambda)y_i)\\geq \\sum_{i=1}^{q-1} \\lambda \\log(x_i)+(1-\\lambda)\\log(y_i)>-\\cvar,\n\\]\nwe conclude that $\\exp(H_{\\geq -\\cvar})$ is strictly convex.\n\\end{proof}\n\nIn what follows we need the \\emph{angle} between the tangent space of $G_\\infty(\\exp(H_{-c}))$ for $c>0$ at $G_\\infty(\\mathbf{x})$ for any $\\mathbf{x}\\in \\exp(H_{-\\cvar})$ and the space $H_0$. This angle is defined as the angle of a normal vector of the tangent space pointing towards the interior of $G_\\infty(\\exp(H_{\\geq -c}))$ and the vector $-\\mathbf{1}$ (which is a normal vector of $H_0$).\n\n\\begin{lemma}\\label{lem: forward infinite try 2}\nFor any $\\cvar\\in[0,q+1]$ and any $\\mathbf{x}\\in \\exp(H_{-\\cvar})$ the angle between the tangent space of $G_\\infty(\\exp(H_{-c}))$ at $G_\\infty(\\mathbf{x})$ and $H_0$ is strictly less than $\\pi/2$.\n\\end{lemma}\n\\begin{proof}\nWe will first show that the tangent space cannot be orthogonal to $H_0$.\n\nThe map $G_\\infty$ is invertible (when restricted to $\\mathbb{R}_{>0}^{q-1}$) with inverse $G^{-1}_\\infty$ whose coordinate functions are given by\n\\[\nG^{-1}_{\\infty,i}(y_1,\\ldots,y_{q-1})= \\frac{-qy_i}{\\sum_{i=1}^{q-1}y_i+q}+1.\n\\]\n\nDefine $g:\\mathbb{R}^{q-1}\\setminus H_{-q}\\to \\mathbb{R}$ by $g(\\mathbf{y})=\\prod_{i=1}^{q-1}G^{-1}_{\\infty,i}(\\mathbf{y})$.\nThen the image of $\\exp(H_{-c})$ under $G_\\infty$ is contained in the hypersurface $\\{\\mathbf{y}\\in \\mathbb{R}^{q-1}\\mid g(\\mathbf{y})=\\exp(-c)\\}.$\nTherefore a normal vector of the tangent space of $G_\\infty(\\exp(H_{-c}))$ at $\\mathbf{y}=G_\\infty(\\mathbf{x})$ is given by the gradient of the function $g$.\nThus to show that this tangent space is not orthogonal to $H_0$, we need to show that \n\\begin{equation}\\label{eq:not orthogonal}\n\\sum_{i=1}^{q-1} \\tfrac{\\partial}{\\partial y_i}g(\\mathbf{y})\\neq 0.\n\\end{equation}\nWe have \n\\begin{align*}\n\\sum_{i=1}^{q-1} \\tfrac{\\partial}{\\partial y_i}g(\\mathbf{y})&=\\sum_{i=1}^{q-1}\\sum_{j=1}^{q-1} \\frac{\\prod_{k=1}^{q-1}G^{-1}_{\\infty,k}(\\mathbf{y})}{G^{-1}_{\\infty,j}(\\mathbf{y})}\\tfrac{\\partial}{\\partial y_i}G^{-1}_{\\infty,j}(\\mathbf{y})\n\\\\\n&=\\sum_{j=1}^{q-1}\\frac{\\prod_{k=1}^{q-1}G^{-1}_{\\infty,k}(\\mathbf{y})}{G^{-1}_{\\infty,j}(\\mathbf{y})}\\sum_{i=1}^{q-1}\\tfrac{\\partial}{\\partial y_i}G^{-1}_{\\infty,j}(\\mathbf{y})\n\\\\\n&=\\sum_{j=1}^{q-1}\\frac{\\prod_{k=1}^{q-1}G^{-1}_{\\infty,k}(\\mathbf{y})}{G^{-1}_{\\infty,j}(\\mathbf{y})} \\cdot \\frac{-q(\\sum_{i=1}^{q-1}y_i+q)+q(q-1)y_j}{(\\sum_{i=1}^{q-1}y_i+q)^2}\n\\\\\n&=\\sum_{j=1}^{q-1}\\frac{\\prod_{k=1}^{q-1}G^{-1}_{\\infty,k}(\\mathbf{y})}{G^{-1}_{\\infty,j}(\\mathbf{y})}\\cdot\\frac{-(q-1)G_{\\infty,j}^{-1}(\\mathbf{y})-1}{\\sum_{i=1}^{q-1}y_i+q}.\n\\end{align*}\n\nSince $G^{-1}_{\\infty,k}(\\mathbf{y})>0$ for each $k$, all terms in the final sum are nonzero and have the same sign. This proves~\\eqref{eq:not orthogonal}.\n\nSince the angle between the tangent space of $G_\\infty(\\exp(H_{-c}))$ at $G_\\infty(\\mathbf{x})$ and $H_0$ depends continuously on $\\mathbf{x}$ this angle should either be always less than $\\pi/2$ or always be bigger.\nSince by the previous lemma the set $G_\\infty(\\exp(H_{\\geq -c}))$ is convex, it is the former.\n\\end{proof}\n\nWe next continue with the finite case.\nWe will need the following definition.\nThe \\emph{hypograph} of a function $f:D\\to \\mathbb{R}$ is the region $\\{(x,y)\\mid x\\in D, y\\leq f(x)\\}$.\nBelow we will consider a hypersurface contained in $\\mathbb{R}^{q-1}$ that we view as the graph of a function with domain contained in $H_0$. \nIn this context the hypograph of such a function is again contained in $\\mathbb{R}^{q-1}$, but the `positive $y$-axis' points in the direction of $\\mathbf{1}$ as seen from $\\mathbf{0} \\in H_0$.\n\n\\begin{figure}\n    \\centering\n\\begin{tikzpicture}\n\\centering\n   \\filldraw[lightBlue](0.0,1.20298)--(0.0854683,1.22135)--(0.169623,1.23746)--(0.252203,1.25111)--(0.332949,1.26209)--(0.411607,1.27023)--(0.487928,1.27532)--(0.561674,1.27719)--(0.632616,1.27568)--(0.700539,1.27064)--(0.765244,1.26192)--(0.826546,1.2494)--(0.88428,1.23299)--(0.938302,1.21258)--(0.988486,1.18813)--(1.03473,1.15958)--(1.07696,1.12691)--(1.1151,1.09013)--(1.14914,1.04926)--(1.17907,1.00434)--(1.20488,0.955462)--(1.22663,0.902707)--(1.24437,0.846195)--(1.25817,0.786064)--(1.26815,0.722475)--(1.2744,0.655603)--(1.27707,0.585644)--(1.27631,0.512808)--(1.27227,0.437319)--(1.26513,0.359413)--(1.25508,0.279335)--(1.2423,0.197338)--(1.22699,0.113679)--(1.20935,0.0286192)--(-1.21547,-1.15836)--(-1.23236,-1.09062)--(-1.24685,-1.02198)--(-1.25874,-0.952488)--(-1.26784,-0.882209)--(-1.27397,-0.811205)--(-1.27693,-0.73954)--(-1.27657,-0.667276)--(-1.27272,-0.594473)--(-1.26524,-0.521195)--(-1.254,-0.447503)--(-1.2389,-0.373456)--(-1.21983,-0.299116)--(-1.19673,-0.224541)--(-1.16955,-0.149791)--(-1.13826,-0.0749248)--(-1.10284,0.0)--(-1.06333,0.0749248)--(-1.01976,0.149791)--(-0.972191,0.224541)--(-0.920715,0.299116)--(-0.865441,0.373456)--(-0.8065,0.447503)--(-0.744045,0.521195)--(-0.678246,0.594473)--(-0.609294,0.667276)--(-0.537392,0.73954)--(-0.462762,0.811205)--(-0.385635,0.882209)--(-0.306253,0.952488)--(-0.224868,1.02198)--(-0.141734,1.09062)--(-0.0571119,1.15836)--(1.18073,-0.0286192)--(1.11331,-0.113679)--(1.04496,-0.197338)--(0.975742,-0.279335)--(0.905718,-0.359413)--(0.83495,-0.437319)--(0.763498,-0.512808)--(0.691427,-0.585644)--(0.618797,-0.655603)--(0.545671,-0.722475)--(0.472109,-0.786064)--(0.398174,-0.846195)--(0.323925,-0.902707)--(0.249422,-0.955462)--(0.174724,-1.00434)--(0.0998897,-1.04926)--(0.0249778,-1.09013)--(-0.0499535,-1.12691)--(-0.124846,-1.15958)--(-0.199641,-1.18813)--(-0.274281,-1.21258)--(-0.348706,-1.23299)--(-0.422857,-1.2494)--(-0.496674,-1.26192)--(-0.570097,-1.27064)--(-0.643065,-1.27568)--(-0.715516,-1.27719)--(-0.787388,-1.27532)--(-0.858618,-1.27023)--(-0.929146,-1.26209)--(-0.998907,-1.25111)--(-1.06784,-1.23746)--(-1.13589,-1.22135)--(-1.20298,-1.20298)--(0.0,1.20298);\n   \\draw[-] (-3.8, 0) -- (2, 0) node[below] {$x_1$};\n  \\draw[-] (0, -2.5) -- (0, 2) node[left] {$x_2$};\n  \\draw[scale=1, domain=0:1, smooth, variable=\\x, gR] plot ({20*ln(1 + (1 - exp(2*(1 - \\x) - 4*\\x))/(\n   7*(6/7 + exp(2*(1 - \\x) - 4*\\x) + exp(-4 *(1 - \\x) + 2*\\x))))},{20*ln(1 + (1 - exp(-4*(1 - \\x)+2*\\x))/(\n   7*(6/7 + exp(2*(1 - \\x) - 4*\\x) + exp(-4 *(1 - \\x) + 2*\\x))))});\n  \\draw[-,dY] (-3, 3) -- (3,-3) node[left, dY] {$H_0$};\n  \\draw[-,gG] (1.7,-1.7) -- (-1.7, 1.7) node[right, gG] {$\\text{Dom}_\\cvar$};\n \\node [gB] at (-0.6,-0.5) {$F_\\yvar(P_\\cvar)$};\n  \\node [gR] at (1.8,1.8) {$F_\\yvar(H_{-\\cvar})$};\n \n\\end{tikzpicture}\n    \\caption{Depicting the situation in Lemma \\ref{lem:forward finite}, for $q=3,\\cvar = 2$ and $\\yvar = \\frac{1}{20}$. The domain $\\text{Dom}_\\cvar$ of the function $h_{\\yvar,\\cvar}$ which we define in the proof of Lemma \\ref{lem:forward finite} is made by choosing $a' = -3$.}\n    \\label{fig:ConexityPicture}\n\\end{figure}\n\n\\begin{lemma}\\label{lem:forward finite}\nThere exists $y_1>0$ such that for all $y\\in[0,y_1)$ and $\\cvar\\in[0,q+1]$ the set $F_y(P_c)$ is contained in the hypograph of a concave function, $h_{\\yvar,\\cvar}$, with a convex compact domain in $H_0$.\n\\end{lemma}\n\\begin{proof}\nWe first prove that for any $\\mathbf{x}\\in H_{0}$ and $\\cvar\\in [0,q+1]$ there exists an open neighborhood $W_{\\cvar,\\mathbf{x}}=Y_{\\cvar,\\mathbf{x}}\\times C_{\\cvar,\\mathbf{x}}\\times X_{\\cvar,\\mathbf{x}}$ of $(0,\\cvar,\\mathbf{x})\\in [0,1]\\times[0,q+1] \\times \\mathbb{R}^{q-1}$ such that the following holds for any $(y',\\cvar',\\mathbf{x}')\\in W_{\\cvar,\\mathbf{x}}$:\n\\begin{align}\n   &\\text{the angle between the tangent space of $F_{1/y'}(H_{-\\cvar'})$ at $F_{1/y'}(\\mathbf{x}'_{\\cvar'})$ and $H_0$}\\nonumber \\\\&\\text{is strictly less than $\\pi/2$,}\\label{eq:90degrees}\n\\end{align}\nwhere we denote $\\mathbf{x}_\\cvar:=\\mathbf{x}-\\frac{\\cvar}{q-1}\\mathbf{1}\\in H_{-\\cvar}$.\nTo see this note that by the previous lemma we have that the tangent space of  $F_{\\infty}(H_{-\\cvar})$ at $F_{\\infty}(\\mathbf{x}_\\cvar)$ is not orthogonal to $H_0$ and in fact makes an angle of less than $\\pi/2$ with $H_0$. \nSay it has angle $\\pi/2-\\gamma$. \nSince $(y,\\cvar,\\mathbf{x})\\mapsto F_{1/y}(\\mathbf{x}_\\cvar)$ is analytic, there exists an open neighborhood $W_0$ of $(0,\\cvar,\\mathbf{x})$ such that for any $(y',\\mathbf{x'},\\cvar')\\in W_0$ the angle between the tangent space of $F_{1/y'}(H_{-\\cvar'})$ at $F_{1/y'}(\\mathbf{x'}_{\\cvar'})$ and $H_0$ is at most $\\pi/2-\\gamma/2$.\nClearly, $W_0$ contains an open neighborhood of $(0,\\cvar,\\mathbf{x})$ of the form $Y\\times C\\times X$ proving~\\eqref{eq:90degrees}.\n\nNext fix $\\cvar\\in [0,q+1]$ and $\\mathbf{x}\\in H_0$ and write $W_{\\cvar,\\mathbf{x}}=Y\\times C\\times X.$\nTogether with the implicit function theorem,~\\eqref{eq:90degrees} now implies that for each $y'\\in Y$ and any $\\cvar'\\in C$, that locally at $\\mathbf{x}_{\\cvar'}$, $F_{1/y'}( H_{-\\cvar})$ is the graph of an analytic function $f_{y',\\cvar',\\mathbf{x}}$ on an open domain contained in $H_0$. \nHere we use that $F_{1/y}$ is invertible with analytic inverse.\nBy choosing $Y$ and $C$ small enough, we may by continuity assume that we have a common open domain, $D_{\\cvar,\\mathbf{x}}$, for these functions for all $\\cvar'\\in C$ and $y'\\in Y$, where we may moreover assume that these functions are all defined on the closure of $D_{\\cvar,\\mathbf{x}}$.\n\nWe next claim, provided the neighbourhood $W=Y_{\\cvar,\\mathbf{x}}\\times C_{\\cvar,\\mathbf{x}}$ is chosen small enough, that for each $y'\\in Y$ and $\\cvar'\\in C$,\n\\begin{equation}\\label{eq:hessian}\n \\text{the largest eigenvalue of the Hessian $f_{y',\\cvar',\\mathbf{x}}$ on $D_{\\cvar, \\mathbf{x}}$ is strictly less than $0$.}\n\\end{equation}\nTo see this we note that by the previous lemma we know that $F_{\\infty}(H_{\\ge -\\cvar})$ is strictly convex. \nTherefore the Hessian\\footnote{Recall that the \\emph{Hessian} of a function $f:U\\to \\mathbb{R}$ for an open set $U\\subseteq\\mathbb{R}^n$ at a point $u\\in U$ is defined as the $n\\times n$ matrix $H_f(u)$ with $(H_f(u))_{i,j}=\\tfrac{\\partial^2 f}{\\partial x_i\\partial x_j}(u)$. When these partial derivatives are continuous and the domain $U$ is convex, $f$ is concave if and only if its Hessian is negative definite at each point of the domain $U$~\\cite{boyd2004convex}.} of $f_{0,\\cvar,\\mathbf{x}}$ on $D_{\\cvar,\\mathbf{x}}$ is negative definite, say its largest eigenvalue is $\\delta<0$. \nSimilarly as before, there exists an open neighborhood $W'\\subseteq W$ of $(0,\\cvar)$ of the form $W'=Y'\\times C'$ such that for each $y'\\in Y'$ and $\\cvar'\\in C'$, the function $f_{y',\\cvar',\\mathbf{x}}$ has a negative definite Hessian with largest eigenvalue at most $\\delta/2<0$ for each $\\mathbf{z}\\in D_{\\cvar,\\mathbf{x}}$ (by compactness of the closure of \n$D_{\\cvar,\\mathbf{x}}$).\nWe now want to patch all these function to form a global function on a compact and convex domain.\nWe first collect some properties of $F_{1/y}$ that will allow us to define the domain.\n\nFirst of all note that by compactness there exists $a>0$ such that for each $c\\in [0,q+1]$, $\\exp(P_c)\\subset H_{\\leq a}$ (where the inclusion is strict).\nWe now fix such a value of $a$.\nSince $G_\\infty$ is $S_q$-equivariant, we know that $G_\\infty(H_{\\leq a})=H_{\\geq a'}$ for some $a'\\in \\mathbb{R}$.\nWe now choose $y^*>0$ small enough such that the following two inclusions hold for all $y\\in [0,y^*]$ and $c\\in [0,q+1]$\n\\begin{align}\n    F_{1/y}(P_c)&\\subset H_{\\geq a'},\\label{eq:image finite in Ha'}\n    \\\\\n    \\text{proj}_{H_0}(F_\\infty(H_{-c})\\cap H_{\\geq a'})&\\subset  \\text{proj}_{H_0}(F_{1/y}(H_{-c})), \\label{eq:contained in image finite}\n\\end{align}\nwhere $\\text{proj}_{H_0}$ denotes the orthogonal projection onto the space $H_0$.\nThe first inclusion holds since $F_{1/y}$ converges uniformly to $F_\\infty$ as $y\\to 0.$\nFor the second inclusion note that  \n\\[\nF_\\infty(H_{-c})\\cap H_{\\geq a'}=G_\\infty(\\exp(H_{-c})\\cap H_{\\leq a})\\subset F_{\\infty}(H_{-c}).\n\\]\nBecause $\\exp(H_{-c})\\cap H_{\\leq a}$ is compact, the desired conclusion follows since $F_{1/y}\\to F_{\\infty}$ uniformly as $y\\to 0$.\n\n\nLet us now consider for $\\cvar\\in [0,q+1]$ the projection \n\\[\n\\text{Dom}_c:=\\text{proj}_{H_0}(F_\\infty(H_{-c})\\cap H_{\\geq a'}),\n\\]\nsee Figure \\ref{fig:ConexityPicture}. Since $F_\\infty(H_{-\\cvar})\\cap H_{\\geq a'}$ is convex by Lemma~\\ref{lem:forward convex infinity} and compact, it follows that $\\text{Dom}_\\cvar$ is compact and convex for each $\\cvar\\in [0,q+1]$.\nMoreover, we claim that\n\\begin{equation}\\label{eq:compact}\n    \\bigcup_{\\cvar\\in [0,q+1]} \\left(\\{\\cvar\\}\\times \\text{Dom}_\\cvar\\right) \\subseteq [0,q+1]\\times H_0 \\text{ is compact.}\n\\end{equation}\nIndeed, it is the continuous image of the compact set $\\exp(H_{\\geq -q-1})\\cap H_{\\leq a}$ under the map\n\\[\n\\exp(H_{\\geq -q-1})\\cap H_{\\leq a}\\to [0,q+1]\\times H_0\n\\] \ndefined by\n\\[\n\\mathbf{x}\\mapsto \\left(\\sum_{i=1}^{q-1}x_i,\\text{proj}_{H_0}(G_\\infty(\\mathbf{x}))\\right).\n\\]\n\n\nBy \\eqref{eq:contained in image finite} $\\text{Dom}_c$ is contained in $\\text{proj}_{H_0}(F_{1/y}(H_{-c}))$ for all $y\\in [0,y^*]$ and $\\cvar\\in [0,q+1]$.\nIt follows that the sets \n$Y_{\\cvar,\\mathbf{x}}\\times C_{\\cvar,\\mathbf{x}}\\times D_{\\cvar,\\mathbf{x}}$, where $\\mathbf{x}$ ranges over $H_{0}$ and $\\cvar$ over $[0,q+1]$, form an open cover of $\\{0\\}\\times \\cup_{\\cvar\\in [0,q+1]}\\left(\\{\\cvar\\}\\times \\text{Dom}_c\\right)$.\nSince the latter set is compact by~\\eqref{eq:compact}, we can take a finite sub cover. \nTherefore there exists $y_1>0$ such that for each $y\\in [0,y_1)$ and each $\\cvar\\in[0,q+1]$ we obtain a unique global function $h_{y,\\cvar}$ on the union of these finitely many domains, which by \\eqref{eq:hessian} has a strictly negative definite Hessian.\nBy construction the union of these domains contains $\\text{Dom}_\\cvar$ for each $\\cvar\\in [0,q+1]$.\nConsequently, restricted to $\\text{Dom}_c$, $h_{y,\\cvar}$ is a concave function for each $y\\in [0,y_1)$ and $\\cvar\\in [0,q+1]$.\nBy~\\eqref{eq:image finite in Ha'}, it follows that $F_{1/y}(P_c)$ is contained in the hypograph of $h_{y,\\cvar}$, as desired.\n\\end{proof}\n\n\nWe can now finally prove Proposition~\\ref{prop:convex}, which we restate here for convenience.\n\\begin{repprop}{prop:convex}\nLet $q\\geq 3$ be an integer. Then there exists $d_1>0$ such that for all $d\\geq d_1$ and $c\\in [0,q+1]$, $F_d(P_c)$ is convex.\n\\end{repprop}\n\\begin{proof}\nBy the previous lemma we conclude that for $d$ larger than $1/\\yvar_1$, $F_d(P_\\cvar)$ is contained in the hypograph of the function $h_{1/d,\\cvar}$, denoted by $\\text{hypo}(h_{\\cvar,1/d})$ and moreover that this hypograph is convex, as the function $h_{1/d,\\cvar}$ is concave on a convex domain.\n\nSince $P_\\cvar$ is invariant under the $S_q$-action, it follows that\n\\[\n\\exp(P_\\cvar)=\\bigcap_{\\pi\\in S_q} \\pi \\cdot (\\exp(H_{\\geq -\\cvar })\\cap H_{\\leq a})\n\\]\nand therefore by Lemma~\\ref{lem:symmetry},\n\\begin{equation}\\label{eq:intersection image}\nF_d(P_c)=\\bigcap_{\\pi\\in S_q}\\pi \\cdot (F_d(P_c))\\subseteq \\bigcap_{\\pi\\in S_q}\\pi \\cdot \\text{hypo}(h_{1/d,\\cvar}).\n\\end{equation}\nWe now claim that the final inclusion in~\\eqref{eq:intersection image} is in fact an equality. \nTo see the other inclusion, take some $\\mathbf{z}\\in \\cap_{\\pi\\in S_q}\\pi \\cdot \\text{hypo}(h_{1/d,\\cvar})$.\nBy symmetry, we may assume that $\\mathbf{z}$ is contained in $\\mathbb{R}^{q-1}_{\\geq 0}$.\nThen $\\mathbf{z}$ is equal to $F_d(\\mathbf{x})$ for some $\\mathbf{x}\\in H_{\\geq -c}\\cap \\mathbb{R}^{q-1}_{\\leq 0}$, implying that $\\mathbf{z}$ is indeed contained in $F_d(P_\\cvar)$.\n\nThis then implies that $F_d(P_\\cvar)$ is indeed convex being equal to the intersection of the convex sets $\\pi \\cdot \\text{hypo}(h_{1/d,\\cvar}).$\n\\end{proof}\n\n\n\\section{Forward invariance of \\texorpdfstring{$P_c$ in two iterations}{Forward invariance of Pc in two iterations}}\\label{sec:forward}\n\nThis section is dedicated to proving Proposition \\ref{prop:2stepforward}.\nWe start with a version of the proposition for $d=\\infty$ and after that consider finite $d$.\n\n\\subsection{Two iterations of \\texorpdfstring{$F_\\infty$}{Two iterations of F\u221e}}\nLet $\\Phi:\\mathbb{R}^{q-1}\\to \\mathbb{R}^{q-1}$ be defined by\n\\[\n    \\Phi(x_1,\\dots,x_{q-1})=F_\\infty^{\\circ 2}(x_1,\\dots,x_{q-1})\n\\]\nand its `restriction' to the half line $\\mathbb{R}_{\\leq/0}\\cdot \\mathbf{1}$, $\\phi:\\mathbb{R}_{\\geq 0}\\to\\mathbb{R}_{\\geq 0}$, by\n\\[\n    \\phi(t)=-\\langle\\Phi(-t/(q-1)\\cdot \\mathbf{1}),\\mathbf{1}\\rangle,\n\\]\nwhere we use $\\langle \\cdot,\\cdot\\rangle$ to denote the standard inner product on $\\mathbb{R}^{q-1}.$\n\nThis subsection is devoted to proving the following result.\n\\begin{prop}\\label{prop:2step-forward infinity}\nFor any $\\cvar\\ge 0$ we have\n\\[\n\\Phi(P_{\\cvar})\\subseteq P_{\\phi(\\cvar)}\\subsetneq P_\\cvar.\n\\]\n\\end{prop}\nBy the definition of $P_c$ in terms of $D_c$, \\eqref{eq:fund domain}, and the $S_q$-equivariance of the map $F_\\infty$ and hence of the map $\\Phi$, it suffices to prove this for $P_c$ replaced by $D_c$.\nThis can be derived from the following two statements:\n\\begin{enumerate}\n    \\item[(i)] For any $\\cvar\\ge 0$ the minimum of $\\langle\\Phi(\\mathbf{x}),\\mathbf{1}\\rangle$ on $-\\cvar\\Delta$ is attained at $-\\cvar/(q-1)\\cdot \\mathbf{1}$.\n    \\item[(ii)] For any $\\cvar> 0$ we have $\\phi(\\cvar)<\\cvar$.\n\\end{enumerate}\nIndeed, these statements imply that for any $\\cvar> 0$ we have that $\\Phi(-\\cvar\\Delta)\\subseteq D_{\\phi(\\cvar)}\\subsetneq D_\\cvar$.\nClearly this is sufficient, since $D_\\cvar=\\cup_{0\\le \\cvar'\\le \\cvar} -\\cvar'\\Delta$ and therefore\n\\[\n    \\Phi(D_\\cvar)=\\cup_{0\\le \\cvar'\\le \\cvar}\\Phi(-\\cvar'\\Delta)\\subseteq \\cup_{0\\le \\cvar'\\le \\cvar}D_{\\phi(\\cvar')}\\subseteq D_{\\phi(\\cvar)}\\subsetneq D_{\\cvar}.\n\\]\nWe next prove both statements, starting with the first one.\n\\subsubsection{Statement (i)}\n\\begin{prop}\\label{prop:infinite2iteration}\nLet $\\cvar\\geq 0$. Then for any $\\mathbf{x}\\in -\\cvar \\Delta$ we have that\n\\[\n    \\langle\\Phi(\\mathbf{x}),\\mathbf{1}\\rangle \\ge \\left\\langle\\Phi\\left(\\frac{-\\cvar}{q-1}\\mathbf{1}\\right),\\mathbf{1}\\right\\rangle.\n\\]\nMoreover, equality happens only at $\\mathbf{x}=\\frac{-\\cvar}{q-1}\\mathbf{1}$.\n\\end{prop}\n\nBefore giving a proof, let us fix some further notation. \nBy definition we have\n\\[\n    \\langle \\Phi(\\mathbf{x}),\\mathbf{1}\\rangle =\\sum_{i=1}^{q-1}q\\frac{1-e^{F_{\\infty;i}(\\mathbf{x})}}{\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}+1}=\\frac{q^2}{{\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}+1}}-q,\n\\]\nwhere we recall that $F_{\\infty;j}$ denotes the $j$th coordinate function of $F_\\infty$.\nThus the $i$th coordinate of the gradient of $\\langle \\Phi(\\mathbf{x}),\\mathbf{1}\\rangle$ is given by\n\\begin{align*}\n    \\psi_i(\\mathbf{x})&:=\\frac{-q^2}{\\left({\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}+1}\\right)^2}\\left(\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}\\cdot \\frac{\\partial F_{\\infty;j}}{\\partial x_i}(\\mathbf{x})\\right)\\\\\n    &=\\frac{q^3e^{x_i}\\left(e^{F_{\\infty;i}(\\mathbf{x})}(1+\\sum_{j=1}^{q-1}e^{x_j})+\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}(1-e^{x_j})\\right)}{\\left({\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}+1}\\right)^2\\left(\\sum_{j=1}^{q-1}e^{x_j}+1\\right)^2}\n\\end{align*}\n\nLet us define the following functions $v_i:\\mathbb{R}^{q-1}\\to\\mathbb{R}$ for $i=1,\\dots,q-1$ as\n\\[\n    v_i(\\mathbf{x}):=x_i \\left(e^{G_{i}} (1+ \\sum_{\\substack{j=1}}^{q-1} x_j) + \\sum_{\\substack{j=1}}^{q-1} e^{G_{j}} (1 - x_j)  \\right),\n\\]\nwhere we write\n\\[\n    G_i:=G_{\\infty;i}(\\mathbf{x}) = \\frac{q (1-x_i)}{1 + x_1 + \\cdots + x_{q-1}}.\n\\]\n\nThen we see that \n\\[\n    \\psi_i(\\mathbf{x})=\\frac{q^3}{\\left({\\sum_{j=1}^{q-1}e^{F_{\\infty;j}(\\mathbf{x})}+1}\\right)^2\\left(\\sum_{j=1}^{q-1}e^{x_j}+1\\right)^2}\\cdot v_i(e^{x_1},\\dots,e^{x_{q-1}}),\n\\]\nand $\\psi_1(\\mathbf{x})=\\dots =\\psi_{q-1}(\\mathbf{x})$ if and only if $v_1(\\exp(\\mathbf{x}))=\\dots=v_{q-1}(\\exp(\\mathbf{x}))$.\n\n\\begin{proof}[Proof of Proposition~\\ref{prop:infinite2iteration}]\nFirst of all observe that the function $\\langle\\Phi(\\mathbf{x}),\\mathbf{1}\\rangle$ is invariant under the permutation of the coordinates of $\\mathbf{x}$. Thus we can assume that \n\\[\n    \\mathbf{x}\\in U:=\\{\\mathbf{y}\\in\\mathbb{R}^{q-1}~|~0\\ge y_1 \\dots\\ge y_{q-1}  \\}\n\\]\n and not all the coordinates of $\\mathbf{x}$ are equal.\n\nNow it is enough to show that there exists a vector $\\mathbf{0}\\neq \\mathbf{w}\\in\\mathbb{R}^{q-1}$ such that in the direction of $\\mathbf{w}$ the function is (strictly) decreasing, $\\langle\\mathbf{w},\\mathbf{1}\\rangle=0$ and $\\mathbf{x}+t_0\\mathbf{w}\\in U$ for some small $t_0>0$.\nLet \\[\\ell=\\min\\{1\\le i\\le q-2 ~|~x_i>x_{i+1}\\},\\] which is finite, since not all of the coordinates of $\\mathbf{x}$ are equal. \n\nWe claim that $\\mathbf{w}=-\\frac{\\mathbf{e}_1+\\dots+\\mathbf{e}_{\\ell}}{\\ell}+\\mathbf{e}_{\\ell+1}$ satisfies the desired conditions. \nClearly, $\\mathbf{w}$ is perpendicular to $\\mathbf{1}$ and $\\mathbf{x}+t\\mathbf{w}\\in U$ for $t$ small enough. \nNow let us calculate the derivative of \n\\[\n    g(t):=\\langle\\Phi(\\mathbf{x}+t\\mathbf{w}),\\mathbf{1}\\rangle.\n\\]\nUsing the notation defined above, we obtain\n\\begin{align*}\n    g'(0)=&-\\frac{\\psi_1(\\mathbf{x})+\\dots+\\psi_{\\ell}(\\mathbf{x})}{\\ell}+\\psi_{\\ell+1}(\\mathbf{x})\\\\\n    &= -\\psi_{\\ell}(\\mathbf{x})+\\psi_{\\ell+1}(\\mathbf{x})\\\\\n    &=-C\\cdot (v_{\\ell}(\\exp(\\mathbf{x}))-v_{\\ell+1}(\\exp(\\mathbf{x})))\n    \\\\\n    &=-C\\cdot (v_{\\ell}(\\mathbf{y})-v_{\\ell+1}(\\mathbf{y})),\n\\end{align*}\nwhere $C>0$ and  $\\mathbf{y}=\\exp(\\mathbf{x})$. \nIn particular, \\[1\\ge y_1=y_2=\\ldots=y_\\ell>y_{\\ell+1}\\ge \\ldots \\ge y_{q-1}\\ge 0.\\]\nSo to conclude that $g'(0)<0$ and finish the proof, we need to show that \\begin{equation}\\label{eq:condition on y}\n    v_\\ell(\\mathbf{y})-v_{\\ell+1}(\\mathbf{y})>0.\n\\end{equation}\nLemma~\\ref{lemma:balancing} shows that we may assume $\\mathbf{y}$ satisfies $1\\ge y_1=y_2=\\ldots=y_\\ell>y_{\\ell+1}\\ge y_{\\ell+2}=\\ldots =y_{q-1}\\ge 0$.\nLemma~\\ref{lemma:3variable} below shows that for those vectors $\\mathbf{y}$ \\eqref{eq:condition on y} is indeed true.\nSo by combining Lemma~\\ref{lemma:balancing} and Lemma~\\ref{lemma:3variable} below we obtain~\\eqref{eq:condition on y} and finish the proof.\n\\end{proof}\n\n\n\\begin{lemma}\\label{lemma:balancing}\nIf $1\\ge y_1=y_2\\ldots=y_\\ell>y_{\\ell+1}\\ge \\ldots\\ge y_{q-1}\\ge 0$ for some $1\\le \\ell \\le q-2$, then \n\\[\n    v_\\ell(\\mathbf{y})-v_{\\ell+1}(\\mathbf{y})\\ge v_\\ell(\\mathbf{x})-v_{\\ell+1}(\\mathbf{x}),\n\\]\nwhere $\\mathbf{x}\\in\\mathbb{R}^{q-1}$ is defined as \n\\[\n    x_j=\\left\\{\\begin{array}{cc}y_j&\\textrm{ if $j\\le  \\ell+1$}\\\\ \\frac{y_{\\ell+2}+\\dots+y_{q-1}}{q-\\ell-2}&\\textrm{ if $j>\\ell+1$}\\end{array}\\right.\n\\]\nfor $1\\le j\\le q-1$.\n\\end{lemma}\n\\begin{proof}\nBy continuity, it suffices to show \n\\begin{equation}\\label{eq:condition yprime}\n    v_\\ell(\\mathbf{y})-v_{\\ell+1}(\\mathbf{y})\\ge v_\\ell(\\mathbf{x})-v_{\\ell+1}(\\mathbf{x}),\n\\end{equation}\nwhere $\\mathbf{x}\\in\\mathbb{R}^{q-1}$ is defined as\n\\[\n    x_j=\\left\\{\\begin{array}{cc}y_j&\\textrm{ if $j\\neq i,i+1$}\\\\ \\frac{y_i+y_{i+1}}{2}&\\textrm{ if $j=i$ or $j=i+1$}\\end{array}\\right.\n\\]\nfor $1\\le j\\le q-1$ and any $i\\geq \\ell+2.$\n\nFor $t\\in \\mathbb{R}$ we define $\\mathbf{y}(t)$ by \n\\[\n    y_j(t):=\\left\\{\\begin{array}{cc}\n        y_{j} & \\textrm{if $j\\neq i,i+1$}\\\\\n        y_i-t & \\textrm{if $j=i$}\\\\\n        y_{i+1}+t & \\textrm{if $j=i+1$}\n    \\end{array}\\right.\n\\]\nfor $j=1,\\ldots,q-1$.\nNote that $\\mathbf{y}(0)=\\mathbf{y}$ and $\\mathbf{y}(y_i/2-y_{i+1}/2)=\\mathbf{x}$.\nWe further define\n\\begin{align*}\n    \\Delta(t):=&v_\\ell(\\mathbf{y}(t))-v_{\\ell+1}(\\mathbf{y}(t)).\n\\end{align*}\n\nAfter a straightforward calculation we can express $\\Delta(t)$ as \n\\begin{align*}\n    \\Delta(t)&=y_\\ell e^{G_\\ell}(1+\\sum_{j\\ge 1}^{q-1} y_j)-y_{\\ell+1}e^{G_{\\ell+1}}(1+\\sum_{j\\ge 1}^{q-1} y_j)\\\\\n    &+y_\\ell\\sum_{j\\neq i,i+1}e^{G_j}(1-y_j)-y_{\\ell+1}\\sum_{j\\neq i,i+1}e^{G_j}(1-y_j)\\\\\n    &+ (y_\\ell-y_{\\ell+1})\\left(e^{G_i(t)}(1-y_i+t)+e^{G_{i+1}(t)}(1-y_{i+1}-t)\\right),\n\\end{align*}\nwhere we write\n$\n    G_\\ell:=G_{\\infty;\\ell}(\\mathbf{y}(t)) = \\frac{q (1-y_\\ell)}{1 + y_1 + \\cdots + y_{q-1}},\n$\nfor $\\ell\\not \\in \\{ i,i+1\\}$ and we write $G_\\ell(t) = G_{\\infty;\\ell}(\\mathbf{y}(t))$ when $\\ell \\in \\{i,i+1\\}$. This notation indicates that $G_\\ell$ is a constant function of $t$ when $\\ell\\not \\in \\{ i,i+1\\}$.\nNow observe that the function appearing in the last row,\n\\[\n    g(t):=e^{G_i(t)}(1-y_i+t)+e^{G_{i+1}(t)}(1-y_{i+1}-t),\n\\]\nis convex on $t\\in[0,y_i-y_{i+1}]$, since its second derivative is given by\n\\begin{align*}\n    g''(t)&=e^{G_i(t)}\\frac{(1-y_i+t)q^2}{(1+y_1+\\dots+y_{q-1})^2}+2e^{G_i(t)}\\frac{q}{1+y_1+\\dots+y_{q-1}}\\\\\n    &+e^{G_{i+1}(t)}\\frac{(1-y_{i+1}-t)q^2}{(1+y_1+\\dots+y_{q-1})^2}+2e^{G_{i+1}(t)}\\frac{q}{1+y_1+\\dots+y_{q-1}}>0.\n\\end{align*}\nAs $g(t)=g(y_{i}-y_{i+1}-t)$, we obtain that $g(t)$ has a unique minimizer in $[0,y_{i}-y_{i+1}]$ exactly at $t$ such that $=y_{i}-y_{i+1}-t$. In other words,\n\\[\nt=\\frac{y_{i}-x_{i+1}}{2}   \n\\]\nis the unique minimizer of $g(t)$ on this interval and thus for $\\Delta(t)$. \nThis implies \\eqref{eq:condition yprime} and hence the lemma.\n\\end{proof}\n\n\\begin{lemma}\\label{lemma:3variable}\n    Let $1 \\geq x_1 > x_2 \\geq x_3 \\geq 0$ and $q-2 \\geq l \\geq 1$. Then \n    \\[\n        v_{l}(\\underbrace{x_1, \\cdots, x_1}_{l}, x_2, \\underbrace{x_3, \\cdots, x_3}_{q-l-2}) > \n        v_{l+1}(\\underbrace{x_1, \\cdots, x_1}_{l}, x_2, \\underbrace{x_3, \\cdots, x_3}_{q-l-2}).\n    \\]\n\\end{lemma}\n\n\\begin{proof}\nThe algebraic manipulations that are done in this proof, while elementary, involve quite large expressions. Therefore we have supplied additional Mathematica code in Appendix~\\ref{sec: Mathematica code} that can be used to verify the computations. We define\n    \\begin{align*}\n        \\Delta(y_1,y_2,y_3;t):= \n        &\\left(y_1\n        y_3 (t-l-1)+(l+1) y_1+(l+1) y_1 y_2-l y_2\\right) e^{A_{1}(y_1,y_2,y_3;t)}+\\\\\n        &\\left(-y_2 y_3 (t-l-1)-(l+1) y_1 y_2+y_1-2 y_2\\right) e^{A_{2}(y_1,y_2,y_3;t)} +\\\\\n           &\\left(y_1-y_2\\right) \\left(1-y_3\\right) (t-l-1) e^{A_{3}(y_1,y_2,y_3;t)},\n  \\end{align*}\n  where \n  \\[\n        A_{i}(y_1,y_2,y_3;t): = \\frac{(t+1)(1-y_i)}{1+ly_1 + y_2 + (t-(l+1))y_3}\n  \\]\n  for $i = 1,2,3$ (see Listing~\\ref{code: Ai Delta}). One can check that\n  \\[\n    \\Delta(x_1,x_2,x_3;q-1) = v_{l}(x_1, \\cdots, x_1, x_2, x_3, \\cdots, x_3) - v_{l+1}(x_1, \\cdots, x_1, x_2, x_3, \\cdots, x_3).\n  \\]\n  We will treat $t$ as a variable and vary it while keeping the values that appear in the exponents constant. To that effect let $C_i = A_i(x_1,x_2,x_3;q-1)$ and define \n  \\begin{align*}\n    y_1(t) &= \\frac{C_1 (l-t-1)+C_3 (t-l-1)+C_2+t+1}{C_3 (t-l-1)+C_1 l+C_2+t+1},\\\\\n    y_2(t) &= \\frac{C_3 (t-l-1)+C_1 l-C_2 t+t+1}{C_3 (t-l-1)+C_1\n   l+C_2+t+1}, \\\\\n    y_3(t) &= \\frac{C_1 l-C_3 (l+2)+C_2+t+1}{C_3 (t-l-1)+C_1 l+C_2+t+1}.\n  \\end{align*}\n  These values are chosen such that for $t_0=q-1$ we have $y_i(t_0) = x_i$ and $A_i(y_1(t),y_2(t),y_3(t);t) = C_i$ independently of $t$ for $i = 1,2,3$ (see Listings~\\ref{code: yi}~and~\\ref{code: yi2}).\n  Therefore $\\Delta(y_1(t),y_2(t),y_3(t);t)$ is a rational function of $t$ and we want to show that it is positive at $t = q-1$. We can explicitly calculate that \n  \\[\n    \\Delta(y_1(t),y_2(t),y_3(t);t) = \n    \\left(\\frac{1+t}{C_3 (t-l-1)+C_1 l+C_2+t+1}\\right)^2 \\cdot r(t),\n  \\]\n  where $r$ is a linear function (see Listing~\\ref{code: r}). It is thus enough to show that $r(q-1) > 0$. We will do this by showing that $r(l+1) > 0$ and that the slope of $r$ is positive.\n  We find that $r(l+1)$ is equal to\n  \\[\n    r(l+1)=u_1\\cdot e^{C_1}+u_2\\cdot e^{C_2},\n  \\]\n  where\n  \\begin{align*}\n      u_1&=2+l+C_2-2C_1+lC_1C_2-lC_1^2\\\\\n      u_2&=-\\left(2+l+lC_1-(l+1)C_2+C_1C_2-C_2^2\\right).\n  \\end{align*}\n  This is part of the output of Listing~\\ref{code: r slope}.\n  Note that by construction, since $1\\geq x_1>x_2\\geq x_3$, we have $0\\leq C_1<C_2\\leq C_3.$\nTherefore the sum of the coefficients of $e^{C_1}$ and $e^{C_2}$ satisfies\n  \\begin{align*}\n      u_1+u_2&=(l+2)(C_2-C_1)+(l-1)C_1C_2-lC_1^2+C_2^2\\\\\n      &=(l+2+C_2+lC_1)(C_2-C_1)>0.\n  \\end{align*}\n  Now we will separate two cases depending on the sign of  the coefficient of $u_2$. If $u_2$ is non-negative, then\n  \\begin{align*}\n    r(l+1)&=u_1e^{C_1}+u_2e^{C_2}\\ge u_1e^{C_1}+u_2e^{C_1}=(u_1+u_2)e^{C_1}>0.\n  \\end{align*}\n  If $u_2$ is negative, then \n  \\[\n    2+(1+C_1-C_2)l>C_2-C_1C_2+C_2^2=(1+C_2-C_1)C_2.\n  \\] \n  In particular $2+(1+C_1-C_2)l>0$. Thus\n  \\begin{align*}\n      r(l+1)&=e^{C_2}(u_1e^{C_1-C_2}-u_2)\\\\\n      &\\ge (1+C_1-C_2)u_1-u_2= C_1(C_2-C_1)(2+(1+C_1-C_2)l)>0.\n  \\end{align*}\n  The slope of $r$ is given by \n  \\[\n    s:=\\left(1+ C_3 - C_1\\right)e^{C_1} - (1 + C_3 - C_2) e^{C_2} + (C_2-C_1) C_3 e^{C_3}. \n  \\]\n  This is part of the output of Listing~\\ref{code: r slope}.\n  To show that this is positive we show that $s \\cdot e^{-C_2}$ is positive. Because both $1+ C_3 - C_1$ and $C_2 - C_1$ are positive we find\n  \\begin{align*}\n    s \\cdot e^{-C_2} &= \\left(1+ C_3 - C_1\\right)e^{C_1-C_2} - (1 + C_3 - C_2)+ (C_2-C_1) C_3 e^{C_3-C_2} \\\\\n    &\\geq \n    \\left(1+ C_3 - C_1\\right)(1+C_1-C_2) - (1 + C_3 - C_2)+ (C_2-C_1) C_3 (1+C_3-C_2)\\\\\n    &= \n    (C_2-C_1)(C_1 + C_3 (C_3 - C_2)),\n  \\end{align*}\n  which is positive because $0\\leq C_1 < C_2 \\leq C_3$. This concludes the proof.\n\\end{proof}\n\nWe now continue with the second statement.\n\\subsubsection{Statement (ii)}\n\\begin{prop}\nFor any $x>0$ we have that\n\\[\n    \\left\\langle\\Phi\\left(\\frac{-x}{q-1}\\mathbf{1}\\right),\\mathbf{1}\\right\\rangle > -x.\n\\]\n\\end{prop}\n\\begin{proof}\nThe statement is equivalent to\n\\[\n    \\phi(x)<x.\n\\]\nfor $x>0$.\nBy definition we know that\n\\[\n    \\phi(x)=(q-1)\\frac{q(e^{f(x)}-1)}{(q-1)e^{f(x)}+1},\n\\]\nwhere \n\\[\n    f(x)=-q\\frac{e^{-x/(q-1)}-1}{(q-1)e^{-x/(q-1)}+1}.\n\\]\n\nFirst note that $\\phi(\\mathbb{R}_{>0})\\subseteq (0,q)$. This means that if $x\\ge q$, the statement holds. \nThus we can assume that $0<x<q$. Now, the inequality $\\phi(x)<x$ can be written as\n\\[\n    e^{f(x)}<\\frac{x+q(q-1)}{(q-1)(q-x)},\n\\]\nbecause $q-x>0$. \nBy taking logarithm of both sides, we see that $\\phi(x)<x$ is equivalent to\n\\[\n    -q\\frac{e^{-x/(q-1)}-1}{(q-1)e^{-x/(q-1)}+1}<\\log\\left(\\frac{x+q(q-1)}{(q-1)(q-x)}\\right).\n\\]\nSince $\\frac{x+q(q-1)}{(q-1)(q-x)}> \\frac{0+q(q-1)}{(q-1)q}\\ge 1 $,\nwe can use the inequality $\\log(b)>2\\frac{b-1}{b+1}$ for $b=\\frac{x+q(q-1)}{(q-1)(q-x)}$. Therefore, to show $\\phi(x)<x$, it is sufficient to prove that\n\\[\n    -q\\frac{e^{-x/(q-1)}-1}{(q-1)e^{-x/(q-1)}+1}\\le \\frac{-2qx}{(q-2)x-2q(q-1)},\n\\]\nor, equivalently\n\\[\n    (2q-2-x)\\le (x+2q-2)e^{-x/(q-1)}.\n\\]\nThis follows from the fact that $g(t)=(t+2q-2)e^{-t/(q-1)}-(2q-2-t)$ is a convex function on $\\mathbb{R}_{\\ge 0}$, its derivative satisfies $g'(0)=0$ and $g(0)=0$.\nThis concludes the proof.\n\\end{proof}\n\n\\subsection{Two iterations of \\texorpdfstring{$F_d$}{Two iterations of Fd}}\n\nAs before, we view $\\yvar=1/d$ as a continuous variable. \nLet us define $\\Phi: \\mathbb{R}^{q-1}  \\times [0,\\frac{1}{2}] \\rightarrow \\mathbb{R}^{q-1}$ by \n\\[\n\\Phi(x_1,\\ldots,x_{q-1},\\yvar) = F_{1/\\yvar}^{\\circ 2} (x_1,\\ldots,x_{q-1}).\n\\]\nNote that this map is analytic in all its variables. \nFor simplicity, if $\\yvar^*$ is fixed, then we use the notation $\\Phi_{\\yvar^*}(x_1,\\ldots,x_{q-1})$ for $\\Phi(x_1,\\ldots,x_{q-1},\\yvar)|_{\\yvar=\\yvar^*}$, and if $\\yvar=0$, then $\\Phi(x_1,\\ldots,x_{q-1}):=\\Phi_0(x_1,\\ldots,x_{q-1})$.\n\\begin{lemma}\nThere exist positive constants $A>0$ and $\\cvar_0>0$, such that for any $0<\\cvar\\leq\\cvar_0$ we have\n\\[\n   \\cvar-\\phi(\\cvar) \\ge A\\cvar^3.\n\\]\n\\end{lemma}\n\\begin{proof}\nBy definition we know that\n\\[\n    \\phi(x)=(g\\circ f)(x)=(q-1)\\frac{q(e^{f(x)}-1)}{(q-1)e^{f(x)}+1},\n\\]\nwhere \n\\begin{align*}\n    f(x)&=-q\\frac{e^{-x/(q-1)}-1}{(q-1)e^{-x/(q-1)}+1},\\\\\n    g(x)&=(q-1)q\\frac{e^x-1}{(q-1)e^x+1}.\n\\end{align*}\n\nLet us calculate the Taylor expansion of $f(x)$ and $g(x)$ around 0:\n\\begin{align*}\n    f(x)&=\\frac{1}{q-1}x+\\frac{q-2}{2(q-1)^2q}x^2+\\frac{(q^2-6q+6)}{6(q-1)^3q^2}x^3+O(x^4),\\\\\n    g(x)&=(q-1)x-\\frac{(q-1)(q-2)}{2q}x^2+\\frac{(q-1)(q^2-6q+6)}{6q^2}x^3+O(x^4).\n\\end{align*}\nThus their composition has the following Taylor expansion around $0$:\n\\begin{align*}\n    (g\\circ f)(x)&=x-\\frac{1}{6(q-1)^2}x^3+O(x^4).\n\\end{align*}\nThis implies that there exists $\\cvar_0>0$ and $A>0$, such that for any $\\cvar_0\\ge x\\ge 0$ we have\n\\[\n    x-\\phi(x)\\ge Ax^3,\n\\]\nas desired.\n\\end{proof}\n\nThe next proposition implies forward invariance of $P_c$ under $F_d^{\\circ 2}$ for $c$ small enough and $d$ large enough. \n\\begin{prop}\\label{prop:forwardunder2iterates q=3 w=w_c}\nThere exists $\\cvar_0>0$ and $d_0>0$. Such that for all $\\cvar\\in (0,\\cvar_0]$ and integers $d\\geq d_0$ there exists $0<\\cvar'<\\cvar$ such that\n\\[F_d^{\\circ 2}(\\Dvar_\\cvar) \\subset \\Dvar_{\\cvar'}.\n\\]\n\\end{prop}\n\\begin{proof}\nBy the previous lemma we know that there is a $\\cvar'_0>0$ and an $A>0$, such that  for any $\\cvar\\le \\cvar'_0$ we have \n\\[\n    \\|\\Phi(-\\cvar/(q-1)\\cdot \\mathbf{1})+\\cvar/(q-1)\\cdot \\mathbf{1}\\|\\ge A\\cvar^3.\n\\]\nHere we denote by $\\|\\mathbf{x} \\|=\\left(\\sum_{i=1}^{q-1}x_i^2\\right)^{1/2}$, the standard $2$-norm on $\\mathbb{R}^{q-1}.$\nBy Proposition~\\ref{prop:infinite2iteration}, we have that for any $\\mathbf{x}\\in \\Dvar_\\cvar$, $\\Phi(\\mathbf{x})$ is contained in $D_{\\phi(\\cvar)}$. \nTherefore, denoting by $B_r(y)$ the ball of radius $r$ around $y$,\n\n\\begin{equation}\\label{eq:strict inequality 2-forward}\n B_{A\\cvar^{3}/2}(\\Phi(\\mathbf{x}))\\cap (-\\infty,0]^{q-1}\\subseteq \\Dvar_{\\phi(\\cvar)+A\\cvar^3/2} \\subsetneq  \\Dvar_\\cvar.\n \\end{equation}\n\nNow let us consider the Taylor approximation of $\\Phi_{\\yvar}(x_1,\\ldots,x_{q-1})$ at $\\mathbf{0} = (0,\\ldots,0)$. \nSince for any $\\yvar^*\\in[0,1]$ the map $F_{1/y^*}(x_1,\\ldots,x_{q-1})$ has $\\mathbf{0}$ as a fixed point of derivative $-\\textrm{Id}$, there exists constants $\\cvar_1,C_1\\ge 0$ such that for any $\\yvar \\in[0,1]$  and $\\mathbf{x} = (x_1,\\ldots,x_{q-1})\\in [-\\cvar_1,0]^{q-1}$  we have\n\\[\n    \\|\\Phi_{\\yvar}(\\mathbf{x}) - \\textrm{Id}(\\mathbf{x})-T_{3,\\yvar}(\\mathbf{x})\\|\\le C_1\\cdot \\|\\mathbf{x}\\|^4,\n\\]\nwhere $\\textrm{Id}(\\mathbf{x})+T_{3,\\yvar}(\\mathbf{x})$ is the 3rd order Taylor approximation of $\\Phi_{\\yvar}(\\mathbf{x})$ at $\\mathbf{0}$. \nNote that the second order term is equal to $0$ because the derivative of $F_{1/y^*}(x_1,\\ldots,x_{q-1})$ at $\\mathbf{0}$ equals $-\\textrm{Id}$.\nIn particular, $T_{3,\\yvar}(\\mathbf{x})=T_\\yvar((\\mathbf{x}),(\\mathbf{x}),(\\mathbf{x}))$ for some multi-linear map $T_\\yvar \\in\\textrm{Mult}((\\mathbb{R}^{q-1})^3,\\mathbb{R}^{q-1})$, and as $\\yvar \\to 0$ the map $T_{3,\\yvar}$ converges uniformly on $[-q,0]^{q-1}$ to  $T_{3,0}$. Specifically, for any $\\mathbf{x} = (x_1,\\ldots,x_{q-1})\\in[-\\cvar_1,0]^{q-1}$\n\\[\n    \\|T_{3,\\yvar}(\\mathbf{x})-T_{3,0}(\\mathbf{x})\\|\\le A_3(\\yvar)\\|\\mathbf{x}\\|^3\n\\]\nfor some function $A_3$ that satisfies $\\lim_{\\yvar \\to 0} A_3(\\yvar)=0$.\n\n\nPutting this together and making use of the triangle inequality, we obtain that for any $0<\\cvar\\leq \\min\\{\\cvar_1,\\cvar'_0\\}$ and any $\\mathbf{x} = (x_1,\\ldots,x_{q-1})\\in\\Dvar_\\cvar$\n\\begin{align*}\n    \\|\\Phi_{\\yvar}(\\mathbf{x})-\\Phi(\\mathbf{x})\\|&\\le\\|\\Phi_{\\yvar}(\\mathbf{x})-\\textrm{Id}(\\mathbf{x})-T_{3,\\yvar}(\\mathbf{x})\\|\\\\\n    &+\\|\\textrm{Id}(\\mathbf{x})+T_{3,\\yvar}(\\mathbf{x})-\\textrm{Id}(\\mathbf{x})-T_{3,0}(\\mathbf{x})\\|\\\\\n    &+\\|\\textrm{Id}(\\mathbf{x})+T_{3,0}(\\mathbf{x})- \\Phi(\\mathbf{x})\\| \\\\\n    &\\le 2C_1\\|x\\|^4+A_3(y))\\|x\\|^3\\leq K(2C_1c+A_3(y))c^3,\n\\end{align*}\nfor some constant $K>0$ (using that the $2$-norm and the $1$-norm are equivalent on $\\mathbb{R}^{q-1}$.)\nNow let us fix $0<\\cvar_0\\leq \\min \\{\\cvar_1,\\cvar_0'\\}$ small enough such that $K2C_1\\cvar_0<A/4$ and fix a $\\yvar_0>0$ such that for any any $0\\le \\yvar\\le \\yvar_0$ we have $K A_3(\\yvar)\\le A/4$.\n\nThen by \\eqref{eq:strict inequality 2-forward}, for any $0\\le \\yvar \\le \\yvar_0$, $0\\le \\cvar \\le \\cvar_0$ and $\\mathbf{x} = (x_1,\\ldots,x_{q-1})\\in \\Dvar_\\cvar$, \n\\begin{align*}\n    \\Phi_\\yvar(\\Dvar_\\cvar)\\subseteq B_{Ac^3/2}(\\Phi(\\Dvar_\\cvar))\\cap (-\\infty,0]^{q-1} \\subseteq \\Dvar_{\\phi(\\cvar)+A\\cvar^3/2}\\subsetneq \\Dvar_\\cvar.\n\\end{align*}\nSo we can take $\\cvar'=\\phi(\\cvar)+A\\cvar^3/2$.\n\\end{proof}\n\n\\subsection{Proof of Proposition~\\ref{prop:2stepforward}}\nWe are now ready to prove Proposition~\\ref{prop:2stepforward}, which we restate here for convenience. \n\n\\begin{repprop}{prop:2stepforward}\nLet $q\\geq 3$ be an integer. There exists $d_2>0$ such that for all integers $d\\geq d_2$ the following holds:\nfor any $\\cvar\\in(0,q+1]$ there exists $0<\\cvar'<\\cvar$ such that\n\\[\nF_d^{\\circ 2}(P_\\cvar)\\subset P_{\\cvar'}.\n\\]\n\\end{repprop}\n\\begin{proof}\nWe know by Proposition~\\ref{prop:forwardunder2iterates q=3 w=w_c} there is a $d_0>0$ and a $\\cvar_0 >0$ such that for $d \\geq d_0$ and $\\cvar \\in (0, \\cvar_0)$ there exist $\\cvar'<\\cvar$ such that $F_d^{\\circ 2}(D_\\cvar) \\subset D_{\\cvar'}$.\nAs $P_\\cvar = \\cup_{\\pi \\in S_q} \\pi \\cdot D_\\cvar$, we see by Lemma \\ref{lem:symmetry} that for  $d \\geq d_0$ and $\\cvar \\in (0, \\cvar_0)$ we have $F_d^{\\circ 2}(P_\\cvar) \\subset P_{\\cvar'}$.\n\nNext we consider $\\cvar \\in [\\cvar_0, q+1]$. \nBy Proposition~\\ref{prop:2step-forward infinity} we know  $F_\\infty^{\\circ 2}(P_\\cvar) \\subset P_{\\phi(\\cvar)}$ and $\\phi(\\cvar)<\\cvar$ for any $\\cvar>0$. \nAs $F_d$ converges to $F_\\infty$ uniformly, we see for each $\\cvar \\in [\\cvar_0, q+1]$ there is a $d_\\cvar>0$ large enough such that for $d \\geq d_\\cvar$ and $\\cvar'=\\cvar/2+\\phi(\\cvar)/2$ we have $F_d^{\\circ 2}(P_{\\hat{\\cvar}}) \\subsetneq P_{\\cvar'}$ for all $\\hat{\\cvar}$ sufficiently close to $\\cvar$. \nBy compactness of $[\\cvar_0, q+1]$, we obtain that there is a $d_{\\text{max}} >0$ such that for any $d> d_{\\text{max}}$ and any $\\cvar \\in [\\cvar_0, q+1]$ there exists $\\cvar'<\\cvar$ such that $F_d^{\\circ 2}(P_\\cvar) \\subsetneq P_{\\cvar'}$.\nThe proposition now follows by taking $d_2 = \\max ( d_0, d_{\\text{max}} ) $.\n\\end{proof}\n\n\\section{Concluding remarks}\nAlthough we have only proved uniqueness of the Gibbs measure on the infinite regular tree for a sufficiently large degree $d$, our method could conceivably be extended to smaller values of $d$. With the aid of a computer we managed to check that for $q=3$ and $q=4$ and all $d\\geq 2$ the map $F_d^{\\circ 2}$ maps $P_c$ into $P_{\\phi_d(-c)}$, where $\\phi_d$ is the restriction of $-F_d^{\\circ 2}$ to the line $\\mathbb{R}\\cdot \\mathbf{1}$. \nIt seems reasonable to expect that for other small values of $q$ a similar statement could be proved. A general approach is elusive so far.\nIt is moreover also not clear that $F_d(P_c)$ is convex, not even for $q=3$. \nIn fact, for $q=3$ and $c$ large enough $F_3(P_c)$ is {\\bf not} convex. \nBut for reasonable values of $c$ it does appear to be convex. For larger values of $q$ this is even less clear.\n\n\nKnowing that there is a unique Gibbs measure on the infinite regular tree is by itself not sufficient to design efficient algorithms to approximately compute the partition function/sample from the associated distribution on all bounded degree graphs.\nOne needs a stronger notion of decay of correlations, often called \\emph{strong spatial mixing}~\\cite{weitz06,Gamarnikcounting,GamarnikSSM,lu2013improved} or absence of complex zeros for the partition function near the real interval $[w,1]$~\\cite{Barbook,PaR17,bencs,liu2019correlation}.\nIt is not clear whether our current approach is capable of proving such statements (these certainly do not follow automatically), but we hope that it may serve as a building block in determining the threshold(s) for strong spatial mixing and absence of complex zeros. \nWe note that even for the case $w=0$, corresponding to proper colorings, the best known bounds for strong spatial mixing on the infinite tree~\\cite{SSMcoloring} are still far from the uniqueness threshold.\n\n\\begin{appendix}\n\\begin{section}{Supplementary Mathematica code to Lemma~\\ref{lemma:3variable}}\n\\label{sec: Mathematica code}\nThe functions $A_i$ for $i=1,2,3$ and $\\Delta$ are defined as follows.\n\n\\begin{lstlisting}[language=Mathematica,caption={The functions $A_i$ and $\\Delta$},label = {code: Ai Delta}]\nA1[y1_, y2_, y3_, m_] := (m + 1) (1 - y1)/(1 + l y1 + y2 + (m - (l + 1)) y3)\nA2[y1_, y2_, y3_, m_] := (m + 1) (1 - y2)/(1 + l y1 + y2 + (m - (l + 1)) y3)\nA3[y1_, y2_, y3_, m_] := (m + 1) (1 - y3)/(1 + l y1 + y2 + (m - (l + 1)) y3)\n\nDelta[y1_, y2_, y3_, m_] := (y1 y3 (m - l - 1) + (l + 1) y1 + (l + 1) y1 y2 - l y2) Exp[A1[y1, y2, y3, m]] \n+ (-y2 y3 (m - l - 1) - (l + 1) y1 y2 + y1 - 2 y2) Exp[A2[y1, y2, y3, m]] \n+ (y1 - y2) (1 - y3) (m - l - 1) Exp[A3[y1, y2, y3, m]]\n\\end{lstlisting}\nThe functions $y_i(t)$ are defined as follows.\n\n\\begin{lstlisting}[language=Mathematica,caption={The functions $y_i$}, label = {code: yi}]\n{y1[t_], y2[t_], y3[t_]} = {y1, y2, y3} /. Solve[A1[y1, y2, y3, t] == C1 && A2[y1, y2, y3, t] == C2 && A3[y1, y2, y3, t] == C3, {y1, y2, y3}][[1]]\n\\end{lstlisting}\n\n\\begin{lstlisting}[language=Mathematica,caption={Verification that $y_i(q-1) = x_i$. This expression yields $\\{x_1,x_2,x_3\\}$}, label = {code: yi2}]\nSimplify[{y1[q - 1], y2[q - 1], y3[q - 1]} /. {Rule[C1, A1[x1, x2, x3, q - 1]], Rule[C2, A2[x1, x2, x3, q - 1]], Rule[C3, A3[x1, x2, x3, q - 1]]}]\n\\end{lstlisting}\n\n\nThe function $r(t)$ can subsequently be found with the following code.\n\n\\begin{lstlisting}[language=Mathematica,caption={The function r}, label = {code: r}]\nr[t_] = Simplify[Delta[y1[t], y2[t], y3[t], t] ((1 + t)/(1 + C2 - C3 + C1 l - C3 l + t + C3 t))^(-2)]\n\\end{lstlisting}\n\nIt can be observed that $r$ is indeed linear in $t$. To calculate $r(l+1)$ and the slope of $r$\nwe use the following piece of code.\n\n\\begin{lstlisting}[language=Mathematica,caption={The values of $r(l+1)$ and the slope of $r$}, label = {code: r slope}]\nSimplify[{r[l + 1], Coefficient[r[t], t]}]\n\\end{lstlisting}\n\n\n\n\\end{section}\n\\end{appendix}\n\n\\bibliographystyle{alpha}\n", "meta": {"timestamp": "2023-01-30T02:12:36", "yymm": "2203", "arxiv_id": "2203.15457", "language": "en", "url": "https://arxiv.org/abs/2203.15457"}}
